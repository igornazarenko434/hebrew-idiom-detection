{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebace83",
   "metadata": {},
   "source": [
    "# Complete Dataset Analysis (Clean)\n",
    "## Hebrew Idiom Identification Dataset - Hebrew-Idioms-4800\n",
    "\n",
    "Purpose: Comprehensive validation, statistics, and visualization of the dataset for professor review.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Data Loading\n",
    "2. Basic Statistics\n",
    "3. Data Quality Validation\n",
    "4. Preprocessing & Span Verification\n",
    "5. Label Distribution Analysis\n",
    "6. Sentence & Idiom Length Analysis\n",
    "7. Idiom Position Analysis\n",
    "8. Polysemy Analysis\n",
    "9. Lexical Diversity & Richness\n",
    "10. Structural Complexity\n",
    "11. Morphological Variance (Hebrew-Specific)\n",
    "12. IOB2 Tag Validation\n",
    "13. Sentence Type Analysis\n",
    "14. Summary & Validation Report\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "13e0b2e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.388624Z",
     "start_time": "2025-11-25T18:04:52.172594Z"
    }
   },
   "source": [
    "# 1. Setup and Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def parse_list(val):\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except Exception:\n",
    "        return str(val).split()\n",
    "\n",
    "DATA_PATH = Path('data/expressions_data_tagged.csv')\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, encoding='utf-8-sig')\n",
    "df['tokens'] = df['tokens'].apply(parse_list)\n",
    "df['iob_tags'] = df['iob_tags'].apply(parse_list)\n",
    "for col in ['label','start_token','end_token','num_tokens','start_char','end_char']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce').astype(int)\n",
    "\n",
    "print('âœ… Dataset loaded')\n",
    "print(f\"Shape: {len(df)} rows x {df.shape[1]} columns\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded\n",
      "Shape: 4800 rows x 17 columns\n",
      "Columns: ['id', 'sentence', 'base_pie', 'pie_span', 'label', 'label_str', 'tokens', 'iob_tags', 'start_token', 'end_token', 'num_tokens', 'char_mask', 'start_char', 'end_char', 'split', 'language', 'source']\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "id": "12c55810",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "id": "a976fe81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.406248Z",
     "start_time": "2025-11-25T18:04:52.401615Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('BASIC DATASET STATISTICS')\n",
    "print('='*70)\n",
    "print(f\"ğŸ“Š Total sentences: {len(df)}\")\n",
    "print(f\"ğŸ“Š Unique idioms: {df['base_pie'].nunique()}\")\n",
    "print(f\"ğŸ“Š Samples per idiom: {len(df) // df['base_pie'].nunique()}\")\n",
    "expr_counts = df['base_pie'].value_counts()\n",
    "print(f\"ğŸ“Š Expression Occurrence Statistics:\")\n",
    "print(f\"  Min: {expr_counts.min()}\")\n",
    "print(f\"  Max: {expr_counts.max()}\")\n",
    "print(f\"  Mean: {expr_counts.mean():.2f}\")\n",
    "print(f\"  Std: {expr_counts.std():.2f}\")\n",
    "print(f\"ğŸ“Š Split sizes: {df['split'].value_counts().to_dict()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BASIC DATASET STATISTICS\n",
      "======================================================================\n",
      "ğŸ“Š Total sentences: 4800\n",
      "ğŸ“Š Unique idioms: 60\n",
      "ğŸ“Š Samples per idiom: 80\n",
      "ğŸ“Š Expression Occurrence Statistics:\n",
      "  Min: 80\n",
      "  Max: 80\n",
      "  Mean: 80.00\n",
      "  Std: 0.00\n",
      "ğŸ“Š Split sizes: {'train': 3456, 'unseen_idiom_test': 480, 'validation': 432, 'test': 432}\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "id": "af1f0789",
   "metadata": {},
   "source": [
    "## 3. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "id": "12c353f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.432371Z",
     "start_time": "2025-11-25T18:04:52.410699Z"
    }
   },
   "source": [
    "print('='*70)\n",
    "print('DATA QUALITY VALIDATION')\n",
    "print('='*70)\n",
    "validation_results = {}\n",
    "missing = df.isnull().sum().sum()\n",
    "validation_results['Missing values'] = missing == 0\n",
    "print(f\"1. Missing values: {missing}/{len(df)*len(df.columns)}\", 'âœ…' if missing == 0 else 'âŒ')\n",
    "hashable_cols = [c for c in df.columns if c not in ['tokens','iob_tags']]\n",
    "duplicates = df[hashable_cols].duplicated().sum()\n",
    "validation_results['Duplicates'] = duplicates == 0\n",
    "print(f\"2. Duplicate rows: {duplicates}/{len(df)}\", 'âœ…' if duplicates == 0 else 'âŒ')\n",
    "id_valid = (df['id'].nunique() == len(df))\n",
    "validation_results['ID sequence'] = id_valid\n",
    "print('3. ID column validity (unique == total rows):', 'âœ…' if id_valid else 'âŒ')\n",
    "label_mismatch = ((df['label_str'] == 'Literal') & (df['label'] != 0)).sum() + ((df['label_str'] == 'Figurative') & (df['label'] != 1)).sum()\n",
    "validation_results['Label consistency'] = label_mismatch == 0\n",
    "print(f\"4. Label consistency errors: {label_mismatch}\", 'âœ…' if label_mismatch == 0 else 'âŒ')\n",
    "# Encoding checks\n",
    "bom_count = df['sentence'].str.contains('ï»¿').sum()\n",
    "zwsp_count = df['sentence'].str.contains('â€‹').sum()\n",
    "zwj_count = df['sentence'].str.contains('â€').sum()\n",
    "lrm_count = df['sentence'].str.contains('â€').sum()\n",
    "rlm_count = df['sentence'].str.contains('â€').sum()\n",
    "encoding_clean = (bom_count + zwsp_count + zwj_count + lrm_count + rlm_count) == 0\n",
    "validation_results['Encoding'] = encoding_clean\n",
    "print(f\"5. Encoding validation: BOM={bom_count}, ZWSP={zwsp_count}, ZWJ={zwj_count}, LRM={lrm_count}, RLM={rlm_count}\", 'âœ…' if encoding_clean else 'âŒ')\n",
    "# Hebrew text presence\n",
    "hebrew_pattern = re.compile(r'[Ö-×¿]')\n",
    "has_hebrew = df['sentence'].apply(lambda x: bool(hebrew_pattern.search(str(x)))).sum()\n",
    "validation_results['Hebrew text'] = has_hebrew == len(df)\n",
    "print(f\"6. Sentences with Hebrew text: {has_hebrew}/{len(df)}\", 'âœ…' if has_hebrew == len(df) else 'âŒ')\n",
    "# Whitespace\n",
    "trailing_ws = df['sentence'].str.endswith(' ').sum() + df['sentence'].str.startswith(' ').sum()\n",
    "multi_spaces = df['sentence'].str.contains('  ').sum()\n",
    "print(\"7. Whitespace issues:\")\n",
    "print(f\"   Trailing/leading whitespace: {trailing_ws} ({100*trailing_ws/len(df):.2f}%)\")\n",
    "print(f\"   Multiple consecutive spaces: {multi_spaces} ({100*multi_spaces/len(df):.2f}%)\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA QUALITY VALIDATION\n",
      "======================================================================\n",
      "1. Missing values: 0/81600 âœ…\n",
      "2. Duplicate rows: 0/4800 âœ…\n",
      "3. ID column validity (unique == total rows): âœ…\n",
      "4. Label consistency errors: 0 âœ…\n",
      "5. Encoding validation: BOM=0, ZWSP=0, ZWJ=0, LRM=0, RLM=0 âœ…\n",
      "6. Sentences with Hebrew text: 4800/4800 âœ…\n",
      "7. Whitespace issues:\n",
      "   Trailing/leading whitespace: 0 (0.00%)\n",
      "   Multiple consecutive spaces: 0 (0.00%)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "8e53c572",
   "metadata": {},
   "source": [
    "## 4. Preprocessing & Span Verification"
   ]
  },
  {
   "cell_type": "code",
   "id": "cce1222e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.548654Z",
     "start_time": "2025-11-25T18:04:52.435536Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('SPAN & MASK VERIFICATION')\n",
    "print('='*70)\n",
    "\n",
    "char_span_errors = []\n",
    "mask_errors = []\n",
    "token_span_errors = []\n",
    "iob_len_errors = []\n",
    "valid_tags = {'O','B-IDIOM','I-IDIOM'}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sent = row['sentence']\n",
    "    sc, ec = int(row['start_char']), int(row['end_char'])\n",
    "    pie = row['pie_span']\n",
    "    if sent[sc:ec] != pie:\n",
    "        char_span_errors.append(row['id'])\n",
    "    mask = str(row['char_mask'])\n",
    "    if len(mask) != len(sent) or mask[sc:ec] != '1'*(ec-sc) or ('1' in mask[:sc] or '1' in mask[ec:]):\n",
    "        mask_errors.append(row['id'])\n",
    "    tokens = row['tokens']\n",
    "    tags = row['iob_tags']\n",
    "    if len(tokens) != len(tags) or len(tokens) != row['num_tokens']:\n",
    "        iob_len_errors.append(row['id'])\n",
    "    st, et = int(row['start_token']), int(row['end_token'])\n",
    "    if not (0 <= st < et <= len(tokens)):\n",
    "        token_span_errors.append(row['id'])\n",
    "    elif ' '.join(tokens[st:et]) != pie:\n",
    "        token_span_errors.append(row['id'])\n",
    "    if any(t not in valid_tags for t in tags):\n",
    "        iob_len_errors.append(row['id'])\n",
    "\n",
    "print(f\"Char span errors: {len(char_span_errors)}\")\n",
    "print(f\"Char mask errors: {len(mask_errors)}\")\n",
    "print(f\"Token span errors: {len(token_span_errors)}\")\n",
    "print(f\"IOB length/tag errors: {len(iob_len_errors)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPAN & MASK VERIFICATION\n",
      "======================================================================\n",
      "Char span errors: 0\n",
      "Char mask errors: 0\n",
      "Token span errors: 0\n",
      "IOB length/tag errors: 0\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "9d9093a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.641256Z",
     "start_time": "2025-11-25T18:04:52.552358Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('ADDITIONAL SPAN/IOB CONSISTENCY')\n",
    "print('='*70)\n",
    "\n",
    "span_mismatches = []\n",
    "for _, row in df.iterrows():\n",
    "    tags = row['iob_tags']\n",
    "    spans = []\n",
    "    current = None\n",
    "    for i, t in enumerate(tags):\n",
    "        if t == 'B-IDIOM':\n",
    "            if current is not None:\n",
    "                spans.append(current)\n",
    "            current = [i, i+1]\n",
    "        elif t == 'I-IDIOM' and current:\n",
    "            current[1] = i+1\n",
    "        else:\n",
    "            if current is not None:\n",
    "                spans.append(current)\n",
    "                current = None\n",
    "    if current is not None:\n",
    "        spans.append(current)\n",
    "\n",
    "    if len(spans) != 1:\n",
    "        span_mismatches.append(row['id'])\n",
    "        continue\n",
    "\n",
    "    s, e = spans[0]\n",
    "    if s != int(row['start_token']) or e != int(row['end_token']):\n",
    "        span_mismatches.append(row['id'])\n",
    "        continue\n",
    "    tok_slice = ' '.join(row['tokens'][s:e])\n",
    "    if tok_slice != row['pie_span']:\n",
    "        span_mismatches.append(row['id'])\n",
    "\n",
    "print(f\"Span/IOB single-span mismatches: {len(span_mismatches)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ADDITIONAL SPAN/IOB CONSISTENCY\n",
      "======================================================================\n",
      "Span/IOB single-span mismatches: 0\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "4a412748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.734401Z",
     "start_time": "2025-11-25T18:04:52.644368Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('CROSS-FIELD SPAN CONSISTENCY')\n",
    "print('='*70)\n",
    "\n",
    "cf_errors = []\n",
    "for _, row in df.iterrows():\n",
    "    sent = row['sentence']\n",
    "    sc, ec = int(row['start_char']), int(row['end_char'])\n",
    "    pie = row['pie_span']\n",
    "    tok_slice = ' '.join(row['tokens'][int(row['start_token']):int(row['end_token'])])\n",
    "    if sent[sc:ec] != pie or tok_slice != pie:\n",
    "        cf_errors.append(row['id'])\n",
    "print(f\"Cross-field span mismatches: {len(cf_errors)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CROSS-FIELD SPAN CONSISTENCY\n",
      "======================================================================\n",
      "Cross-field span mismatches: 0\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "bd9d7058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.740869Z",
     "start_time": "2025-11-25T18:04:52.737797Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('LABEL STRING CONSISTENCY')\n",
    "print('='*70)\n",
    "\n",
    "mismatch = df[~(((df['label']==0) & (df['label_str']=='Literal')) | ((df['label']==1) & (df['label_str']=='Figurative')))]\n",
    "print(f\"Labelâ†”label_str mismatches: {len(mismatch)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LABEL STRING CONSISTENCY\n",
      "======================================================================\n",
      "Labelâ†”label_str mismatches: 0\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "id": "27a6fa80",
   "metadata": {},
   "source": [
    "## 5. Label Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ec4b6e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.748729Z",
     "start_time": "2025-11-25T18:04:52.744781Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('LABEL DISTRIBUTION')\n",
    "print('='*70)\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "print(label_counts)\n",
    "print('Per split:')\n",
    "print(df.groupby(['split','label'])['id'].count())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LABEL DISTRIBUTION\n",
      "======================================================================\n",
      "label\n",
      "0    2400\n",
      "1    2400\n",
      "Name: count, dtype: int64\n",
      "Per split:\n",
      "split              label\n",
      "test               0         216\n",
      "                   1         216\n",
      "train              0        1728\n",
      "                   1        1728\n",
      "unseen_idiom_test  0         240\n",
      "                   1         240\n",
      "validation         0         216\n",
      "                   1         216\n",
      "Name: id, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "id": "ef8691d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.759600Z",
     "start_time": "2025-11-25T18:04:52.753362Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('SPLIT INTEGRITY CHECKS')\n",
    "print('='*70)\n",
    "\n",
    "# IDs should be unique across splits\n",
    "id_split_counts = df.groupby('id')['split'].nunique()\n",
    "leak_ids = id_split_counts[id_split_counts > 1].index.tolist()\n",
    "print(f\"IDs appearing in multiple splits: {len(leak_ids)}\")\n",
    "\n",
    "# Unseen idioms should not appear in seen splits\n",
    "unseen_idioms = set(df[df['split']=='unseen_idiom_test']['base_pie'])\n",
    "seen_idioms = set(df[df['split']!='unseen_idiom_test']['base_pie'])\n",
    "leaking_unseen = unseen_idioms & seen_idioms\n",
    "print(f\"Unseen idioms leaking into seen splits: {len(leaking_unseen)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPLIT INTEGRITY CHECKS\n",
      "======================================================================\n",
      "IDs appearing in multiple splits: 0\n",
      "Unseen idioms leaking into seen splits: 0\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "id": "6b5abf57",
   "metadata": {},
   "source": [
    "## 6. Sentence & Idiom Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "90410ea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.767778Z",
     "start_time": "2025-11-25T18:04:52.762173Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('LENGTH ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "sent_tokens = df['num_tokens']\n",
    "sent_chars = df['sentence'].str.len()\n",
    "idiom_tokens = df['end_token'] - df['start_token']\n",
    "idiom_chars = df['pie_span'].str.len()\n",
    "\n",
    "print('Sentence length (tokens):')\n",
    "print(f\"  Mean: {sent_tokens.mean():.2f} | Median: {sent_tokens.median():.0f} | Std: {sent_tokens.std():.2f} | Range: {sent_tokens.min()}-{sent_tokens.max()}\")\n",
    "print('Sentence length (chars):')\n",
    "print(f\"  Mean: {sent_chars.mean():.2f} | Median: {sent_chars.median():.0f} | Std: {sent_chars.std():.2f} | Range: {sent_chars.min()}-{sent_chars.max()}\")\n",
    "print('Idiom length (tokens):')\n",
    "print(f\"  Mean: {idiom_tokens.mean():.2f} | Median: {idiom_tokens.median():.0f} | Range: {idiom_tokens.min()}-{idiom_tokens.max()}\")\n",
    "print('Idiom length (chars):')\n",
    "print(f\"  Mean: {idiom_chars.mean():.2f} | Median: {idiom_chars.median():.0f} | Range: {idiom_chars.min()}-{idiom_chars.max()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LENGTH ANALYSIS\n",
      "======================================================================\n",
      "Sentence length (tokens):\n",
      "  Mean: 17.47 | Median: 13 | Std: 9.11 | Range: 5-47\n",
      "Sentence length (chars):\n",
      "  Mean: 83.03 | Median: 63 | Std: 42.55 | Range: 22-193\n",
      "Idiom length (tokens):\n",
      "  Mean: 2.48 | Median: 2 | Range: 2-5\n",
      "Idiom length (chars):\n",
      "  Mean: 11.39 | Median: 11 | Range: 5-23\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "id": "e7e3ae08",
   "metadata": {},
   "source": [
    "## 7. Idiom Position Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "adadb52c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.776886Z",
     "start_time": "2025-11-25T18:04:52.770459Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('IDIOM POSITION ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "df['position_ratio'] = df['start_token']/df['num_tokens']\n",
    "cat = lambda r: 'start' if r<=0.33 else ('middle' if r<=0.67 else 'end')\n",
    "df['position_category'] = df['position_ratio'].apply(cat)\n",
    "\n",
    "print('Overall position counts:', df['position_category'].value_counts().to_dict())\n",
    "print(f\"Mean position ratio: {df['position_ratio'].mean():.4f}\")\n",
    "print('By label:')\n",
    "print(df.groupby(['label','position_category'])['id'].count())\n",
    "print('By split:')\n",
    "print(df.groupby(['split','position_category'])['id'].count())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IDIOM POSITION ANALYSIS\n",
      "======================================================================\n",
      "Overall position counts: {'start': 3123, 'middle': 1449, 'end': 228}\n",
      "Mean position ratio: 0.2678\n",
      "By label:\n",
      "label  position_category\n",
      "0      end                    91\n",
      "       middle                756\n",
      "       start                1553\n",
      "1      end                   137\n",
      "       middle                693\n",
      "       start                1570\n",
      "Name: id, dtype: int64\n",
      "By split:\n",
      "split              position_category\n",
      "test               end                    21\n",
      "                   middle                126\n",
      "                   start                 285\n",
      "train              end                   170\n",
      "                   middle               1041\n",
      "                   start                2245\n",
      "unseen_idiom_test  end                    14\n",
      "                   middle                143\n",
      "                   start                 323\n",
      "validation         end                    23\n",
      "                   middle                139\n",
      "                   start                 270\n",
      "Name: id, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "id": "d8560b6a",
   "metadata": {},
   "source": [
    "## 8. Polysemy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "048cf162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.783514Z",
     "start_time": "2025-11-25T18:04:52.779779Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('POLYSEMY ANALYSIS')\n",
    "print('='*70)\n",
    "expr_label = df.groupby(['base_pie','label'])['id'].count().unstack(fill_value=0)\n",
    "polysemous = expr_label[(expr_label.get(0,0)>0) & (expr_label.get(1,0)>0)]\n",
    "print(f\"Polysemous idioms: {len(polysemous)}/{df['base_pie'].nunique()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POLYSEMY ANALYSIS\n",
      "======================================================================\n",
      "Polysemous idioms: 60/60\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "id": "143af64f",
   "metadata": {},
   "source": [
    "## 9. Lexical Diversity & Richness"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9d41ef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.907314Z",
     "start_time": "2025-11-25T18:04:52.789052Z"
    }
   },
   "source": [
    "print('='*70)\n",
    "print('LEXICAL DIVERSITY')\n",
    "print('='*70)\n",
    "all_tokens = [t for row in df['tokens'] for t in row]\n",
    "word_freq = Counter(all_tokens)\n",
    "vocab = set(all_tokens)\n",
    "print(f\"Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"Total tokens: {len(all_tokens):,}\")\n",
    "print(f\"Type-Token Ratio (TTR): {len(vocab)/len(all_tokens):.4f}\")\n",
    "hapax = sum(1 for v in word_freq.values() if v==1)\n",
    "dis = sum(1 for v in word_freq.values() if v==2)\n",
    "print(f\"Hapax legomena: {hapax} ({hapax/len(vocab)*100:.1f}% of vocab)\")\n",
    "print(f\"Dis legomena: {dis}\")\n",
    "print('Top 10 tokens:')\n",
    "print(word_freq.most_common(10))\n",
    "ctx=[]\n",
    "for _, row in df.iterrows():\n",
    "    toks=row['tokens']\n",
    "    s=int(row['start_token']); e=int(row['end_token'])\n",
    "    left=max(0,s-3); right=min(len(toks), e+3)\n",
    "    ctx.extend(toks[left:s]); ctx.extend(toks[e:right])\n",
    "ctx_freq=Counter(ctx)\n",
    "print('Context window stats:')\n",
    "print(f\"  Total: {len(ctx)} | Unique: {len(ctx_freq)}\")\n",
    "print('  Top 10:', ctx_freq.most_common(10))\n",
    "# Word-only metrics (exclude tokens without Hebrew/Latin letters)\n",
    "word_only = [t for t in all_tokens if re.search(r'[A-Za-z\\u0590-\\u05FF]', t)]\n",
    "word_only_freq = Counter(word_only)\n",
    "word_only_vocab = set(word_only)\n",
    "print('\\nWord-only Lexical Stats:')\n",
    "print(f\"  Vocab size: {len(word_only_vocab):,}\")\n",
    "print(f\"  Total tokens: {len(word_only):,}\")\n",
    "print(f\"  TTR: {len(word_only_vocab)/len(word_only):.4f}\")\n",
    "hapax_w = sum(1 for v in word_only_freq.values() if v==1)\n",
    "dis_w = sum(1 for v in word_only_freq.values() if v==2)\n",
    "print(f\"  Hapax legomena: {hapax_w} ({hapax_w/len(word_only_vocab)*100:.1f}% of vocab)\")\n",
    "print(f\"  Dis legomena: {dis_w}\")\n",
    "print('  Top 10 words:', word_only_freq.most_common(10))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LEXICAL DIVERSITY\n",
      "======================================================================\n",
      "Vocabulary size: 15,107\n",
      "Total tokens: 83,844\n",
      "Type-Token Ratio (TTR): 0.1802\n",
      "Hapax legomena: 8594 (56.9% of vocab)\n",
      "Dis legomena: 2430\n",
      "Top 10 tokens:\n",
      "[(',', 3733), ('.', 3690), ('××ª', 2295), ('×œ×', 1296), ('×”×•×', 1107), ('×”×™×', 1007), ('×¢×œ', 918), ('×©×œ', 756), ('××', 659), ('â€“', 558)]\n",
      "Context window stats:\n",
      "  Total: 23955 | Unique: 7095\n",
      "  Top 10: [(',', 1424), ('×”×•×', 841), ('×”×™×', 742), ('.', 557), ('×œ×', 456), ('×”×', 420), ('×¢×œ', 345), ('××ª', 301), ('×¢×', 243), ('×›×“×™', 236)]\n",
      "\n",
      "Word-only Lexical Stats:\n",
      "  Vocab size: 15,089\n",
      "  Total tokens: 74,883\n",
      "  TTR: 0.2015\n",
      "  Hapax legomena: 8588 (56.9% of vocab)\n",
      "  Dis legomena: 2430\n",
      "  Top 10 words: [('××ª', 2295), ('×œ×', 1296), ('×”×•×', 1107), ('×”×™×', 1007), ('×¢×œ', 918), ('×©×œ', 756), ('××', 659), ('×”×', 521), ('××—×¨×™', 518), ('×”×™×”', 510)]\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "markdown",
   "id": "e7b336e4",
   "metadata": {},
   "source": [
    "## 10. Structural Complexity"
   ]
  },
  {
   "cell_type": "code",
   "id": "d6206388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.954547Z",
     "start_time": "2025-11-25T18:04:52.910719Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('STRUCTURAL COMPLEXITY')\n",
    "print('='*70)\n",
    "subclause_markers = ['×©','×›×™','××','×›××©×¨','×œ××¨×•×ª','×‘×’×œ×œ','×¢×“','×œ×¤× ×™','××—×¨×™']\n",
    "\n",
    "def count_markers(tokens):\n",
    "    toks = tokens if isinstance(tokens, list) else str(tokens).split()\n",
    "    return sum(1 for t in toks for m in subclause_markers if t.startswith(m))\n",
    "\n",
    "df['subclause_count'] = df['tokens'].apply(count_markers)\n",
    "df['punct_count'] = df['sentence'].str.count(r'[.,!?;:\\-â€“]')\n",
    "print('Subclause count mean:', df['subclause_count'].mean())\n",
    "print('Punctuation mean:', df['punct_count'].mean())\n",
    "print('By label (punct mean):')\n",
    "print(df.groupby('label')['punct_count'].mean())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STRUCTURAL COMPLEXITY\n",
      "======================================================================\n",
      "Subclause count mean: 1.9114583333333333\n",
      "Punctuation mean: 1.7816666666666667\n",
      "By label (punct mean):\n",
      "label\n",
      "0    1.728750\n",
      "1    1.834583\n",
      "Name: punct_count, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "id": "50ee55df",
   "metadata": {},
   "source": [
    "## 11. Morphological Variance (Hebrew-Specific)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b540c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:52.964208Z",
     "start_time": "2025-11-25T18:04:52.957561Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('MORPHOLOGICAL VARIANCE')\n",
    "print('='*70)\n",
    "variants = df.groupby('base_pie')['pie_span'].nunique().sort_values(ascending=False)\n",
    "print('Top 10 idioms by variant count:')\n",
    "print(variants.head(10))\n",
    "\n",
    "prefixes = ('×”','×•','×œ','×›','×‘','×')\n",
    "df['prefixed_token_present'] = df['tokens'].apply(lambda toks: any(t.startswith(prefixes) for t in toks))\n",
    "print(f\"Sentences with prefixed tokens: {df['prefixed_token_present'].sum()} ({df['prefixed_token_present'].mean()*100:.2f}%)\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MORPHOLOGICAL VARIANCE\n",
      "======================================================================\n",
      "Top 10 idioms by variant count:\n",
      "base_pie\n",
      "×©× ×¨×’×œ×™×™×         35\n",
      "×©×‘×¨ ××ª ×”×œ×‘        32\n",
      "×¤×ª×— ×“×œ×ª×•×ª         29\n",
      "×¡×’×¨ ×—×©×‘×•×Ÿ         28\n",
      "×”×•×¨×™×“ ×¤×¨×•×¤×™×œ      23\n",
      "×™×¦× ××”×§×•×¤×¡×”       22\n",
      "×§×™×‘×œ ×¡×˜×™×¨×”        18\n",
      "×©× ×¢×œ×™×• ×¤×¡        18\n",
      "×—×ª×š ×¤×™× ×”          18\n",
      "×™×¨×“ ×œ×• ×”××¡×™××•×Ÿ    17\n",
      "Name: pie_span, dtype: int64\n",
      "Sentences with prefixed tokens: 4800 (100.00%)\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "id": "4000cdc4",
   "metadata": {},
   "source": "## 12. IOB Tag Validation"
  },
  {
   "cell_type": "code",
   "id": "e5008a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:07:39.543719Z",
     "start_time": "2025-11-25T18:07:39.466552Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('IOB VALIDATION')\n",
    "print('='*70)\n",
    "valid_tags={'O','B-IDIOM','I-IDIOM'}\n",
    "seq_errors=0\n",
    "for _, row in df.iterrows():\n",
    "    tags=row['iob_tags']\n",
    "    if any(t not in valid_tags for t in tags):\n",
    "        seq_errors+=1\n",
    "        continue\n",
    "    if len(tags)!=len(row['tokens']):\n",
    "        seq_errors+=1\n",
    "        continue\n",
    "    for i,t in enumerate(tags):\n",
    "        if t=='I-IDIOM' and (i==0 or tags[i-1] not in ('B-IDIOM','I-IDIOM')):\n",
    "            seq_errors+=1\n",
    "            break\n",
    "print(f\"IOB sequence/validity errors: {seq_errors}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IOB VALIDATION\n",
      "======================================================================\n",
      "IOB sequence/validity errors: 0\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "id": "b3da2a5e",
   "metadata": {},
   "source": [
    "## 13. Sentence Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "33c41be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:53.060760Z",
     "start_time": "2025-11-25T18:04:53.052329Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('SENTENCE TYPE ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "def classify(text):\n",
    "    text=str(text).strip()\n",
    "    if text.endswith('?'): return 'Question'\n",
    "    if text.endswith('!'): return 'Exclamatory'\n",
    "    return 'Declarative'\n",
    "\n",
    "df['sentence_type']=df['sentence'].apply(classify)\n",
    "print(df['sentence_type'].value_counts())\n",
    "print('By label:')\n",
    "print(pd.crosstab(df['sentence_type'], df['label']))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SENTENCE TYPE ANALYSIS\n",
      "======================================================================\n",
      "sentence_type\n",
      "Declarative    4549\n",
      "Question        239\n",
      "Exclamatory      12\n",
      "Name: count, dtype: int64\n",
      "By label:\n",
      "label             0     1\n",
      "sentence_type            \n",
      "Declarative    2294  2255\n",
      "Exclamatory       5     7\n",
      "Question        101   138\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "id": "2b642783",
   "metadata": {},
   "source": [
    "## 14. Summary & Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "id": "0fea5790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:53.076215Z",
     "start_time": "2025-11-25T18:04:53.073774Z"
    }
   },
   "source": [
    "\n",
    "print('='*70)\n",
    "print('SUMMARY & VALIDATION REPORT')\n",
    "print('='*70)\n",
    "for k,v in validation_results.items():\n",
    "    print(f\"{k}:\", 'âœ…' if v else 'âŒ')\n",
    "print('All checks complete.')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY & VALIDATION REPORT\n",
      "======================================================================\n",
      "Missing values: âœ…\n",
      "Duplicates: âœ…\n",
      "ID sequence: âœ…\n",
      "Label consistency: âœ…\n",
      "Encoding: âœ…\n",
      "Hebrew text: âœ…\n",
      "All checks complete.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "id": "b20e30c61b664316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T18:04:53.084362Z",
     "start_time": "2025-11-25T18:04:53.083282Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
