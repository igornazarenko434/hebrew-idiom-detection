
import sys
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Add project root to path
project_root = Path.cwd()
sys.path.insert(0, str(project_root))

from src.data_preparation import DatasetLoader

def run_validation():
    print("âœ… Imports successful!")
    print(f"ğŸ“ Project root: {project_root}")

    # 1. Load Dataset
    data_path = project_root / 'data' / 'expressions_data_tagged_v2.csv'
    loader = DatasetLoader(data_path=data_path)
    loader.load_dataset()
    print(f"\nâœ… Dataset loaded: {len(loader.df)} rows, {len(loader.df.columns)} columns")

    # 2. Quick Dataset Overview
    print("Dataset Shape:", loader.df.shape)
    print("\nColumn Names:")
    print(loader.df.columns.tolist())
    print("\nFirst 5 rows:")
    print(loader.df.head())

    # 3. PART 1: REQUIRED ANALYSES
    # 3.1 Basic Dataset Statistics
    basic_stats = loader.generate_statistics()

    # 3.2 Sentence Type Analysis
    sentence_types = loader.analyze_sentence_types()

    # 3.3 Idiom Position Analysis
    position_stats = loader.analyze_idiom_position()
    print("\nğŸ“Š Key Finding: Most idioms appear at sentence start!")
    print(f"   Start: {position_stats['position_percentages']['start']:.2f}%")
    print(f"   Middle: {position_stats['position_percentages']['middle']:.2f}%")
    print(f"   End: {position_stats['position_percentages']['end']:.2f}%")

    # 3.4 Polysemy Analysis
    polysemy_stats = loader.analyze_polysemy()
    print("\nğŸ“Š Key Finding: All idioms are polysemous!")
    print(f"   Total expressions: {polysemy_stats['total_expressions']}")
    print(f"   Polysemous: {polysemy_stats['polysemous_count']} ({polysemy_stats['polysemous_percentage']:.2f}%) # Fixed percentage access")
    print(f"   Only literal: {polysemy_stats['only_literal_count']}")
    print(f"   Only figurative: {polysemy_stats['only_figurative_count']}")

    # 3.5 Lexical Statistics
    lexical_stats = loader.analyze_lexical_statistics()
    print("\nğŸ“Š Key Finding: High lexical diversity!")
    print(f"   Vocabulary size: {lexical_stats['vocabulary_size']:,} unique words")
    print(f"   Type-Token Ratio: {lexical_stats['ttr_overall']:.4f}")
    print(f"   Avg unique words per sentence: {lexical_stats['avg_unique_per_sentence']:.2f}")

    # 4. PART 2: OPTIONAL/RECOMMENDED ANALYSES
    # 4.1 Structural Complexity Analysis
    complexity_stats = loader.analyze_structural_complexity()
    print("\nğŸ“Š Key Finding: Figurative sentences are more complex!")
    print(f"   Mean subclause markers: {complexity_stats['mean_subclause_count']:.2f}")
    print(f"   Sentences with subclauses: {complexity_stats['sentences_with_subclauses_pct']:.2f}%")
    print(f"   Mean punctuation: {complexity_stats['mean_punctuation_count']:.2f}")

    # 4.2 Lexical Richness Analysis
    richness_stats = loader.analyze_lexical_richness()
    print("\nğŸ“Š Key Finding: Very high lexical richness!")
    print(f"   Hapax legomena: {richness_stats['hapax_legomena_count']:,} ({richness_stats['hapax_ratio']*100:.2f}%)")
    print(f"   Dis legomena: {richness_stats['dis_legomena_count']:,}")
    print(f"   Maas Index: {richness_stats['maas_index']:.4f}")

    # 4.3 Collocational Analysis
    collocation_stats = loader.analyze_collocations()
    print("\nğŸ“Š Key Finding: Rich context around idioms!")
    print(f"   Total context words: {collocation_stats['total_context_words']:,}")
    print(f"   Unique context words: {collocation_stats['unique_context_words']:,}")

    # 4.4 Annotation Consistency Analysis
    consistency_stats = loader.analyze_annotation_consistency()
    print("\nğŸ“Š Key Finding: Significant morphological variance!")
    print(f"   Prefix attachments: {consistency_stats['prefix_attachment_count']} ({consistency_stats['prefix_attachment_rate']*100:.2f}%)")
    print(f"   Mean consistency rate: {consistency_stats['mean_consistency_rate']:.4f}")

    # 5. Create All Visualizations
    print("Creating standard visualizations...")
    loader.create_visualizations()
    print("\nâœ… Standard visualizations saved to paper/figures/")

    print("Creating advanced visualizations...")
    loader.create_advanced_visualizations()
    print("\nâœ… Advanced visualizations saved to paper/figures/")

    # 7. Explore Specific Examples
    # 7.1 Compare Literal vs Figurative for Same Idiom
    sample_idiom = "×©×‘×¨ ××ª ×”×¨××©"  # "broke the head"
    print(f"Examples for idiom: '{sample_idiom}'")
    print("=" * 80)

    # UPDATED COLUMN NAMES HERE
    idiom_samples = loader.df[loader.df['base_pie'] == sample_idiom]
    literal_examples = idiom_samples[idiom_samples['label'] == 0].head(3)
    figurative_examples = idiom_samples[idiom_samples['label'] == 1].head(3)

    print("\nğŸ”¹ LITERAL Examples:")
    for idx, row in literal_examples.iterrows():
        print(f"\n{row['sentence']}")

    print("\n" + "=" * 80)
    print("ğŸ”¸ FIGURATIVE Examples:")
    for idx, row in figurative_examples.iterrows():
        print(f"\n{row['sentence']}")

    # 7.2 Examine Top Context Words
    print("Top 15 Context Words Around Idioms:")
    print("="*50)
    for i, (word, count) in enumerate(collocation_stats['top_20_context_overall'][:15], 1):
        print(f"{i:2d}. '{word}': {count:4d} occurrences")

    # 7.3 Examine Idioms with Most Variance
    print("Idioms with Most Variant Forms:")
    print("="*50)
    # UPDATED COLUMN NAMES HERE
    variance_df = loader.df.groupby('base_pie')['pie_span'].nunique().sort_values(ascending=False)

    for i, (idiom, variants) in enumerate(variance_df.head(10).items(), 1):
        print(f"{i:2d}. {idiom:40s}: {variants:2d} variants")
        # UPDATED COLUMN NAMES HERE
        sample_variants = loader.df[loader.df['base_pie'] == idiom]['pie_span'].value_counts().head(3)
        for variant, count in sample_variants.items():
            print(f"    - {variant}: {count} times")
        print()

    # 8. Summary Statistics Table
    summary_data = {
        'Metric': [
            'Total Sentences',
            'Unique Idioms',
            'Avg Sentence Length (tokens)',
            'Avg Idiom Length (tokens)',
            'Vocabulary Size',
            'Type-Token Ratio',
            'Hapax Legomena',
            'Polysemous Idioms',
            'Idioms at Start (%)',
            'Sentences with Subclauses (%)',
            'Mean Prefix Attachment Rate (%)'
        ],
        'Value': [
            f"{len(loader.df):,}",
            f"{loader.df['base_pie'].nunique()}",
            f"{loader.df['num_tokens'].mean():.2f}",
            f"{(loader.df['end_token'] - loader.df['start_token']).mean():.2f}",
            f"{lexical_stats['vocabulary_size']:,}",
            f"{lexical_stats['ttr_overall']:.4f}",
            f"{richness_stats['hapax_legomena_count']:,} ({richness_stats['hapax_ratio']*100:.2f}%)",
            f"{polysemy_stats['polysemous_count']} (100%)",
            f"{position_stats['position_percentages']['start']:.2f}%",
            f"{complexity_stats['sentences_with_subclauses_pct']:.2f}%",
            f"{consistency_stats['prefix_attachment_rate']*100:.2f}%"
        ]
    }

    summary_df = pd.DataFrame(summary_data)
    print("\n" + "="*80)
    print("COMPREHENSIVE DATASET SUMMARY")
    print("="*80)
    print(summary_df.to_string(index=False))

    # 9. Validation Checklist
    print("="*80)
    print("COMPREHENSIVE VALIDATION CHECKLIST")
    print("="*80)

    checklist = [
        ("Dataset loads successfully", len(loader.df) > 0),
        ("4,800 total sentences", len(loader.df) == 4800),
        ("Perfect label balance (50/50)", abs(loader.df['label'].value_counts()[0] - loader.df['label'].value_counts()[1]) == 0),
        ("60 unique idioms", loader.df['base_pie'].nunique() == 60),
        ("No duplicate rows", loader.df.duplicated().sum() == 0),
        ("All idioms are polysemous", polysemy_stats['polysemous_count'] == 60),
        ("High lexical diversity (>60% hapax)", richness_stats['hapax_ratio'] > 0.6),
        ("Most idioms at sentence start", position_stats['position_percentages']['start'] > 50), # Adjusted expectation if needed, but keeping logic
    ]

    all_passed = True
    for criterion, passed in checklist:
        status = "âœ…" if passed else "âŒ"
        print(f"{status} {criterion}")
        if not passed:
            all_passed = False

    if all_passed:
        print("\n" + "="*80)
        print("ğŸ‰ ALL VALIDATION CRITERIA PASSED!")
        print("="*80)
    else:
        print("\nâš ï¸ Some criteria not met.")

if __name__ == "__main__":
    run_validation()
