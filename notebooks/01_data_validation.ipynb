{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission 2: Comprehensive Dataset Analysis and Validation\n",
    "# Hebrew Idiom Detection Project - Enhanced with PART 1 + PART 2 Analyses\n",
    "\n",
    "**Purpose:** Comprehensive exploration, validation, and statistical analysis of the Hebrew idiom dataset\n",
    "\n",
    "**Date:** November 10, 2025\n",
    "\n",
    "**Analysis Coverage:**\n",
    "- PART 1: Required analyses (basic statistics, polysemy, position, lexical)\n",
    "- PART 2: Optional analyses (structural complexity, lexical richness, collocations, consistency)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:22.507376Z",
     "iopub.status.busy": "2025-11-19T12:11:22.507216Z",
     "iopub.status.idle": "2025-11-19T12:11:30.901072Z",
     "shell.execute_reply": "2025-11-19T12:11:30.900656Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:51.760127Z",
     "start_time": "2025-11-25T16:11:51.733627Z"
    }
   },
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import our data preparation module\n",
    "from src.data_preparation import DatasetLoader\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "print(f\"ğŸ“ Project root: {project_root}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n",
      "ğŸ“ Project root: /Users/igornazarenko/PycharmProjects/Final_Project_NLP\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.902609Z",
     "iopub.status.busy": "2025-11-19T12:11:30.902493Z",
     "iopub.status.idle": "2025-11-19T12:11:30.931139Z",
     "shell.execute_reply": "2025-11-19T12:11:30.930762Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:51.818828Z",
     "start_time": "2025-11-25T16:11:51.767113Z"
    }
   },
   "source": [
    "# Initialize loader and load dataset\n",
    "data_path = project_root / 'data' / 'expressions_data_tagged.csv'\n",
    "loader = DatasetLoader(data_path=data_path)\n",
    "loader.load_dataset()  # IMPORTANT: Must explicitly load!\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded: {len(loader.df)} rows, {len(loader.df.columns)} columns\")\n",
    "print(f\"   â€¢ Loaded from: {loader.data_path}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/data/expressions_data_tagged_v2.csv\n",
      "âœ… Dataset loaded successfully!\n",
      "Total rows: 4800\n",
      "\n",
      "âœ… Dataset loaded: 4800 rows, 17 columns\n",
      "   â€¢ Loaded from: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/data/expressions_data_tagged_v2.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.932431Z",
     "iopub.status.busy": "2025-11-19T12:11:30.932366Z",
     "iopub.status.idle": "2025-11-19T12:11:30.939817Z",
     "shell.execute_reply": "2025-11-19T12:11:30.939479Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:51.838870Z",
     "start_time": "2025-11-25T16:11:51.829692Z"
    }
   },
   "source": [
    "# Display basic info\n",
    "print(\"Dataset Shape:\", loader.df.shape)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(loader.df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "loader.df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (4800, 17)\n",
      "\n",
      "Column Names:\n",
      "['id', 'sentence', 'base_pie', 'pie_span', 'label', 'label_str', 'tokens', 'iob_tags', 'start_token', 'end_token', 'num_tokens', 'char_mask', 'start_char', 'end_char', 'split', 'language', 'source']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        id  \\\n",
       "0  1_lit_1   \n",
       "1  1_lit_2   \n",
       "2  1_lit_3   \n",
       "3  1_lit_4   \n",
       "4  1_lit_5   \n",
       "\n",
       "                                                                                              sentence  \\\n",
       "0  ×× ×”×™× ×œ× ×”×™×™×ª×” ×××‘×“×ª ××ª ×”×¦×¤×•×Ÿ ×‘×××¦×¢ ×”×˜×™×•×œ ×‘×”×¨, ××•×œ×™ ×”×™×™×ª×” ××•×¦××ª ××ª ×”×©×‘×™×œ ×‘×–××Ÿ, ××‘×œ ×‘××§×•× ×–×” ×”××©...   \n",
       "1  ×× ×”× ×œ× ×”×™×• ×××‘×“×™× ××ª ×”×¦×¤×•×Ÿ ×‘×–××Ÿ ×”×¦×¢×“×” ×”××¨×•×›×” ×©×œ ×ª×—×¨×•×ª ×”× ×™×•×•×˜, ×”× ×›× ×¨××” ×”×™×• ×–×•×›×™× ×‘××“×œ×™×”, ××‘×œ ×œ...   \n",
       "2                                                                  ×‘×××¦×¢ ×”×“×¨×š ×œ×¢×™×¨, ×”× ××™×‘×“×• ××ª ×”×¦×¤×•×Ÿ.   \n",
       "3  ×‘×˜×™×•×œ ×‘×¦×¤×•×Ÿ ×”×¨×—×•×§ ×©×œ × ×•×¨×‘×’×™×”, ×”×•× ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ ×œ×’××¨×™ ×›×™ ×”××•×¨ ×”×¦×¤×•× ×™ ×¡×™× ×•×•×¨ ××•×ª×• â€“ â€œ××™×š ××¤×©×¨ ×œ×“×¢...   \n",
       "4                                                                 ×‘××¡×¢ ×‘××“×‘×¨ ×¡×”×¨×”, ×”×™× ××™×‘×“×” ××ª ×”×¦×¤×•×Ÿ.   \n",
       "\n",
       "        base_pie         pie_span  label label_str  \\\n",
       "0  ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ   ×××‘×“×ª ××ª ×”×¦×¤×•×Ÿ      0   Literal   \n",
       "1  ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ  ×××‘×“×™× ××ª ×”×¦×¤×•×Ÿ      0   Literal   \n",
       "2  ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ   ××™×‘×“×• ××ª ×”×¦×¤×•×Ÿ      0   Literal   \n",
       "3  ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ    ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ      0   Literal   \n",
       "4  ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ   ××™×‘×“×” ××ª ×”×¦×¤×•×Ÿ      0   Literal   \n",
       "\n",
       "                                                                                                tokens  \\\n",
       "0  ['××', '×”×™×', '×œ×', '×”×™×™×ª×”', '×××‘×“×ª', '××ª', '×”×¦×¤×•×Ÿ', '×‘×××¦×¢', '×”×˜×™×•×œ', '×‘×”×¨', ',', '××•×œ×™', '×”×™×™×ª...   \n",
       "1  ['××', '×”×', '×œ×', '×”×™×•', '×××‘×“×™×', '××ª', '×”×¦×¤×•×Ÿ', '×‘×–××Ÿ', '×”×¦×¢×“×”', '×”××¨×•×›×”', '×©×œ', '×ª×—×¨×•×ª', '×”× ...   \n",
       "2                                    ['×‘×××¦×¢', '×”×“×¨×š', '×œ×¢×™×¨', ',', '×”×', '××™×‘×“×•', '××ª', '×”×¦×¤×•×Ÿ', '.']   \n",
       "3  ['×‘×˜×™×•×œ', '×‘×¦×¤×•×Ÿ', '×”×¨×—×•×§', '×©×œ', '× ×•×¨×‘×’×™×”', ',', '×”×•×', '××™×‘×“', '××ª', '×”×¦×¤×•×Ÿ', '×œ×’××¨×™', '×›×™', '...   \n",
       "4                                   ['×‘××¡×¢', '×‘××“×‘×¨', '×¡×”×¨×”', ',', '×”×™×', '××™×‘×“×”', '××ª', '×”×¦×¤×•×Ÿ', '.']   \n",
       "\n",
       "                                                                                              iob_tags  \\\n",
       "0  ['O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O...   \n",
       "1  ['O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O...   \n",
       "2                                      ['O', 'O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O']   \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O', 'O', 'O', 'O', 'O', 'O...   \n",
       "4                                      ['O', 'O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O']   \n",
       "\n",
       "   start_token  end_token  num_tokens  \\\n",
       "0            4          7          35   \n",
       "1            4          7          32   \n",
       "2            5          8           9   \n",
       "3            7         10          34   \n",
       "4            5          8           9   \n",
       "\n",
       "                                                                                             char_mask  \\\n",
       "0  000000000000000011111111111111000000000000000000000000000000000000000000000000000000000000000000...   \n",
       "1  000000000000011111111111111100000000000000000000000000000000000000000000000000000000000000000000...   \n",
       "2                                                                  00000000000000000000111111111111110   \n",
       "3  000000000000000000000000000000000011111111111110000000000000000000000000000000000000000000000000...   \n",
       "4                                                                 000000000000000000000111111111111110   \n",
       "\n",
       "   start_char  end_char       split language   source  \n",
       "0          16        30       train       he  inhouse  \n",
       "1          13        28       train       he  inhouse  \n",
       "2          20        34       train       he  inhouse  \n",
       "3          34        47       train       he  inhouse  \n",
       "4          21        35  validation       he  inhouse  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>base_pie</th>\n",
       "      <th>pie_span</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "      <th>tokens</th>\n",
       "      <th>iob_tags</th>\n",
       "      <th>start_token</th>\n",
       "      <th>end_token</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>char_mask</th>\n",
       "      <th>start_char</th>\n",
       "      <th>end_char</th>\n",
       "      <th>split</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_lit_1</td>\n",
       "      <td>×× ×”×™× ×œ× ×”×™×™×ª×” ×××‘×“×ª ××ª ×”×¦×¤×•×Ÿ ×‘×××¦×¢ ×”×˜×™×•×œ ×‘×”×¨, ××•×œ×™ ×”×™×™×ª×” ××•×¦××ª ××ª ×”×©×‘×™×œ ×‘×–××Ÿ, ××‘×œ ×‘××§×•× ×–×” ×”××©...</td>\n",
       "      <td>××™×‘×“ ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>×××‘×“×ª ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>0</td>\n",
       "      <td>Literal</td>\n",
       "      <td>['××', '×”×™×', '×œ×', '×”×™×™×ª×”', '×××‘×“×ª', '××ª', '×”×¦×¤×•×Ÿ', '×‘×××¦×¢', '×”×˜×™×•×œ', '×‘×”×¨', ',', '××•×œ×™', '×”×™×™×ª...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>000000000000000011111111111111000000000000000000000000000000000000000000000000000000000000000000...</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>train</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_lit_2</td>\n",
       "      <td>×× ×”× ×œ× ×”×™×• ×××‘×“×™× ××ª ×”×¦×¤×•×Ÿ ×‘×–××Ÿ ×”×¦×¢×“×” ×”××¨×•×›×” ×©×œ ×ª×—×¨×•×ª ×”× ×™×•×•×˜, ×”× ×›× ×¨××” ×”×™×• ×–×•×›×™× ×‘××“×œ×™×”, ××‘×œ ×œ...</td>\n",
       "      <td>××™×‘×“ ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>×××‘×“×™× ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>0</td>\n",
       "      <td>Literal</td>\n",
       "      <td>['××', '×”×', '×œ×', '×”×™×•', '×××‘×“×™×', '××ª', '×”×¦×¤×•×Ÿ', '×‘×–××Ÿ', '×”×¦×¢×“×”', '×”××¨×•×›×”', '×©×œ', '×ª×—×¨×•×ª', '×”× ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O...</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>000000000000011111111111111100000000000000000000000000000000000000000000000000000000000000000000...</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>train</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_lit_3</td>\n",
       "      <td>×‘×××¦×¢ ×”×“×¨×š ×œ×¢×™×¨, ×”× ××™×‘×“×• ××ª ×”×¦×¤×•×Ÿ.</td>\n",
       "      <td>××™×‘×“ ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>××™×‘×“×• ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>0</td>\n",
       "      <td>Literal</td>\n",
       "      <td>['×‘×××¦×¢', '×”×“×¨×š', '×œ×¢×™×¨', ',', '×”×', '××™×‘×“×•', '××ª', '×”×¦×¤×•×Ÿ', '.']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O']</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>00000000000000000000111111111111110</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>train</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_lit_4</td>\n",
       "      <td>×‘×˜×™×•×œ ×‘×¦×¤×•×Ÿ ×”×¨×—×•×§ ×©×œ × ×•×¨×‘×’×™×”, ×”×•× ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ ×œ×’××¨×™ ×›×™ ×”××•×¨ ×”×¦×¤×•× ×™ ×¡×™× ×•×•×¨ ××•×ª×• â€“ â€œ××™×š ××¤×©×¨ ×œ×“×¢...</td>\n",
       "      <td>××™×‘×“ ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>××™×‘×“ ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>0</td>\n",
       "      <td>Literal</td>\n",
       "      <td>['×‘×˜×™×•×œ', '×‘×¦×¤×•×Ÿ', '×”×¨×—×•×§', '×©×œ', '× ×•×¨×‘×’×™×”', ',', '×”×•×', '××™×‘×“', '××ª', '×”×¦×¤×•×Ÿ', '×œ×’××¨×™', '×›×™', '...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O', 'O', 'O', 'O', 'O', 'O...</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>000000000000000000000000000000000011111111111110000000000000000000000000000000000000000000000000...</td>\n",
       "      <td>34</td>\n",
       "      <td>47</td>\n",
       "      <td>train</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_lit_5</td>\n",
       "      <td>×‘××¡×¢ ×‘××“×‘×¨ ×¡×”×¨×”, ×”×™× ××™×‘×“×” ××ª ×”×¦×¤×•×Ÿ.</td>\n",
       "      <td>××™×‘×“ ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>××™×‘×“×” ××ª ×”×¦×¤×•×Ÿ</td>\n",
       "      <td>0</td>\n",
       "      <td>Literal</td>\n",
       "      <td>['×‘××¡×¢', '×‘××“×‘×¨', '×¡×”×¨×”', ',', '×”×™×', '××™×‘×“×”', '××ª', '×”×¦×¤×•×Ÿ', '.']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'B-IDIOM', 'I-IDIOM', 'I-IDIOM', 'O']</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>000000000000000000000111111111111110</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>validation</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PART 1: REQUIRED ANALYSES\n",
    "\n",
    "### 3.1 Basic Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.941103Z",
     "iopub.status.busy": "2025-11-19T12:11:30.941033Z",
     "iopub.status.idle": "2025-11-19T12:11:30.951129Z",
     "shell.execute_reply": "2025-11-19T12:11:30.950775Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:51.891875Z",
     "start_time": "2025-11-25T16:11:51.876615Z"
    }
   },
   "source": [
    "# Run basic statistics (includes expression occurrences, char lengths, etc.)\n",
    "basic_stats = loader.generate_statistics()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Total Sentences: 4800\n",
      "\n",
      "ğŸ“Š Label Distribution:\n",
      "  â€¢  0:  2400 (50.00%)\n",
      "  â€¢  1:  2400 (50.00%)\n",
      "\n",
      "ğŸ“Š Unique Idioms/Expressions: 60\n",
      "\n",
      "ğŸ“Š Expression Occurrence Statistics:\n",
      "  â€¢ Min occurrences per idiom: 80\n",
      "  â€¢ Max occurrences per idiom: 80\n",
      "  â€¢ Mean occurrences per idiom: 80.00\n",
      "  â€¢ Median occurrences per idiom: 80.00\n",
      "  â€¢ Std occurrences per idiom: 0.00\n",
      "\n",
      "ğŸ“Š Sentence Length Statistics (tokens):\n",
      "  â€¢ Average: 17.47 tokens\n",
      "  â€¢ Median:  13 tokens\n",
      "  â€¢ Std:     9.11 tokens\n",
      "  â€¢ Min:     5 tokens\n",
      "  â€¢ Max:     47 tokens\n",
      "\n",
      "ğŸ“Š Sentence Length Statistics (characters):\n",
      "  â€¢ Average: 83.03 chars\n",
      "  â€¢ Median:  63.00 chars\n",
      "  â€¢ Std:     42.55 chars\n",
      "  â€¢ Min:     22 chars\n",
      "  â€¢ Max:     193 chars\n",
      "\n",
      "ğŸ“Š Idiom Length Statistics (tokens):\n",
      "  â€¢ Average: 2.48 tokens\n",
      "  â€¢ Median:  2 tokens\n",
      "  â€¢ Std:     0.64 tokens\n",
      "  â€¢ Min:     2 tokens\n",
      "  â€¢ Max:     5 tokens\n",
      "\n",
      "ğŸ“Š Idiom Length Statistics (characters):\n",
      "  â€¢ Average: 11.39 chars\n",
      "  â€¢ Median:  11.00 chars\n",
      "  â€¢ Std:     3.15 chars\n",
      "  â€¢ Min:     5 chars\n",
      "  â€¢ Max:     23 chars\n",
      "\n",
      "ğŸ“Š Top 10 Most Frequent Expressions:\n",
      "   1. ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ                            :  80 occurrences\n",
      "   2. ××™×‘×“ ××ª ×”×¨××©                             :  80 occurrences\n",
      "   3. × ×©××¨ ×××—×•×¨                               :  80 occurrences\n",
      "   4. × ×©×‘×¨ ××‘×¤× ×™×                              :  80 occurrences\n",
      "   5. × ×©×š ×©×¤×ª×™×™×                               :  80 occurrences\n",
      "   6. × ×ª×Ÿ ×’×–                                   :  80 occurrences\n",
      "   7. × ×ª×Ÿ ×™×“                                   :  80 occurrences\n",
      "   8. ×¡×’×¨ ×—×©×‘×•×Ÿ                                :  80 occurrences\n",
      "   9. ×¢×©×” ×¡×œ×˜                                  :  80 occurrences\n",
      "  10. ×¢×©×” ×¡×¦× ×”                                 :  80 occurrences\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sentence Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.952314Z",
     "iopub.status.busy": "2025-11-19T12:11:30.952245Z",
     "iopub.status.idle": "2025-11-19T12:11:30.970228Z",
     "shell.execute_reply": "2025-11-19T12:11:30.969853Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:51.956712Z",
     "start_time": "2025-11-25T16:11:51.933284Z"
    }
   },
   "source": [
    "# Analyze sentence types (declarative, question, exclamatory)\n",
    "sentence_types = loader.analyze_sentence_types()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SENTENCE TYPE ANALYSIS (Mission 2.4)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Sentence Type Distribution:\n",
      "  â€¢ Declarative    :  4549 (94.77%)\n",
      "  â€¢ Question       :   239 ( 4.98%)\n",
      "  â€¢ Exclamatory    :    12 ( 0.25%)\n",
      "\n",
      "ğŸ“Š Sentence Type by Label (Literal vs Figurative):\n",
      "label             0     1   All\n",
      "sentence_type                  \n",
      "Declarative    2294  2255  4549\n",
      "Exclamatory       5     7    12\n",
      "Question        101   138   239\n",
      "All            2400  2400  4800\n",
      "\n",
      "ğŸ“Š Percentage Distribution within each Label:\n",
      "label              0      1\n",
      "sentence_type              \n",
      "Declarative    95.58  93.96\n",
      "Exclamatory     0.21   0.29\n",
      "Question        4.21   5.75\n",
      "\n",
      "ğŸ“Š Balance Check (are sentence types distributed evenly across labels?):\n",
      "  â€¢ Declarative    : Literal=50.4%, Figurative=49.6%\n",
      "  â€¢ Question       : Literal=42.3%, Figurative=57.7%\n",
      "  â€¢ Exclamatory    : Literal=41.7%, Figurative=58.3%\n",
      "\n",
      "ğŸ“Š Sentence Types by Top Expressions:\n",
      "\n",
      "  ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ:\n",
      "    - Declarative: 79 (98.8%)\n",
      "    - Question: 1 (1.2%)\n",
      "\n",
      "  ××™×‘×“ ××ª ×”×¨××©:\n",
      "    - Declarative: 71 (88.8%)\n",
      "    - Question: 9 (11.2%)\n",
      "\n",
      "  × ×©××¨ ×××—×•×¨:\n",
      "    - Declarative: 79 (98.8%)\n",
      "    - Question: 1 (1.2%)\n",
      "\n",
      "  × ×©×‘×¨ ××‘×¤× ×™×:\n",
      "    - Declarative: 70 (87.5%)\n",
      "    - Question: 10 (12.5%)\n",
      "\n",
      "  × ×©×š ×©×¤×ª×™×™×:\n",
      "    - Declarative: 78 (97.5%)\n",
      "    - Question: 2 (2.5%)\n",
      "\n",
      "  × ×ª×Ÿ ×’×–:\n",
      "    - Declarative: 75 (93.8%)\n",
      "    - Question: 4 (5.0%)\n",
      "    - Exclamatory: 1 (1.2%)\n",
      "\n",
      "  × ×ª×Ÿ ×™×“:\n",
      "    - Declarative: 77 (96.2%)\n",
      "    - Question: 3 (3.8%)\n",
      "\n",
      "  ×¡×’×¨ ×—×©×‘×•×Ÿ:\n",
      "    - Declarative: 78 (97.5%)\n",
      "    - Question: 2 (2.5%)\n",
      "\n",
      "  ×¢×©×” ×¡×œ×˜:\n",
      "    - Declarative: 76 (95.0%)\n",
      "    - Question: 3 (3.8%)\n",
      "    - Exclamatory: 1 (1.2%)\n",
      "\n",
      "  ×¢×©×” ×¡×¦× ×”:\n",
      "    - Declarative: 76 (95.0%)\n",
      "    - Question: 4 (5.0%)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Idiom Position Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.971430Z",
     "iopub.status.busy": "2025-11-19T12:11:30.971356Z",
     "iopub.status.idle": "2025-11-19T12:11:30.976296Z",
     "shell.execute_reply": "2025-11-19T12:11:30.975847Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:51.977623Z",
     "start_time": "2025-11-25T16:11:51.967388Z"
    }
   },
   "source": [
    "# Analyze where idioms appear in sentences\n",
    "position_stats = loader.analyze_idiom_position()\n",
    "\n",
    "# Show position distribution\n",
    "print(\"\\nğŸ“Š Key Finding: Most idioms appear at sentence start!\")\n",
    "print(f\"   Start: {position_stats['position_percentages']['start']:.2f}%\")\n",
    "print(f\"   Middle: {position_stats['position_percentages']['middle']:.2f}%\")\n",
    "print(f\"   End: {position_stats['position_percentages']['end']:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IDIOM POSITION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Position Ratio Statistics:\n",
      "  â€¢ Mean: 0.2678\n",
      "  â€¢ Median: 0.1818\n",
      "  â€¢ Std: 0.2047\n",
      "  â€¢ Range: [0.0000, 0.9167]\n",
      "\n",
      "ğŸ“Š Position Distribution:\n",
      "  â€¢ Start (0-33%): 3123 (65.06%)\n",
      "  â€¢ Middle (33-67%): 1449 (30.19%)\n",
      "  â€¢ End (67-100%): 228 (4.75%)\n",
      "\n",
      "ğŸ” Split 'test':\n",
      "  â€¢ Start: 285 (65.97%)\n",
      "  â€¢ Middle: 126 (29.17%)\n",
      "  â€¢ End: 21 (4.86%)\n",
      "\n",
      "ğŸ” Split 'train':\n",
      "  â€¢ Start: 2245 (64.96%)\n",
      "  â€¢ Middle: 1041 (30.12%)\n",
      "  â€¢ End: 170 (4.92%)\n",
      "\n",
      "ğŸ” Split 'unseen_idiom_test':\n",
      "  â€¢ Start: 323 (67.29%)\n",
      "  â€¢ Middle: 143 (29.79%)\n",
      "  â€¢ End: 14 (2.92%)\n",
      "\n",
      "ğŸ” Split 'validation':\n",
      "  â€¢ Start: 270 (62.50%)\n",
      "  â€¢ Middle: 139 (32.18%)\n",
      "  â€¢ End: 23 (5.32%)\n",
      "\n",
      "ğŸ“Š Position Distribution by Label:\n",
      "\n",
      "  0:\n",
      "    â€¢ Start: 1553 (64.71%)\n",
      "    â€¢ Middle: 756 (31.50%)\n",
      "    â€¢ End: 91 (3.79%)\n",
      "    â€¢ Mean position ratio: 0.2682\n",
      "\n",
      "  1:\n",
      "    â€¢ Start: 1570 (65.42%)\n",
      "    â€¢ Middle: 693 (28.88%)\n",
      "    â€¢ End: 137 (5.71%)\n",
      "    â€¢ Mean position ratio: 0.2673\n",
      "\n",
      "ğŸ“Š Key Finding: Most idioms appear at sentence start!\n",
      "   Start: 65.06%\n",
      "   Middle: 30.19%\n",
      "   End: 4.75%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Polysemy Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.977557Z",
     "iopub.status.busy": "2025-11-19T12:11:30.977491Z",
     "iopub.status.idle": "2025-11-19T12:11:30.981468Z",
     "shell.execute_reply": "2025-11-19T12:11:30.981128Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:51.992735Z",
     "start_time": "2025-11-25T16:11:51.986196Z"
    }
   },
   "source": [
    "# Analyze polysemy (idioms in both literal & figurative contexts)\n",
    "polysemy_stats = loader.analyze_polysemy()\n",
    "\n",
    "# Show polysemy summary\n",
    "print(\"\\nğŸ“Š Key Finding: All idioms are polysemous!\")\n",
    "print(f\"   Total expressions: {polysemy_stats['total_expressions']}\")\n",
    "print(f\"   Polysemous: {polysemy_stats['polysemous_count']} (100%)\")\n",
    "print(f\"   Only literal: {polysemy_stats['only_literal_count']}\")\n",
    "print(f\"   Only figurative: {polysemy_stats['only_figurative_count']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "POLYSEMY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Polysemy Statistics:\n",
      "  â€¢ Total expressions: 60\n",
      "  â€¢ Polysemous idioms (both literal & figurative): 60 (100.00%)\n",
      "  â€¢ Only literal: 0\n",
      "  â€¢ Only figurative: 0\n",
      "\n",
      "ğŸ“Š Top 10 Most Polysemous Idioms (by balance):\n",
      "  1. ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  2. ××™×‘×“ ××ª ×”×¨××©\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  3. × ×©××¨ ×××—×•×¨\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  4. × ×©×‘×¨ ××‘×¤× ×™×\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  5. × ×©×š ×©×¤×ª×™×™×\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  6. × ×ª×Ÿ ×’×–\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  7. × ×ª×Ÿ ×™×“\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  8. ×¡×’×¨ ×—×©×‘×•×Ÿ\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  9. ×¢×©×” ×¡×œ×˜\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  10. ×¢×©×” ×¡×¦× ×”\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "\n",
      "ğŸ“Š Key Finding: All idioms are polysemous!\n",
      "   Total expressions: 60\n",
      "   Polysemous: 60 (100%)\n",
      "   Only literal: 0\n",
      "   Only figurative: 0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Lexical Statistics (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.982719Z",
     "iopub.status.busy": "2025-11-19T12:11:30.982646Z",
     "iopub.status.idle": "2025-11-19T12:11:31.014136Z",
     "shell.execute_reply": "2025-11-19T12:11:31.013657Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:52.049911Z",
     "start_time": "2025-11-25T16:11:52.004156Z"
    }
   },
   "source": [
    "# Compute comprehensive lexical statistics\n",
    "lexical_stats = loader.analyze_lexical_statistics()\n",
    "\n",
    "# Show lexical summary\n",
    "print(\"\\nğŸ“Š Key Finding: High lexical diversity!\")\n",
    "print(f\"   Vocabulary size: {lexical_stats['vocabulary_size']:,} unique words\")\n",
    "print(f\"   Type-Token Ratio: {lexical_stats['ttr_overall']:.4f}\")\n",
    "print(f\"   Avg unique words per sentence: {lexical_stats['avg_unique_per_sentence']:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEXICAL STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Overall Lexical Statistics:\n",
      "  â€¢ Vocabulary size (unique words): 16,301\n",
      "  â€¢ Total tokens: 83,844\n",
      "  â€¢ Type-Token Ratio (TTR): 0.1944\n",
      "  â€¢ Average unique words per sentence: 16.87\n",
      "  â€¢ Function word ratio: 0.0000 (0.00%)\n",
      "\n",
      "ğŸ“Š Top 20 Most Frequent Words:\n",
      "   1. '',',': 3732 (4.45%)\n",
      "   2. ''.']': 3653 (4.36%)\n",
      "   3. ''××ª',': 2295 (2.74%)\n",
      "   4. ''×œ×',': 1288 (1.54%)\n",
      "   5. ''×¢×œ',':  916 (1.09%)\n",
      "   6. ''×©×œ',':  756 (0.90%)\n",
      "   7. '['×”×•×',':  616 (0.73%)\n",
      "   8. ''â€“',':  558 (0.67%)\n",
      "   9. '['×”×™×',':  525 (0.63%)\n",
      "  10. ''×”×™×”',':  510 (0.61%)\n",
      "  11. ''×”×•×',':  491 (0.59%)\n",
      "  12. ''×”×™×',':  482 (0.57%)\n",
      "  13. '['××',':  445 (0.53%)\n",
      "  14. ''×¢×',':  423 (0.50%)\n",
      "  15. ''××—×¨×™',':  419 (0.50%)\n",
      "  16. ''×›×œ',':  400 (0.48%)\n",
      "  17. ''×›×“×™',':  394 (0.47%)\n",
      "  18. ''××‘×œ',':  328 (0.39%)\n",
      "  19. ''×–×”',':  328 (0.39%)\n",
      "  20. ''×›×™',':  322 (0.38%)\n",
      "\n",
      "ğŸ“Š Top 20 Words in Idioms:\n",
      "   1. '××ª':  965\n",
      "   2. '×”×¨××©':  240\n",
      "   3. '×©×‘×¨':  201\n",
      "   4. '×‘××§×•×':  160\n",
      "   5. '×‘×™×Ÿ':  160\n",
      "   6. '×”×–× ×‘':  159\n",
      "   7. '×™×¨×“':  119\n",
      "   8. '×¤×ª×—':  105\n",
      "   9. '×¢×¦××•':  105\n",
      "  10. '×©×‘×¨×”':   99\n",
      "  11. '×”×¨×™×':   95\n",
      "  12. '×œ×•':   83\n",
      "  13. '×¢×œ':   82\n",
      "  14. '×›××•':   81\n",
      "  15. '×”×¦×¤×•×Ÿ':   80\n",
      "  16. '×‘×¢× × ×™×':   80\n",
      "  17. '×”×˜×™×¤×•×ª':   80\n",
      "  18. '×™×“×™×™×':   80\n",
      "  19. '×¡×›×™×Ÿ':   80\n",
      "  20. '×œ××¡×œ×•×œ':   80\n",
      "\n",
      "ğŸ“Š Lexical Statistics by Label:\n",
      "\n",
      "  0:\n",
      "    â€¢ Vocabulary size: 11,296\n",
      "    â€¢ Total tokens: 36,828\n",
      "    â€¢ TTR: 0.3067\n",
      "    â€¢ Top 5 words: ['××ª', '×œ×', '×”×•×', '×¢×œ', '×”×™×']\n",
      "\n",
      "  1:\n",
      "    â€¢ Vocabulary size: 11,211\n",
      "    â€¢ Total tokens: 38,572\n",
      "    â€¢ TTR: 0.2907\n",
      "    â€¢ Top 5 words: ['××ª', '×œ×', '×”×•×', '×”×™×', '×¢×œ']\n",
      "\n",
      "ğŸ“Š Function Word Frequencies:\n",
      "\n",
      "ğŸ“Š Key Finding: High lexical diversity!\n",
      "   Vocabulary size: 16,301 unique words\n",
      "   Type-Token Ratio: 0.1944\n",
      "   Avg unique words per sentence: 16.87\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PART 2: OPTIONAL/RECOMMENDED ANALYSES\n",
    "\n",
    "### 4.1 Structural Complexity Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.015407Z",
     "iopub.status.busy": "2025-11-19T12:11:31.015333Z",
     "iopub.status.idle": "2025-11-19T12:11:31.177665Z",
     "shell.execute_reply": "2025-11-19T12:11:31.177231Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:52.302864Z",
     "start_time": "2025-11-25T16:11:52.055418Z"
    }
   },
   "source": [
    "# Analyze structural complexity (subclauses, punctuation)\n",
    "complexity_stats = loader.analyze_structural_complexity()\n",
    "\n",
    "# Show complexity summary (using correct key names!)\n",
    "print(\"\\nğŸ“Š Key Finding: Figurative sentences are more complex!\")\n",
    "print(f\"   Mean subclause markers: {complexity_stats['mean_subclause_count']:.2f}\")\n",
    "print(f\"   Sentences with subclauses: {complexity_stats['sentences_with_subclauses_pct']:.2f}%\")\n",
    "print(f\"   Mean punctuation: {complexity_stats['mean_punctuation_count']:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STRUCTURAL COMPLEXITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Overall Structural Complexity:\n",
      "  â€¢ Mean subclause markers per sentence: 0.28\n",
      "  â€¢ Mean subclause ratio: 0.0146\n",
      "  â€¢ Mean punctuation marks per sentence: 1.81\n",
      "  â€¢ Sentences with subclauses: 1177 (24.52%)\n",
      "\n",
      "ğŸ“Š Structural Complexity by Label:\n",
      "\n",
      "  0:\n",
      "    â€¢ Mean subclause markers: 0.25\n",
      "    â€¢ Mean subclause ratio: 0.0122\n",
      "    â€¢ Mean punctuation: 1.75\n",
      "\n",
      "  1:\n",
      "    â€¢ Mean subclause markers: 0.31\n",
      "    â€¢ Mean subclause ratio: 0.0170\n",
      "    â€¢ Mean punctuation: 1.87\n",
      "\n",
      "ğŸ“Š Key Finding: Figurative sentences are more complex!\n",
      "   Mean subclause markers: 0.28\n",
      "   Sentences with subclauses: 24.52%\n",
      "   Mean punctuation: 1.81\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Lexical Richness Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.178971Z",
     "iopub.status.busy": "2025-11-19T12:11:31.178898Z",
     "iopub.status.idle": "2025-11-19T12:11:31.201608Z",
     "shell.execute_reply": "2025-11-19T12:11:31.201237Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:52.343264Z",
     "start_time": "2025-11-25T16:11:52.308599Z"
    }
   },
   "source": [
    "# Analyze lexical richness (hapax legomena, Zipf's law)\n",
    "richness_stats = loader.analyze_lexical_richness()\n",
    "\n",
    "# Show richness summary\n",
    "print(\"\\nğŸ“Š Key Finding: Very high lexical richness!\")\n",
    "print(f\"   Hapax legomena: {richness_stats['hapax_legomena_count']:,} ({richness_stats['hapax_ratio']*100:.2f}%)\")\n",
    "print(f\"   Dis legomena: {richness_stats['dis_legomena_count']:,}\")\n",
    "print(f\"   Maas Index: {richness_stats['maas_index']:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEXICAL RICHNESS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Lexical Richness Statistics:\n",
      "  â€¢ Total tokens: 83,844\n",
      "  â€¢ Unique words: 16,301\n",
      "  â€¢ Type-Token Ratio (TTR): 0.1944\n",
      "  â€¢ Hapax legomena (words appearing once): 9,638 (59.13%)\n",
      "  â€¢ Dis legomena (words appearing twice): 2,534\n",
      "  â€¢ Maas Index: 0.0127\n",
      "\n",
      "ğŸ“Š Lexical Richness by Label:\n",
      "\n",
      "  0:\n",
      "    â€¢ Unique words: 11,296\n",
      "    â€¢ TTR: 0.3067\n",
      "    â€¢ Hapax legomena: 7,578 (67.09%)\n",
      "\n",
      "  1:\n",
      "    â€¢ Unique words: 11,211\n",
      "    â€¢ TTR: 0.2907\n",
      "    â€¢ Hapax legomena: 7,373 (65.77%)\n",
      "\n",
      "ğŸ“Š Key Finding: Very high lexical richness!\n",
      "   Hapax legomena: 9,638 (59.13%)\n",
      "   Dis legomena: 2,534\n",
      "   Maas Index: 0.0127\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Collocational Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.203009Z",
     "iopub.status.busy": "2025-11-19T12:11:31.202928Z",
     "iopub.status.idle": "2025-11-19T12:11:31.271988Z",
     "shell.execute_reply": "2025-11-19T12:11:31.271539Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:52.454762Z",
     "start_time": "2025-11-25T16:11:52.351843Z"
    }
   },
   "source": [
    "# Analyze collocations (context words around idioms)\n",
    "collocation_stats = loader.analyze_collocations()\n",
    "\n",
    "# Show collocation summary\n",
    "print(\"\\nğŸ“Š Key Finding: Rich context around idioms!\")\n",
    "print(f\"   Total context words: {collocation_stats['total_context_words']:,}\")\n",
    "print(f\"   Unique context words: {collocation_stats['unique_context_words']:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLLOCATIONAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Collocational Statistics:\n",
      "  â€¢ Total context words (Â±3 tokens): 23,105\n",
      "  â€¢ Unique context words: 8,296\n",
      "\n",
      "ğŸ“Š Top 20 Context Words (Overall):\n",
      "   1. '×”×•×':  835 (3.61%)\n",
      "   2. '×”×™×':  732 (3.17%)\n",
      "   3. '×œ×':  486 (2.10%)\n",
      "   4. '×”×':  410 (1.77%)\n",
      "   5. '××ª':  355 (1.54%)\n",
      "   6. '×¢×œ':  346 (1.50%)\n",
      "   7. '××—×¨×™':  224 (0.97%)\n",
      "   8. '×¢×':  224 (0.97%)\n",
      "   9. '×›×“×™':  222 (0.96%)\n",
      "  10. '×©×œ':  213 (0.92%)\n",
      "  11. '×”×™×”':  185 (0.80%)\n",
      "  12. '×›×™':  160 (0.69%)\n",
      "  13. '×”×™×œ×“':  136 (0.59%)\n",
      "  14. '××':  116 (0.50%)\n",
      "  15. '×”×™×™×ª×”':  102 (0.44%)\n",
      "  16. '×œ××—×¨':   98 (0.42%)\n",
      "  17. '×›×œ':   98 (0.42%)\n",
      "  18. '××•×œ':   96 (0.42%)\n",
      "  19. '×‘×–××Ÿ':   92 (0.40%)\n",
      "  20. '××œ':   76 (0.33%)\n",
      "\n",
      "ğŸ“Š Top 10 Context Words by Label:\n",
      "\n",
      "  Literal:\n",
      "     1. '×”×•×':  373\n",
      "     2. '×”×™×':  314\n",
      "     3. '×œ×':  205\n",
      "     4. '×¢×œ':  198\n",
      "     5. '×”×':  184\n",
      "     6. '××ª':  154\n",
      "     7. '×›×“×™':  148\n",
      "     8. '×©×œ':  119\n",
      "     9. '×”×™×œ×“':  117\n",
      "    10. '×¢×':  102\n",
      "\n",
      "  Figurative:\n",
      "     1. '×”×•×':  462\n",
      "     2. '×”×™×':  418\n",
      "     3. '×œ×':  281\n",
      "     4. '×”×':  226\n",
      "     5. '××ª':  201\n",
      "     6. '×¢×œ':  148\n",
      "     7. '××—×¨×™':  132\n",
      "     8. '×¢×':  122\n",
      "     9. '×›×™':  107\n",
      "    10. '×©×œ':   94\n",
      "\n",
      "ğŸ“Š Key Finding: Rich context around idioms!\n",
      "   Total context words: 23,105\n",
      "   Unique context words: 8,296\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Annotation Consistency Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.273328Z",
     "iopub.status.busy": "2025-11-19T12:11:31.273241Z",
     "iopub.status.idle": "2025-11-19T12:11:31.346949Z",
     "shell.execute_reply": "2025-11-19T12:11:31.346523Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:52.556236Z",
     "start_time": "2025-11-25T16:11:52.464465Z"
    }
   },
   "source": [
    "# Analyze annotation consistency\n",
    "consistency_stats = loader.analyze_annotation_consistency()\n",
    "\n",
    "# Show consistency summary\n",
    "print(\"\\nğŸ“Š Key Finding: Significant morphological variance!\")\n",
    "print(f\"   Prefix attachments: {consistency_stats['prefix_attachment_count']} ({consistency_stats['prefix_attachment_rate']*100:.2f}%)\")\n",
    "print(f\"   Mean consistency rate: {consistency_stats['mean_consistency_rate']:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANNOTATION CONSISTENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Annotation Consistency:\n",
      "  â€¢ Prefix attachments found: 2172 (45.25%)\n",
      "  â€¢ Mean consistency rate per idiom: 0.3954\n",
      "\n",
      "ğŸ“Š Idioms with Variant Forms (Top 10):\n",
      "  1. ×©× ×¨×’×œ×™×™×: 35 variants (consistency: 0.19)\n",
      "  2. ×©×‘×¨ ××ª ×”×œ×‘: 32 variants (consistency: 0.34)\n",
      "  3. ×¤×ª×— ×“×œ×ª×•×ª: 29 variants (consistency: 0.29)\n",
      "  4. ×¡×’×¨ ×—×©×‘×•×Ÿ: 28 variants (consistency: 0.24)\n",
      "  5. ×”×•×¨×™×“ ×¤×¨×•×¤×™×œ: 23 variants (consistency: 0.31)\n",
      "  6. ×™×¦× ××”×§×•×¤×¡×”: 22 variants (consistency: 0.44)\n",
      "  7. ×—×ª×š ×¤×™× ×”: 18 variants (consistency: 0.31)\n",
      "  8. ×§×™×‘×œ ×¡×˜×™×¨×”: 18 variants (consistency: 0.55)\n",
      "  9. ×©× ×¢×œ×™×• ×¤×¡: 18 variants (consistency: 0.28)\n",
      "  10. ×—×¦×” ×§×• ××“×•×: 17 variants (consistency: 0.39)\n",
      "\n",
      "ğŸ“Š Key Finding: Significant morphological variance!\n",
      "   Prefix attachments: 2172 (45.25%)\n",
      "   Mean consistency rate: 0.3954\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create All Visualizations\n",
    "\n",
    "### 5.1 Standard Visualizations (PART 1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.348441Z",
     "iopub.status.busy": "2025-11-19T12:11:31.348370Z",
     "iopub.status.idle": "2025-11-19T12:11:35.216771Z",
     "shell.execute_reply": "2025-11-19T12:11:35.216295Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:54.838822Z",
     "start_time": "2025-11-25T16:11:52.561004Z"
    }
   },
   "source": [
    "# Create all 11 standard visualizations\n",
    "print(\"Creating standard visualizations...\")\n",
    "loader.create_visualizations()\n",
    "print(\"\\nâœ… Standard visualizations saved to paper/figures/\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating standard visualizations...\n",
      "\n",
      "================================================================================\n",
      "CREATING VISUALIZATIONS (Missions 2.2 & 2.4)\n",
      "================================================================================\n",
      "\n",
      "[1/6] Creating label distribution bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/label_distribution.png\n",
      "\n",
      "[2/6] Creating sentence length distribution histogram...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_length_distribution.png\n",
      "\n",
      "[3/6] Creating idiom length distribution histogram...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/idiom_length_distribution.png\n",
      "\n",
      "[4/6] Creating top 10 idioms bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/top_10_idioms.png\n",
      "\n",
      "[5/6] Creating sentence type distribution pie chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_type_distribution.png\n",
      "\n",
      "[6/6] Creating sentence type by label stacked bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_type_by_label.png\n",
      "\n",
      "[7/11] Creating sentence length boxplot by label...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_length_boxplot_by_label.png\n",
      "\n",
      "[8/11] Creating polysemy heatmap...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/polysemy_heatmap.png\n",
      "\n",
      "[9/11] Creating idiom position histogram...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/idiom_position_histogram.png\n",
      "\n",
      "[10/11] Creating idiom position by label bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/idiom_position_by_label.png\n",
      "\n",
      "[11/11] Creating sentence length violin plot...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_length_violin_by_label.png\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL VISUALIZATIONS CREATED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Location: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures\n",
      "\n",
      "Created files:\n",
      "  1. label_distribution.png\n",
      "  2. sentence_length_distribution.png\n",
      "  3. idiom_length_distribution.png\n",
      "  4. top_10_idioms.png\n",
      "  5. sentence_type_distribution.png\n",
      "  6. sentence_type_by_label.png\n",
      "  7. sentence_length_boxplot_by_label.png (NEW)\n",
      "  8. polysemy_heatmap.png (NEW)\n",
      "  9. idiom_position_histogram.png (NEW)\n",
      "  10. idiom_position_by_label.png (NEW)\n",
      "  11. sentence_length_violin_by_label.png (NEW)\n",
      "\n",
      "âœ… Standard visualizations saved to paper/figures/\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Advanced Visualizations (PART 2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:35.218080Z",
     "iopub.status.busy": "2025-11-19T12:11:35.218000Z",
     "iopub.status.idle": "2025-11-19T12:11:37.040247Z",
     "shell.execute_reply": "2025-11-19T12:11:37.039772Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:28:23.873422Z",
     "start_time": "2025-11-25T16:28:21.777846Z"
    }
   },
   "source": [
    "# Create all 6 advanced visualizations\n",
    "print(\"Creating advanced visualizations...\")\n",
    "loader.create_advanced_visualizations()\n",
    "print(\"\\nâœ… Advanced visualizations saved to paper/figures/\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced visualizations...\n",
      "\n",
      "================================================================================\n",
      "CREATING ADVANCED VISUALIZATIONS (PART 2)\n",
      "================================================================================\n",
      "\n",
      "[1/6] Creating Zipf's law plot...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/zipf_law_plot.png\n",
      "\n",
      "[2/6] Creating structural complexity comparison...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/structural_complexity_by_label.png\n",
      "\n",
      "[3/6] Creating collocation word clouds...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/collocation_word_clouds.png\n",
      "\n",
      "[4/6] Creating vocabulary diversity scatter plot...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/vocabulary_diversity_scatter.png\n",
      "\n",
      "[5/6] Creating hapax legomena comparison...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/hapax_legomena_comparison.png\n",
      "\n",
      "[6/6] Creating context words bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/context_words_bar_chart.png\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL ADVANCED VISUALIZATIONS CREATED!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Location: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures\n",
      "\n",
      "Created advanced visualization files:\n",
      "  1. zipf_law_plot.png\n",
      "  2. structural_complexity_by_label.png\n",
      "  3. collocation_word_clouds.png (if wordcloud library available)\n",
      "  4. vocabulary_diversity_scatter.png\n",
      "  5. hapax_legomena_comparison.png\n",
      "  6. context_words_bar_chart.png\n",
      "\n",
      "âœ… Advanced visualizations saved to paper/figures/\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COMPREHENSIVE ANALYSIS - ALL IN ONE\n",
    "\n",
    "Alternatively, run everything at once:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.041715Z",
     "iopub.status.busy": "2025-11-19T12:11:37.041610Z",
     "iopub.status.idle": "2025-11-19T12:11:37.043295Z",
     "shell.execute_reply": "2025-11-19T12:11:37.042937Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:57.099090Z",
     "start_time": "2025-11-25T16:11:57.097428Z"
    }
   },
   "source": [
    "# Run comprehensive analysis (PART 1 + PART 2, including visualizations)\n",
    "# NOTE: Only run this if you haven't run the analyses above\n",
    "\n",
    "# Uncomment to run:\n",
    "# loader_new = DatasetLoader()\n",
    "# loader_new.load_dataset()\n",
    "# results = loader_new.run_comprehensive_analysis(include_part2=True, create_visualizations=True)\n",
    "# print(\"\\nâœ… Comprehensive analysis complete!\")\n",
    "# print(f\"   Total sentences: {results['statistics']['total_sentences']}\")\n",
    "# print(f\"   Unique idioms: {results['statistics']['unique_expressions']}\")\n",
    "# print(f\"   Vocabulary: {results['lexical']['vocabulary_size']:,} words\")\n",
    "# print(f\"   Hapax legomena: {results['lexical_richness']['hapax_legomena_count']:,}\")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explore Specific Examples\n",
    "\n",
    "### 7.1 Compare Literal vs Figurative for Same Idiom"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.044477Z",
     "iopub.status.busy": "2025-11-19T12:11:37.044411Z",
     "iopub.status.idle": "2025-11-19T12:11:37.047911Z",
     "shell.execute_reply": "2025-11-19T12:11:37.047512Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:28:30.988227Z",
     "start_time": "2025-11-25T16:28:30.981615Z"
    }
   },
   "source": [
    "# Show examples of literal vs figurative for same idiom\n",
    "sample_idiom = \"×©×‘×¨ ××ª ×”×¨××©\"  # \"broke the head\"\n",
    "\n",
    "print(f\"Examples for idiom: '{sample_idiom}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "idiom_samples = loader.df[loader.df['base_pie'] == sample_idiom]\n",
    "literal_examples = idiom_samples[idiom_samples['label'] == 0].head(3)\n",
    "figurative_examples = idiom_samples[idiom_samples['label'] == 1].head(3)\n",
    "\n",
    "print(\"ğŸ”¹ LITERAL Examples:\")\n",
    "for idx, row in literal_examples.iterrows():\n",
    "    print(f\"{row['sentence']}\")\n",
    "\n",
    "print(\"\" + \"=\" * 80)\n",
    "print(\"ğŸ”¸ FIGURATIVE Examples:\")\n",
    "for idx, row in figurative_examples.iterrows():\n",
    "    print(f\"{row['sentence']}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for idiom: '×©×‘×¨ ××ª ×”×¨××©'\n",
      "================================================================================\n",
      "ğŸ”¹ LITERAL Examples:\n",
      "××—×¨×™ ×©× ×¤×œ ×‘××¤×¢×œ ×ª×•×š ×›×“×™ ×¢×‘×•×“×ª×• ×‘××©××¨×ª ×œ×™×œ×” ××¨×•×›×” ×”×•× ×©×‘×¨ ××ª ×”×¨××© ×•× ×–×§×§ ×œ×˜×™×¤×•×œ ××™×™×“×™ ×‘×—×“×¨ × ×™×ª×•×—.\n",
      "×‘×–××Ÿ ×©×”×ª× ×“× ×“ ×”× ×“× ×“×” ×™×¦××” ×××§×•××” ×”×™×œ×“ ×©×‘×¨ ××ª ×”×¨××© ×•×”×”×•×¨×™× ×”×–×•×¢××™× ×“×¨×©×• ××”×¢×™×¨×™×™×” ×œ×‘×“×•×§ ××ª ×‘×˜×™×—×•×ª ×›×œ ×”××ª×§× ×™× ×‘×’×™× ×”.\n",
      "×‘××”×œ×š ×”×¡×§×™ ×‘×”×¨×™ ×”××œ×¤×™×, ×”×’×•×œ×© ×©×‘×¨ ××ª ×”×¨××© ×•× ×œ×§×— ×‘××¡×•×§.\n",
      "================================================================================\n",
      "ğŸ”¸ FIGURATIVE Examples:\n",
      "××—×¨×™ ×©×“×™×‘×¨×ª×™ ×‘××©×š ×©×¢×” ×¢× ××©×ª×™ ×©×‘×¨×ª×™ ××ª ×”×¨××© ×œ×”×‘×™×Ÿ ××” ×”×™× ×‘×¢×¦× ×× ×¡×” ×œ×•××¨ ×œ×™.\n",
      "××—×¨×™ ×©×¢×‘×“×ª×™ ×›×œ ×”×©× ×” ×× ×™ ×¢×“×™×™×Ÿ ×©×•×‘×¨×ª ××ª ×”×¨××© ××™×š ×œ×—×¡×•×š ×›×¡×£ ×œ×—×•×¤×©×” ×”×§×¨×•×‘×”\n",
      "××™×’×•×¨ ×××¨ ×œ×™×•×‘×œ ×‘×™×™××•×© ×©×‘×¨×ª ××ª ×”×¨××© ×¢×œ ×–×” ×™×•×ª×¨ ××“×™ ×–××Ÿ, ×ª× ×•×— ×›×‘×¨!\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Examine Top Context Words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.049189Z",
     "iopub.status.busy": "2025-11-19T12:11:37.049105Z",
     "iopub.status.idle": "2025-11-19T12:11:37.051273Z",
     "shell.execute_reply": "2025-11-19T12:11:37.050854Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:57.117521Z",
     "start_time": "2025-11-25T16:11:57.115172Z"
    }
   },
   "source": [
    "# Show top context words from collocation analysis\n",
    "if 'collocation_stats' in locals():\n",
    "    print(\"Top 15 Context Words Around Idioms:\")\n",
    "    print(\"=\"*50)\n",
    "    for i, (word, count) in enumerate(collocation_stats['top_20_context_overall'][:15], 1):\n",
    "        print(f\"{i:2d}. '{word}': {count:4d} occurrences\")\n",
    "else:\n",
    "    print(\"Run analyze_collocations() first to see context words\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Context Words Around Idioms:\n",
      "==================================================\n",
      " 1. '×”×•×':  835 occurrences\n",
      " 2. '×”×™×':  732 occurrences\n",
      " 3. '×œ×':  486 occurrences\n",
      " 4. '×”×':  410 occurrences\n",
      " 5. '××ª':  355 occurrences\n",
      " 6. '×¢×œ':  346 occurrences\n",
      " 7. '××—×¨×™':  224 occurrences\n",
      " 8. '×¢×':  224 occurrences\n",
      " 9. '×›×“×™':  222 occurrences\n",
      "10. '×©×œ':  213 occurrences\n",
      "11. '×”×™×”':  185 occurrences\n",
      "12. '×›×™':  160 occurrences\n",
      "13. '×”×™×œ×“':  136 occurrences\n",
      "14. '××':  116 occurrences\n",
      "15. '×”×™×™×ª×”':  102 occurrences\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Examine Idioms with Most Variance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.052589Z",
     "iopub.status.busy": "2025-11-19T12:11:37.052510Z",
     "iopub.status.idle": "2025-11-19T12:11:37.060403Z",
     "shell.execute_reply": "2025-11-19T12:11:37.059953Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:57.137076Z",
     "start_time": "2025-11-25T16:11:57.125928Z"
    }
   },
   "source": [
    "# Show idioms with most morphological variance\n",
    "print(\"Idioms with Most Variant Forms:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count unique matched_expression per idiom\n",
    "variance_df = loader.df.groupby('base_pie')['pie_span'].nunique().sort_values(ascending=False)\n",
    "\n",
    "for i, (idiom, variants) in enumerate(variance_df.head(10).items(), 1):\n",
    "    print(f\"{i:2d}. {idiom:40s}: {variants:2d} variants\")\n",
    "    \n",
    "    # Show sample variants\n",
    "    sample_variants = loader.df[loader.df['base_pie'] == idiom]['pie_span'].value_counts().head(3)\n",
    "    for variant, count in sample_variants.items():\n",
    "        print(f\"    - {variant}: {count} times\")\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idioms with Most Variant Forms:\n",
      "==================================================\n",
      " 1. ×©× ×¨×’×œ×™×™×                               : 35 variants\n",
      "    - ×©× ×¨×’×œ×™×™×: 15 times\n",
      "    - ×©××” ×¨×’×œ×™×™×: 14 times\n",
      "    - ×©××• ×œ×• ×¨×’×œ×™×™×: 4 times\n",
      "\n",
      " 2. ×©×‘×¨ ××ª ×”×œ×‘                              : 32 variants\n",
      "    - ×©×‘×¨ ××ª ×”×œ×‘: 27 times\n",
      "    - ×©×‘×¨×” ××ª ×”×œ×‘: 13 times\n",
      "    - ×©×‘×¨×• ××ª ×”×œ×‘: 3 times\n",
      "\n",
      " 3. ×¤×ª×— ×“×œ×ª×•×ª                               : 29 variants\n",
      "    - ×¤×ª×— ×“×œ×ª×•×ª: 23 times\n",
      "    - ×¤×ª×—×” ×“×œ×ª×•×ª: 12 times\n",
      "    - ×¤×ª×— ×œ×• ×“×œ×ª×•×ª: 5 times\n",
      "\n",
      " 4. ×¡×’×¨ ×—×©×‘×•×Ÿ                               : 28 variants\n",
      "    - ×¡×’×¨ ×—×©×‘×•×Ÿ: 19 times\n",
      "    - ×œ×¡×’×•×¨ ×—×©×‘×•×Ÿ: 11 times\n",
      "    - ×¡×’×¨×” ×—×©×‘×•×Ÿ: 9 times\n",
      "\n",
      " 5. ×”×•×¨×™×“ ×¤×¨×•×¤×™×œ                            : 23 variants\n",
      "    - ×”×•×¨×™×“ ×¤×¨×•×¤×™×œ: 25 times\n",
      "    - ×”×•×¨×™×“×” ×¤×¨×•×¤×™×œ: 14 times\n",
      "    - ×œ×”×•×¨×“×ª ×¤×¨×•×¤×™×œ: 6 times\n",
      "\n",
      " 6. ×™×¦× ××”×§×•×¤×¡×”                             : 22 variants\n",
      "    - ×™×¦× ××”×§×•×¤×¡×”: 35 times\n",
      "    - ×™×¦××” ××”×§×•×¤×¡×”: 10 times\n",
      "    - ×™×¦××• ××”×§×•×¤×¡×”: 5 times\n",
      "\n",
      " 7. ×§×™×‘×œ ×¡×˜×™×¨×”                              : 18 variants\n",
      "    - ×§×™×‘×œ ×¡×˜×™×¨×”: 44 times\n",
      "    - ×§×™×‘×œ×” ×¡×˜×™×¨×”: 14 times\n",
      "    - ×§×™×‘×œ×• ×¡×˜×™×¨×”: 4 times\n",
      "\n",
      " 8. ×©× ×¢×œ×™×• ×¤×¡                              : 18 variants\n",
      "    - ×©× ×¤×¡: 22 times\n",
      "    - ×©× ×¢×œ×™×• ×¤×¡: 17 times\n",
      "    - ×©××• ×¢×œ×™×• ×¤×¡: 11 times\n",
      "\n",
      " 9. ×—×ª×š ×¤×™× ×”                                : 18 variants\n",
      "    - ×—×ª×š ×¤×™× ×”: 25 times\n",
      "    - ×—×ª×›×” ×¤×™× ×”: 12 times\n",
      "    - ×œ×—×ª×•×š ×¤×™× ×”: 9 times\n",
      "\n",
      "10. ×™×¨×“ ×œ×• ×”××¡×™××•×Ÿ                          : 17 variants\n",
      "    - ×™×¨×“ ×œ×• ×”××¡×™××•×Ÿ: 35 times\n",
      "    - ×™×¨×“ ×œ×” ×”××¡×™××•×Ÿ: 9 times\n",
      "    - ×™×¨×“ ×œ×™ ×”××¡×™××•×Ÿ: 8 times\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.061681Z",
     "iopub.status.busy": "2025-11-19T12:11:37.061615Z",
     "iopub.status.idle": "2025-11-19T12:11:37.065198Z",
     "shell.execute_reply": "2025-11-19T12:11:37.064762Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:57.147813Z",
     "start_time": "2025-11-25T16:11:57.141839Z"
    }
   },
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Sentences',\n",
    "        'Unique Idioms',\n",
    "        'Avg Sentence Length (tokens)',\n",
    "        'Avg Idiom Length (tokens)',\n",
    "        'Vocabulary Size',\n",
    "        'Type-Token Ratio',\n",
    "        'Hapax Legomena',\n",
    "        'Polysemous Idioms',\n",
    "        'Idioms at Start (%)',\n",
    "        'Sentences with Subclauses (%)',\n",
    "        'Mean Prefix Attachment Rate (%)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{len(loader.df):,}\",\n",
    "        f\"{loader.df['base_pie'].nunique()}\",\n",
    "        f\"{loader.df['num_tokens'].mean():.2f}\",\n",
    "        f\"{(loader.df['end_token'] - loader.df['start_token']).mean():.2f}\",\n",
    "        f\"{lexical_stats['vocabulary_size']:,}\" if 'lexical_stats' in locals() else \"Run lexical analysis\",\n",
    "        f\"{lexical_stats['ttr_overall']:.4f}\" if 'lexical_stats' in locals() else \"Run lexical analysis\",\n",
    "        f\"{richness_stats['hapax_legomena_count']:,} ({richness_stats['hapax_ratio']*100:.2f}%)\" if 'richness_stats' in locals() else \"Run richness analysis\",\n",
    "        f\"{polysemy_stats['polysemous_count']} (100%)\" if 'polysemy_stats' in locals() else \"Run polysemy analysis\",\n",
    "        f\"{position_stats['position_percentages']['start']:.2f}%\" if 'position_stats' in locals() else \"Run position analysis\",\n",
    "        f\"{complexity_stats['sentences_with_subclauses_pct']:.2f}%\" if 'complexity_stats' in locals() else \"Run complexity analysis\",\n",
    "        f\"{consistency_stats['prefix_attachment_rate']*100:.2f}%\" if 'consistency_stats' in locals() else \"Run consistency analysis\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE DATASET SUMMARY\n",
      "================================================================================\n",
      "                         Metric          Value\n",
      "                Total Sentences          4,800\n",
      "                  Unique Idioms             60\n",
      "   Avg Sentence Length (tokens)          17.47\n",
      "      Avg Idiom Length (tokens)           2.48\n",
      "                Vocabulary Size         16,301\n",
      "               Type-Token Ratio         0.1944\n",
      "                 Hapax Legomena 9,638 (59.13%)\n",
      "              Polysemous Idioms      60 (100%)\n",
      "            Idioms at Start (%)         65.06%\n",
      "  Sentences with Subclauses (%)         24.52%\n",
      "Mean Prefix Attachment Rate (%)         45.25%\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validation Checklist"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.066576Z",
     "iopub.status.busy": "2025-11-19T12:11:37.066470Z",
     "iopub.status.idle": "2025-11-19T12:11:37.075909Z",
     "shell.execute_reply": "2025-11-19T12:11:37.075546Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-25T16:28:44.117447Z",
     "start_time": "2025-11-25T16:28:44.093898Z"
    }
   },
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE VALIDATION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checklist = [\n",
    "    (\"Dataset loads successfully\", len(loader.df) > 0),\n",
    "    (\"4,800 total sentences\", len(loader.df) == 4800),\n",
    "    (\"Perfect label balance (50/50)\", abs(loader.df['label'].value_counts()[0] - loader.df['label'].value_counts()[1]) == 0),\n",
    "    (\"60 unique idioms\", loader.df['base_pie'].nunique() == 60),\n",
    "    (\"No duplicate rows\", loader.df.duplicated().sum() == 0),\n",
    "    (\"All idioms are polysemous\", True if 'polysemy_stats' in locals() and polysemy_stats['polysemous_count'] == 60 else False),\n",
    "    (\"High lexical diversity (>60% hapax)\", True if 'richness_stats' in locals() and richness_stats['hapax_ratio'] > 0.6 else False),\n",
    "    (\"Most idioms at sentence start\", True if 'position_stats' in locals() and position_stats['position_percentages']['start'] > 80 else False),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for criterion, passed in checklist:\n",
    "    status = \"âœ…\" if passed else \"âŒ\"\n",
    "    print(f\"{status} {criterion}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ‰ ALL VALIDATION CRITERIA PASSED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"âœ… Dataset is fully analyzed and validated\")\n",
    "    print(\"âœ… Ready for Mission 2.5: Dataset Splitting\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some criteria not met. Run all analyses above.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE VALIDATION CHECKLIST\n",
      "================================================================================\n",
      "âœ… Dataset loads successfully\n",
      "âœ… 4,800 total sentences\n",
      "âœ… Perfect label balance (50/50)\n",
      "âœ… 60 unique idioms\n",
      "âœ… No duplicate rows\n",
      "âœ… All idioms are polysemous\n",
      "âŒ High lexical diversity (>60% hapax)\n",
      "âŒ Most idioms at sentence start\n",
      "\n",
      "âš ï¸ Some criteria not met. Run all analyses above.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Comprehensive Analysis\n",
    "\n",
    "**Key Findings:**\n",
    "1. **Dataset Quality**: Perfectly balanced, no duplicates, 4,800 sentences\n",
    "2. **Polysemy**: All 60 idioms are polysemous (50/50 literal/figurative)\n",
    "3. **Position**: 87.96% of idioms appear at sentence start\n",
    "4. **Lexical Diversity**: Very high (63.43% hapax legomena)\n",
    "5. **Complexity**: Figurative sentences are more complex\n",
    "6. **Morphology**: Significant variance (41.40% prefix attachments)\n",
    "\n",
    "**Output Files:**\n",
    "- Statistics: `experiments/results/dataset_statistics_full.txt`\n",
    "- Visualizations: `paper/figures/` (17 total)\n",
    "\n",
    "**Next Steps:**\n",
    "- Mission 2.5: Dataset Splitting (Expression-Based Strategy)\n",
    "- Mission 3: Model Training and Evaluation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T16:11:57.186738Z",
     "start_time": "2025-11-25T16:11:57.185068Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
