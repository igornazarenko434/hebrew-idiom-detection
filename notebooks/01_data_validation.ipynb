{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission 2: Comprehensive Dataset Analysis and Validation\n",
    "# Hebrew Idiom Detection Project - Enhanced with PART 1 + PART 2 Analyses\n",
    "\n",
    "**Purpose:** Comprehensive exploration, validation, and statistical analysis of the Hebrew idiom dataset\n",
    "\n",
    "**Date:** November 10, 2025\n",
    "\n",
    "**Analysis Coverage:**\n",
    "- PART 1: Required analyses (basic statistics, polysemy, position, lexical)\n",
    "- PART 2: Optional analyses (structural complexity, lexical richness, collocations, consistency)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:38:37.672209Z",
     "start_time": "2025-11-10T13:38:37.667287Z"
    }
   },
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import our data preparation module\n",
    "from src.data_preparation import DatasetLoader\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "print(f\"ğŸ“ Project root: {project_root}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n",
      "ğŸ“ Project root: /Users/igornazarenko/PycharmProjects/Final_Project_NLP\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:38:40.743411Z",
     "start_time": "2025-11-10T13:38:40.709021Z"
    }
   },
   "source": [
    "# Initialize loader and load dataset\n",
    "loader = DatasetLoader()\n",
    "loader.load_dataset()  # IMPORTANT: Must explicitly load!\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded: {len(loader.df)} rows, {len(loader.df.columns)} columns\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/data/expressions_data_tagged.csv\n",
      "âœ… Dataset loaded successfully!\n",
      "Total rows: 4800\n",
      "\n",
      "âœ… Dataset loaded: 4800 rows, 16 columns\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:38:42.184869Z",
     "start_time": "2025-11-10T13:38:42.171677Z"
    }
   },
   "source": [
    "# Display basic info\n",
    "print(\"Dataset Shape:\", loader.df.shape)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(loader.df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "loader.df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (4800, 16)\n",
      "\n",
      "Column Names:\n",
      "['id', 'split', 'language', 'source', 'text', 'expression', 'matched_expression', 'span_start', 'span_end', 'token_span_start', 'token_span_end', 'num_tokens', 'label', 'label_2', 'iob2_tags', 'char_mask']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   id        split language   source  \\\n",
       "0   0  unspecified       he  inhouse   \n",
       "1   1  unspecified       he  inhouse   \n",
       "2   2  unspecified       he  inhouse   \n",
       "3   3  unspecified       he  inhouse   \n",
       "4   4  unspecified       he  inhouse   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  ×× ×©×¨ ×”××•×¦×¨ ×œ× ×™××¦× ×¤×ª×¨×•×Ÿ ×œ××©×‘×¨ ×”×—××•×¨ ×©× ×•×¦×¨ ×‘×¢×§×‘×•×ª ×”××œ×—××” ×”××¨×•×›×” ×‘×™×•×ª×¨ ×¢×“ ×›×”, ×”×•× ×¢×“×™×™×Ÿ ×™××©×™×š ×œ×©...   \n",
       "1  ×›××” ×–××Ÿ ×©×‘×¨×ª ××ª ×”×¨××© ×¢×œ × ×™×¡×•×— ×”××™×™×œ ×¢× ×”×¨×¢×™×•×Ÿ ×”××”×¤×›× ×™ ×”×–×” ×©×œ×š ×œ×“×™×¨×§×˜×•×¨×™×•×Ÿ, ×•×”×× ×‘×¡×•×£ ×©×œ×—×ª ××•×ª×• ×›...   \n",
       "2  ×”×™× ××¢×•×œ× ×œ× ×©×‘×¨×” ××ª ×”×¨××© ×›×œ ×›×š ×”×¨×‘×” ×–××Ÿ ×›×“×™ ×œ××¦×•× ×¨×¢×™×•×Ÿ ×™×¦×™×¨×ª×™ ×œ××ª× ×ª ×™×•× ×”×•×œ×“×ª ××¤×ª×™×¢×” ×›×–××ª, ×•×‘×¡...   \n",
       "3  ×”×¦×•×•×ª ×›×•×œ×• ×©×‘×¨ ××ª ×”×¨××© ×¢×œ ××™×š ×œ×¢××•×“ ×‘×“×“×œ×™×™×Ÿ, ×ª×•×š ×›×“×™ ×©× ××œ×¥ ×œ×”×ª××•×“×“ ×¢× ×‘×¢×™×•×ª ×˜×›× ×™×•×ª ×‘×œ×ª×™ ×¦×¤×•×™×•×ª, ...   \n",
       "4                                                        ×©×‘×¨×ª×™ ××ª ×”×¨××© ×œ×”×‘×™×Ÿ ××” ×”×™× ×‘×¢×¦× ×× ×¡×” ×œ×•××¨ ×œ×™.   \n",
       "\n",
       "    expression matched_expression  span_start  span_end  token_span_start  \\\n",
       "0  ×©×‘×¨ ××ª ×”×¨××©      ×œ×©×‘×•×¨ ××ª ×”×¨××©          94       107                18   \n",
       "1  ×©×‘×¨ ××ª ×”×¨××©       ×©×‘×¨×ª ××ª ×”×¨××©           8        20                 2   \n",
       "2  ×©×‘×¨ ××ª ×”×¨××©       ×©×‘×¨×” ××ª ×”×¨××©          13        25                 3   \n",
       "3  ×©×‘×¨ ××ª ×”×¨××©        ×©×‘×¨ ××ª ×”×¨××©          11        22                 2   \n",
       "4  ×©×‘×¨ ××ª ×”×¨××©      ×©×‘×¨×ª×™ ××ª ×”×¨××©           0        13                 0   \n",
       "\n",
       "   token_span_end  num_tokens      label  label_2  \\\n",
       "0              21          28  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "1               5          20  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "2               6          31  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "3               5          25  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "4               3          10  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "\n",
       "                                                                         iob2_tags  \\\n",
       "0        O O O O O O O O O O O O O O O O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O   \n",
       "1                        O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O   \n",
       "2  O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O O O O O O O O O O O   \n",
       "3              O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O O O O O O   \n",
       "4                                            B-IDIOM I-IDIOM I-IDIOM O O O O O O O   \n",
       "\n",
       "                                                                                             char_mask  \n",
       "0  000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000011...  \n",
       "1  000000001111111111110000000000000000000000000000000000000000000000000000000000000000000000000000...  \n",
       "2  000000000000011111111111100000000000000000000000000000000000000000000000000000000000000000000000...  \n",
       "3  000000000001111111111100000000000000000000000000000000000000000000000000000000000000000000000000...  \n",
       "4                                                        111111111111100000000000000000000000000000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>expression</th>\n",
       "      <th>matched_expression</th>\n",
       "      <th>span_start</th>\n",
       "      <th>span_end</th>\n",
       "      <th>token_span_start</th>\n",
       "      <th>token_span_end</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>label_2</th>\n",
       "      <th>iob2_tags</th>\n",
       "      <th>char_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>×× ×©×¨ ×”××•×¦×¨ ×œ× ×™××¦× ×¤×ª×¨×•×Ÿ ×œ××©×‘×¨ ×”×—××•×¨ ×©× ×•×¦×¨ ×‘×¢×§×‘×•×ª ×”××œ×—××” ×”××¨×•×›×” ×‘×™×•×ª×¨ ×¢×“ ×›×”, ×”×•× ×¢×“×™×™×Ÿ ×™××©×™×š ×œ×©...</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×œ×©×‘×•×¨ ××ª ×”×¨××©</td>\n",
       "      <td>94</td>\n",
       "      <td>107</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O</td>\n",
       "      <td>000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>×›××” ×–××Ÿ ×©×‘×¨×ª ××ª ×”×¨××© ×¢×œ × ×™×¡×•×— ×”××™×™×œ ×¢× ×”×¨×¢×™×•×Ÿ ×”××”×¤×›× ×™ ×”×–×” ×©×œ×š ×œ×“×™×¨×§×˜×•×¨×™×•×Ÿ, ×•×”×× ×‘×¡×•×£ ×©×œ×—×ª ××•×ª×• ×›...</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×©×‘×¨×ª ××ª ×”×¨××©</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O</td>\n",
       "      <td>000000001111111111110000000000000000000000000000000000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>×”×™× ××¢×•×œ× ×œ× ×©×‘×¨×” ××ª ×”×¨××© ×›×œ ×›×š ×”×¨×‘×” ×–××Ÿ ×›×“×™ ×œ××¦×•× ×¨×¢×™×•×Ÿ ×™×¦×™×¨×ª×™ ×œ××ª× ×ª ×™×•× ×”×•×œ×“×ª ××¤×ª×™×¢×” ×›×–××ª, ×•×‘×¡...</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×©×‘×¨×” ××ª ×”×¨××©</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "      <td>000000000000011111111111100000000000000000000000000000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>×”×¦×•×•×ª ×›×•×œ×• ×©×‘×¨ ××ª ×”×¨××© ×¢×œ ××™×š ×œ×¢××•×“ ×‘×“×“×œ×™×™×Ÿ, ×ª×•×š ×›×“×™ ×©× ××œ×¥ ×œ×”×ª××•×“×“ ×¢× ×‘×¢×™×•×ª ×˜×›× ×™×•×ª ×‘×œ×ª×™ ×¦×¤×•×™×•×ª, ...</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O O O O O O</td>\n",
       "      <td>000000000001111111111100000000000000000000000000000000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>×©×‘×¨×ª×™ ××ª ×”×¨××© ×œ×”×‘×™×Ÿ ××” ×”×™× ×‘×¢×¦× ×× ×¡×” ×œ×•××¨ ×œ×™.</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×©×‘×¨×ª×™ ××ª ×”×¨××©</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>B-IDIOM I-IDIOM I-IDIOM O O O O O O O</td>\n",
       "      <td>111111111111100000000000000000000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PART 1: REQUIRED ANALYSES\n",
    "\n",
    "### 3.1 Basic Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:38:46.162039Z",
     "start_time": "2025-11-10T13:38:46.150193Z"
    }
   },
   "source": [
    "# Run basic statistics (includes expression occurrences, char lengths, etc.)\n",
    "basic_stats = loader.generate_statistics()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Total Sentences: 4800\n",
      "\n",
      "ğŸ“Š Label Distribution:\n",
      "  â€¢ ×¤×™×’×•×¨×˜×™×‘×™      :  2400 (50.00%)\n",
      "  â€¢ ××™×œ×•×œ×™         :  2400 (50.00%)\n",
      "\n",
      "ğŸ“Š Unique Idioms/Expressions: 60\n",
      "\n",
      "ğŸ“Š Expression Occurrence Statistics:\n",
      "  â€¢ Min occurrences per idiom: 80\n",
      "  â€¢ Max occurrences per idiom: 80\n",
      "  â€¢ Mean occurrences per idiom: 80.00\n",
      "  â€¢ Median occurrences per idiom: 80.00\n",
      "  â€¢ Std occurrences per idiom: 0.00\n",
      "\n",
      "ğŸ“Š Sentence Length Statistics (tokens):\n",
      "  â€¢ Average: 14.95 tokens\n",
      "  â€¢ Median:  10 tokens\n",
      "  â€¢ Std:     8.22 tokens\n",
      "  â€¢ Min:     5 tokens\n",
      "  â€¢ Max:     37 tokens\n",
      "\n",
      "ğŸ“Š Sentence Length Statistics (characters):\n",
      "  â€¢ Average: 78.83 chars\n",
      "  â€¢ Median:  54.00 chars\n",
      "  â€¢ Std:     43.69 chars\n",
      "  â€¢ Min:     22 chars\n",
      "  â€¢ Max:     193 chars\n",
      "\n",
      "ğŸ“Š Idiom Length Statistics (tokens):\n",
      "  â€¢ Average: 2.48 tokens\n",
      "  â€¢ Median:  2 tokens\n",
      "  â€¢ Std:     0.64 tokens\n",
      "  â€¢ Min:     2 tokens\n",
      "  â€¢ Max:     5 tokens\n",
      "\n",
      "ğŸ“Š Idiom Length Statistics (characters):\n",
      "  â€¢ Average: 11.37 chars\n",
      "  â€¢ Median:  11.00 chars\n",
      "  â€¢ Std:     3.14 chars\n",
      "  â€¢ Min:     5 chars\n",
      "  â€¢ Max:     22 chars\n",
      "\n",
      "ğŸ“Š Top 10 Most Frequent Expressions:\n",
      "   1. ×©×‘×¨ ××ª ×”×¨××©                              :  80 occurrences\n",
      "   2. ×©×™×—×§ ×‘××©                                 :  80 occurrences\n",
      "   3. ××™×‘×“ ××ª ×”×¨××©                             :  80 occurrences\n",
      "   4. ×œ×‘ ×–×”×‘                                   :  80 occurrences\n",
      "   5. ×—×•×ª×š ×›××• ×¡×›×™×Ÿ                            :  80 occurrences\n",
      "   6. ×”×™×™×ª×” ×‘×¢× × ×™×                             :  80 occurrences\n",
      "   7. ×—×˜×£ ×—×•×                                  :  80 occurrences\n",
      "   8. ×©×¤×›×” ××•×¨                                 :  80 occurrences\n",
      "   9. ×©×‘×¨ ×©×ª×™×§×”                                :  80 occurrences\n",
      "  10. ×¨×¥ ××—×¨×™ ×”×–× ×‘ ×©×œ ×¢×¦××•                     :  80 occurrences\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sentence Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:38:58.339985Z",
     "start_time": "2025-11-10T13:38:58.316209Z"
    }
   },
   "source": [
    "# Analyze sentence types (declarative, question, exclamatory)\n",
    "sentence_types = loader.analyze_sentence_types()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SENTENCE TYPE ANALYSIS (Mission 2.4)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Sentence Type Distribution:\n",
      "  â€¢ Declarative    :  4425 (92.19%)\n",
      "  â€¢ Question       :   341 ( 7.10%)\n",
      "  â€¢ Exclamatory    :    34 ( 0.71%)\n",
      "\n",
      "ğŸ“Š Sentence Type by Label (Literal vs Figurative):\n",
      "label          ××™×œ×•×œ×™  ×¤×™×’×•×¨×˜×™×‘×™   All\n",
      "sentence_type                         \n",
      "Declarative      2225       2200  4425\n",
      "Exclamatory        22         12    34\n",
      "Question          153        188   341\n",
      "All              2400       2400  4800\n",
      "\n",
      "ğŸ“Š Percentage Distribution within each Label:\n",
      "label          ××™×œ×•×œ×™  ×¤×™×’×•×¨×˜×™×‘×™\n",
      "sentence_type                   \n",
      "Declarative     92.71      91.67\n",
      "Exclamatory      0.92       0.50\n",
      "Question         6.38       7.83\n",
      "\n",
      "ğŸ“Š Balance Check (are sentence types distributed evenly across labels?):\n",
      "  â€¢ Declarative    : Literal=50.3%, Figurative=49.7%\n",
      "  â€¢ Question       : Literal=44.9%, Figurative=55.1%\n",
      "  â€¢ Exclamatory    : Literal=64.7%, Figurative=35.3%\n",
      "\n",
      "ğŸ“Š Sentence Types by Top Expressions:\n",
      "\n",
      "  ×©×‘×¨ ××ª ×”×¨××©:\n",
      "    - Declarative: 73 (91.2%)\n",
      "    - Question: 6 (7.5%)\n",
      "    - Exclamatory: 1 (1.2%)\n",
      "\n",
      "  ×©×™×—×§ ×‘××©:\n",
      "    - Declarative: 78 (97.5%)\n",
      "    - Question: 1 (1.2%)\n",
      "    - Exclamatory: 1 (1.2%)\n",
      "\n",
      "  ××™×‘×“ ××ª ×”×¨××©:\n",
      "    - Declarative: 71 (88.8%)\n",
      "    - Question: 9 (11.2%)\n",
      "\n",
      "  ×œ×‘ ×–×”×‘:\n",
      "    - Declarative: 70 (87.5%)\n",
      "    - Question: 10 (12.5%)\n",
      "\n",
      "  ×—×•×ª×š ×›××• ×¡×›×™×Ÿ:\n",
      "    - Declarative: 77 (96.2%)\n",
      "    - Question: 3 (3.8%)\n",
      "\n",
      "  ×”×™×™×ª×” ×‘×¢× × ×™×:\n",
      "    - Declarative: 75 (93.8%)\n",
      "    - Question: 4 (5.0%)\n",
      "    - Exclamatory: 1 (1.2%)\n",
      "\n",
      "  ×—×˜×£ ×—×•×:\n",
      "    - Declarative: 76 (95.0%)\n",
      "    - Question: 4 (5.0%)\n",
      "\n",
      "  ×©×¤×›×” ××•×¨:\n",
      "    - Declarative: 75 (93.8%)\n",
      "    - Question: 4 (5.0%)\n",
      "    - Exclamatory: 1 (1.2%)\n",
      "\n",
      "  ×©×‘×¨ ×©×ª×™×§×”:\n",
      "    - Declarative: 74 (92.5%)\n",
      "    - Question: 6 (7.5%)\n",
      "\n",
      "  ×¨×¥ ××—×¨×™ ×”×–× ×‘ ×©×œ ×¢×¦××•:\n",
      "    - Declarative: 75 (93.8%)\n",
      "    - Question: 4 (5.0%)\n",
      "    - Exclamatory: 1 (1.2%)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Idiom Position Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:39:04.959490Z",
     "start_time": "2025-11-10T13:39:04.947764Z"
    }
   },
   "source": [
    "# Analyze where idioms appear in sentences\n",
    "position_stats = loader.analyze_idiom_position()\n",
    "\n",
    "# Show position distribution\n",
    "print(\"\\nğŸ“Š Key Finding: Most idioms appear at sentence start!\")\n",
    "print(f\"   Start: {position_stats['position_percentages']['start']:.2f}%\")\n",
    "print(f\"   Middle: {position_stats['position_percentages']['middle']:.2f}%\")\n",
    "print(f\"   End: {position_stats['position_percentages']['end']:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IDIOM POSITION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Position Ratio Statistics:\n",
      "  â€¢ Mean: 0.1670\n",
      "  â€¢ Median: 0.1250\n",
      "  â€¢ Std: 0.1445\n",
      "  â€¢ Range: [0.0000, 0.9231]\n",
      "\n",
      "ğŸ“Š Position Distribution:\n",
      "  â€¢ Start (0-33%): 4179 (87.06%)\n",
      "  â€¢ Middle (33-67%): 553 (11.52%)\n",
      "  â€¢ End (67-100%): 68 (1.42%)\n",
      "\n",
      "ğŸ“Š Position Distribution by Label:\n",
      "\n",
      "  ×¤×™×’×•×¨×˜×™×‘×™:\n",
      "    â€¢ Start: 2110 (87.92%)\n",
      "    â€¢ Middle: 251 (10.46%)\n",
      "    â€¢ End: 39 (1.62%)\n",
      "    â€¢ Mean position ratio: 0.1594\n",
      "\n",
      "  ××™×œ×•×œ×™:\n",
      "    â€¢ Start: 2069 (86.21%)\n",
      "    â€¢ Middle: 302 (12.58%)\n",
      "    â€¢ End: 29 (1.21%)\n",
      "    â€¢ Mean position ratio: 0.1745\n",
      "\n",
      "ğŸ“Š Key Finding: Most idioms appear at sentence start!\n",
      "   Start: 87.06%\n",
      "   Middle: 11.52%\n",
      "   End: 1.42%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Polysemy Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:39:07.523600Z",
     "start_time": "2025-11-10T13:39:07.512044Z"
    }
   },
   "source": [
    "# Analyze polysemy (idioms in both literal & figurative contexts)\n",
    "polysemy_stats = loader.analyze_polysemy()\n",
    "\n",
    "# Show polysemy summary\n",
    "print(\"\\nğŸ“Š Key Finding: All idioms are polysemous!\")\n",
    "print(f\"   Total expressions: {polysemy_stats['total_expressions']}\")\n",
    "print(f\"   Polysemous: {polysemy_stats['polysemous_count']} (100%)\")\n",
    "print(f\"   Only literal: {polysemy_stats['only_literal_count']}\")\n",
    "print(f\"   Only figurative: {polysemy_stats['only_figurative_count']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "POLYSEMY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Polysemy Statistics:\n",
      "  â€¢ Total expressions: 60\n",
      "  â€¢ Polysemous idioms (both literal & figurative): 60 (100.00%)\n",
      "  â€¢ Only literal: 0\n",
      "  â€¢ Only figurative: 0\n",
      "\n",
      "ğŸ“Š Top 10 Most Polysemous Idioms (by balance):\n",
      "  1. ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  2. ××™×‘×“ ××ª ×”×¨××©\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  3. × ×©××¨ ×××—×•×¨\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  4. × ×©×‘×¨ ××‘×¤× ×™×\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  5. × ×©×š ×©×¤×ª×™×™×\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  6. × ×ª×Ÿ ×’×–\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  7. × ×ª×Ÿ ×™×“\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  8. ×¡×’×¨ ×—×©×‘×•×Ÿ\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  9. ×¢×©×” ×¡×œ×˜\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  10. ×¢×©×” ×¡×¦× ×”\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "\n",
      "ğŸ“Š Key Finding: All idioms are polysemous!\n",
      "   Total expressions: 60\n",
      "   Polysemous: 60 (100%)\n",
      "   Only literal: 0\n",
      "   Only figurative: 0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Lexical Statistics (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:39:11.582327Z",
     "start_time": "2025-11-10T13:39:11.528080Z"
    }
   },
   "source": [
    "# Compute comprehensive lexical statistics\n",
    "lexical_stats = loader.analyze_lexical_statistics()\n",
    "\n",
    "# Show lexical summary\n",
    "print(\"\\nğŸ“Š Key Finding: High lexical diversity!\")\n",
    "print(f\"   Vocabulary size: {lexical_stats['vocabulary_size']:,} unique words\")\n",
    "print(f\"   Type-Token Ratio: {lexical_stats['ttr_overall']:.4f}\")\n",
    "print(f\"   Avg unique words per sentence: {lexical_stats['avg_unique_per_sentence']:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEXICAL STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Overall Lexical Statistics:\n",
      "  â€¢ Vocabulary size (unique words): 17,787\n",
      "  â€¢ Total tokens: 71,775\n",
      "  â€¢ Type-Token Ratio (TTR): 0.2478\n",
      "  â€¢ Average unique words per sentence: 14.64\n",
      "  â€¢ Function word ratio: 0.1294 (12.94%)\n",
      "\n",
      "ğŸ“Š Top 20 Most Frequent Words:\n",
      "   1. '××ª': 2231 (3.11%)\n",
      "   2. '×œ×': 1241 (1.73%)\n",
      "   3. '×”×•×': 1134 (1.58%)\n",
      "   4. '×”×™×': 1049 (1.46%)\n",
      "   5. '×¢×œ':  886 (1.23%)\n",
      "   6. '×©×œ':  722 (1.01%)\n",
      "   7. 'â€“':  649 (0.90%)\n",
      "   8. '××':  624 (0.87%)\n",
      "   9. '×”×':  565 (0.79%)\n",
      "  10. '×”×™×”':  497 (0.69%)\n",
      "  11. '××—×¨×™':  491 (0.68%)\n",
      "  12. '×›×“×™':  423 (0.59%)\n",
      "  13. '×¢×':  405 (0.56%)\n",
      "  14. '×›×œ':  377 (0.53%)\n",
      "  15. '×›×™':  360 (0.50%)\n",
      "  16. '××‘×œ':  331 (0.46%)\n",
      "  17. '×”×™×™×ª×”':  288 (0.40%)\n",
      "  18. '×¨×§':  256 (0.36%)\n",
      "  19. '×–×”':  255 (0.36%)\n",
      "  20. '×‘×™×Ÿ':  251 (0.35%)\n",
      "\n",
      "ğŸ“Š Top 20 Words in Idioms:\n",
      "   1. '××ª':  965\n",
      "   2. '×”×¨××©':  240\n",
      "   3. '×©×‘×¨':  202\n",
      "   4. '×‘××§×•×':  160\n",
      "   5. '×‘×™×Ÿ':  160\n",
      "   6. '×”×–× ×‘':  159\n",
      "   7. '×™×¨×“':  123\n",
      "   8. '×¤×ª×—':  106\n",
      "   9. '×¢×¦××•':  104\n",
      "  10. '×©×‘×¨×”':  103\n",
      "  11. '×”×¨×™×':   97\n",
      "  12. '×œ×•':   83\n",
      "  13. '×¢×œ':   82\n",
      "  14. '×›××•':   81\n",
      "  15. '×‘××©':   80\n",
      "  16. '×™×“×™×™×':   80\n",
      "  17. '×”×˜×™×¤×•×ª':   80\n",
      "  18. '×”×›×œ×™×':   80\n",
      "  19. '×™×“':   80\n",
      "  20. '×¢×™× ×™×™×':   80\n",
      "\n",
      "ğŸ“Š Lexical Statistics by Label:\n",
      "\n",
      "  ×¤×™×’×•×¨×˜×™×‘×™:\n",
      "    â€¢ Vocabulary size: 10,585\n",
      "    â€¢ Total tokens: 36,791\n",
      "    â€¢ TTR: 0.2877\n",
      "    â€¢ Top 5 words: ['××ª', '×œ×', '×”×™×', '×”×•×', '×¢×œ']\n",
      "\n",
      "  ××™×œ×•×œ×™:\n",
      "    â€¢ Vocabulary size: 10,668\n",
      "    â€¢ Total tokens: 34,984\n",
      "    â€¢ TTR: 0.3049\n",
      "    â€¢ Top 5 words: ['××ª', '×œ×', '×”×•×', '×”×™×', '×¢×œ']\n",
      "\n",
      "ğŸ“Š Function Word Frequencies:\n",
      "  â€¢ '××ª': 2231 (3.11%)\n",
      "  â€¢ '×œ×': 1241 (1.73%)\n",
      "  â€¢ '×”×•×': 1134 (1.58%)\n",
      "  â€¢ '×¢×œ':  886 (1.23%)\n",
      "  â€¢ '×©×œ':  722 (1.01%)\n",
      "  â€¢ '××':  624 (0.87%)\n",
      "  â€¢ '×”×™×”':  497 (0.69%)\n",
      "  â€¢ '×¢×':  405 (0.56%)\n",
      "  â€¢ '×›×™':  360 (0.50%)\n",
      "  â€¢ '××‘×œ':  331 (0.46%)\n",
      "\n",
      "ğŸ“Š Key Finding: High lexical diversity!\n",
      "   Vocabulary size: 17,787 unique words\n",
      "   Type-Token Ratio: 0.2478\n",
      "   Avg unique words per sentence: 14.64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PART 2: OPTIONAL/RECOMMENDED ANALYSES\n",
    "\n",
    "### 4.1 Structural Complexity Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:39:37.852214Z",
     "start_time": "2025-11-10T13:39:37.619610Z"
    }
   },
   "source": [
    "# Analyze structural complexity (subclauses, punctuation)\n",
    "complexity_stats = loader.analyze_structural_complexity()\n",
    "\n",
    "# Show complexity summary (using correct key names!)\n",
    "print(\"\\nğŸ“Š Key Finding: Figurative sentences are more complex!\")\n",
    "print(f\"   Mean subclause markers: {complexity_stats['mean_subclause_count']:.2f}\")\n",
    "print(f\"   Sentences with subclauses: {complexity_stats['sentences_with_subclauses_pct']:.2f}%\")\n",
    "print(f\"   Mean punctuation: {complexity_stats['mean_punctuation_count']:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STRUCTURAL COMPLEXITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Overall Structural Complexity:\n",
      "  â€¢ Mean subclause markers per sentence: 0.28\n",
      "  â€¢ Mean subclause ratio: 0.0149\n",
      "  â€¢ Mean punctuation marks per sentence: 1.67\n",
      "  â€¢ Sentences with subclauses: 1172 (24.42%)\n",
      "\n",
      "ğŸ“Š Structural Complexity by Label:\n",
      "\n",
      "  ×¤×™×’×•×¨×˜×™×‘×™:\n",
      "    â€¢ Mean subclause markers: 0.32\n",
      "    â€¢ Mean subclause ratio: 0.0179\n",
      "    â€¢ Mean punctuation: 1.72\n",
      "\n",
      "  ××™×œ×•×œ×™:\n",
      "    â€¢ Mean subclause markers: 0.24\n",
      "    â€¢ Mean subclause ratio: 0.0119\n",
      "    â€¢ Mean punctuation: 1.62\n",
      "\n",
      "ğŸ“Š Key Finding: Figurative sentences are more complex!\n",
      "   Mean subclause markers: 0.28\n",
      "   Sentences with subclauses: 24.42%\n",
      "   Mean punctuation: 1.67\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Lexical Richness Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:39:41.704548Z",
     "start_time": "2025-11-10T13:39:41.677310Z"
    }
   },
   "source": [
    "# Analyze lexical richness (hapax legomena, Zipf's law)\n",
    "richness_stats = loader.analyze_lexical_richness()\n",
    "\n",
    "# Show richness summary\n",
    "print(\"\\nğŸ“Š Key Finding: Very high lexical richness!\")\n",
    "print(f\"   Hapax legomena: {richness_stats['hapax_legomena_count']:,} ({richness_stats['hapax_ratio']*100:.2f}%)\")\n",
    "print(f\"   Dis legomena: {richness_stats['dis_legomena_count']:,}\")\n",
    "print(f\"   Maas Index: {richness_stats['maas_index']:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEXICAL RICHNESS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Lexical Richness Statistics:\n",
      "  â€¢ Total tokens: 71,775\n",
      "  â€¢ Unique words: 17,787\n",
      "  â€¢ Type-Token Ratio (TTR): 0.2478\n",
      "  â€¢ Hapax legomena (words appearing once): 11,341 (63.76%)\n",
      "  â€¢ Dis legomena (words appearing twice): 2,703\n",
      "  â€¢ Maas Index: 0.0112\n",
      "\n",
      "ğŸ“Š Lexical Richness by Label:\n",
      "\n",
      "  ×¤×™×’×•×¨×˜×™×‘×™:\n",
      "    â€¢ Unique words: 10,585\n",
      "    â€¢ TTR: 0.2877\n",
      "    â€¢ Hapax legomena: 6,989 (66.03%)\n",
      "\n",
      "  ××™×œ×•×œ×™:\n",
      "    â€¢ Unique words: 10,668\n",
      "    â€¢ TTR: 0.3049\n",
      "    â€¢ Hapax legomena: 7,177 (67.28%)\n",
      "\n",
      "ğŸ“Š Key Finding: Very high lexical richness!\n",
      "   Hapax legomena: 11,341 (63.76%)\n",
      "   Dis legomena: 2,703\n",
      "   Maas Index: 0.0112\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Collocational Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:39:45.882985Z",
     "start_time": "2025-11-10T13:39:45.774867Z"
    }
   },
   "source": [
    "# Analyze collocations (context words around idioms)\n",
    "collocation_stats = loader.analyze_collocations()\n",
    "\n",
    "# Show collocation summary\n",
    "print(\"\\nğŸ“Š Key Finding: Rich context around idioms!\")\n",
    "print(f\"   Total context words: {collocation_stats['total_context_words']:,}\")\n",
    "print(f\"   Unique context words: {collocation_stats['unique_context_words']:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLLOCATIONAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Collocational Statistics:\n",
      "  â€¢ Total context words (Â±3 tokens): 21,583\n",
      "  â€¢ Unique context words: 7,415\n",
      "\n",
      "ğŸ“Š Top 20 Context Words (Overall):\n",
      "   1. '×”×•×':  905 (4.19%)\n",
      "   2. '×”×™×':  826 (3.83%)\n",
      "   3. '×”×':  496 (2.30%)\n",
      "   4. '×œ×':  470 (2.18%)\n",
      "   5. '×¢×œ':  352 (1.63%)\n",
      "   6. '××ª':  343 (1.59%)\n",
      "   7. '××—×¨×™':  277 (1.28%)\n",
      "   8. '×›×“×™':  271 (1.26%)\n",
      "   9. '×¢×':  249 (1.15%)\n",
      "  10. '×©×œ':  216 (1.00%)\n",
      "  11. '×›×™':  179 (0.83%)\n",
      "  12. '×”×™×”':  169 (0.78%)\n",
      "  13. '×”×™×œ×“':  153 (0.71%)\n",
      "  14. '××•×œ':  120 (0.56%)\n",
      "  15. '××':  117 (0.54%)\n",
      "  16. '×‘×–××Ÿ':  112 (0.52%)\n",
      "  17. '××œ':   94 (0.44%)\n",
      "  18. '×”×™×™×ª×”':   90 (0.42%)\n",
      "  19. '×›×œ':   84 (0.39%)\n",
      "  20. '×‘×××¦×¢':   78 (0.36%)\n",
      "\n",
      "ğŸ“Š Top 10 Context Words by Label:\n",
      "\n",
      "  Literal:\n",
      "     1. '×”×•×':  411\n",
      "     2. '×”×™×':  355\n",
      "     3. '×”×':  228\n",
      "     4. '×¢×œ':  199\n",
      "     5. '×œ×':  196\n",
      "     6. '×›×“×™':  171\n",
      "     7. '×”×™×œ×“':  135\n",
      "     8. '××ª':  130\n",
      "     9. '×©×œ':  129\n",
      "    10. '×¢×':  117\n",
      "\n",
      "  Figurative:\n",
      "     1. '×”×•×':  494\n",
      "     2. '×”×™×':  471\n",
      "     3. '×œ×':  274\n",
      "     4. '×”×':  268\n",
      "     5. '××ª':  213\n",
      "     6. '××—×¨×™':  162\n",
      "     7. '×¢×œ':  153\n",
      "     8. '×¢×':  132\n",
      "     9. '×›×™':  129\n",
      "    10. '×›×“×™':  100\n",
      "\n",
      "ğŸ“Š Key Finding: Rich context around idioms!\n",
      "   Total context words: 21,583\n",
      "   Unique context words: 7,415\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Annotation Consistency Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:42:20.604292Z",
     "start_time": "2025-11-10T13:42:20.509278Z"
    }
   },
   "source": [
    "# Analyze annotation consistency\n",
    "consistency_stats = loader.analyze_annotation_consistency()\n",
    "\n",
    "# Show consistency summary\n",
    "print(\"\\nğŸ“Š Key Finding: Significant morphological variance!\")\n",
    "print(f\"   Prefix attachments: {consistency_stats['prefix_attachment_count']} ({consistency_stats['prefix_attachment_rate']*100:.2f}%)\")\n",
    "print(f\"   Mean consistency rate: {consistency_stats['mean_consistency_rate']:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANNOTATION CONSISTENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Annotation Consistency:\n",
      "  â€¢ Prefix attachments found: 2097 (43.69%)\n",
      "  â€¢ Mean consistency rate per idiom: 0.4008\n",
      "\n",
      "ğŸ“Š Idioms with Variant Forms (Top 10):\n",
      "  1. ×©× ×¨×’×œ×™×™×: 33 variants (consistency: 0.19)\n",
      "  2. ×©×‘×¨ ××ª ×”×œ×‘: 31 variants (consistency: 0.35)\n",
      "  3. ×¤×ª×— ×“×œ×ª×•×ª: 27 variants (consistency: 0.30)\n",
      "  4. ×¡×’×¨ ×—×©×‘×•×Ÿ: 25 variants (consistency: 0.26)\n",
      "  5. ×”×•×¨×™×“ ×¤×¨×•×¤×™×œ: 23 variants (consistency: 0.31)\n",
      "  6. ×—×ª×š ×¤×™× ×”: 18 variants (consistency: 0.31)\n",
      "  7. ×§×™×‘×œ ×¡×˜×™×¨×”: 18 variants (consistency: 0.55)\n",
      "  8. ×©× ×¢×œ×™×• ×¤×¡: 18 variants (consistency: 0.28)\n",
      "  9. ×—×¦×” ×§×• ××“×•×: 17 variants (consistency: 0.39)\n",
      "  10. ×™×¦× ××”×§×•×¤×¡×”: 17 variants (consistency: 0.45)\n",
      "\n",
      "ğŸ“Š Key Finding: Significant morphological variance!\n",
      "   Prefix attachments: 2097 (43.69%)\n",
      "   Mean consistency rate: 0.4008\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create All Visualizations\n",
    "\n",
    "### 5.1 Standard Visualizations (PART 1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:42:25.308819Z",
     "start_time": "2025-11-10T13:42:23.122673Z"
    }
   },
   "source": [
    "# Create all 11 standard visualizations\n",
    "print(\"Creating standard visualizations...\")\n",
    "loader.create_visualizations()\n",
    "print(\"\\nâœ… Standard visualizations saved to paper/figures/\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating standard visualizations...\n",
      "\n",
      "================================================================================\n",
      "CREATING VISUALIZATIONS (Missions 2.2 & 2.4)\n",
      "================================================================================\n",
      "\n",
      "[1/6] Creating label distribution bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/label_distribution.png\n",
      "\n",
      "[2/6] Creating sentence length distribution histogram...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_length_distribution.png\n",
      "\n",
      "[3/6] Creating idiom length distribution histogram...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/idiom_length_distribution.png\n",
      "\n",
      "[4/6] Creating top 10 idioms bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/top_10_idioms.png\n",
      "\n",
      "[5/6] Creating sentence type distribution pie chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_type_distribution.png\n",
      "\n",
      "[6/6] Creating sentence type by label stacked bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_type_by_label.png\n",
      "\n",
      "[7/11] Creating sentence length boxplot by label...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_length_boxplot_by_label.png\n",
      "\n",
      "[8/11] Creating polysemy heatmap...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/polysemy_heatmap.png\n",
      "\n",
      "[9/11] Creating idiom position histogram...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/idiom_position_histogram.png\n",
      "\n",
      "[10/11] Creating idiom position by label bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/idiom_position_by_label.png\n",
      "\n",
      "[11/11] Creating sentence length violin plot...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_length_violin_by_label.png\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL VISUALIZATIONS CREATED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Location: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures\n",
      "\n",
      "Created files:\n",
      "  1. label_distribution.png\n",
      "  2. sentence_length_distribution.png\n",
      "  3. idiom_length_distribution.png\n",
      "  4. top_10_idioms.png\n",
      "  5. sentence_type_distribution.png\n",
      "  6. sentence_type_by_label.png\n",
      "  7. sentence_length_boxplot_by_label.png (NEW)\n",
      "  8. polysemy_heatmap.png (NEW)\n",
      "  9. idiom_position_histogram.png (NEW)\n",
      "  10. idiom_position_by_label.png (NEW)\n",
      "  11. sentence_length_violin_by_label.png (NEW)\n",
      "\n",
      "âœ… Standard visualizations saved to paper/figures/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igornazarenko/PycharmProjects/Final_Project_NLP/src/data_preparation.py:1612: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(data=plot_data, x='label', y='num_tokens', ax=ax,\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Advanced Visualizations (PART 2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:42:33.928148Z",
     "start_time": "2025-11-10T13:42:32.757136Z"
    }
   },
   "source": [
    "# Create all 6 advanced visualizations\n",
    "print(\"Creating advanced visualizations...\")\n",
    "loader.create_advanced_visualizations()\n",
    "print(\"\\nâœ… Advanced visualizations saved to paper/figures/\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced visualizations...\n",
      "\n",
      "================================================================================\n",
      "CREATING ADVANCED VISUALIZATIONS (PART 2)\n",
      "================================================================================\n",
      "\n",
      "[1/6] Creating Zipf's law plot...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/zipf_law_plot.png\n",
      "\n",
      "[2/6] Creating structural complexity comparison...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/structural_complexity_by_label.png\n",
      "\n",
      "[3/6] Creating collocation word clouds...\n",
      "   âš ï¸  wordcloud library not available. Skipping word clouds.\n",
      "      Install with: pip install wordcloud\n",
      "\n",
      "[4/6] Creating vocabulary diversity scatter plot...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/vocabulary_diversity_scatter.png\n",
      "\n",
      "[5/6] Creating hapax legomena comparison...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/hapax_legomena_comparison.png\n",
      "\n",
      "[6/6] Creating context words bar chart...\n",
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/context_words_bar_chart.png\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL ADVANCED VISUALIZATIONS CREATED!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Location: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures\n",
      "\n",
      "Created advanced visualization files:\n",
      "  1. zipf_law_plot.png\n",
      "  2. structural_complexity_by_label.png\n",
      "  3. collocation_word_clouds.png (if wordcloud library available)\n",
      "  4. vocabulary_diversity_scatter.png\n",
      "  5. hapax_legomena_comparison.png\n",
      "  6. context_words_bar_chart.png\n",
      "\n",
      "âœ… Advanced visualizations saved to paper/figures/\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COMPREHENSIVE ANALYSIS - ALL IN ONE\n",
    "\n",
    "Alternatively, run everything at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive analysis (PART 1 + PART 2, including visualizations)\n",
    "# NOTE: Only run this if you haven't run the analyses above\n",
    "\n",
    "# Uncomment to run:\n",
    "# loader_new = DatasetLoader()\n",
    "# loader_new.load_dataset()\n",
    "# results = loader_new.run_comprehensive_analysis(include_part2=True, create_visualizations=True)\n",
    "# print(\"\\nâœ… Comprehensive analysis complete!\")\n",
    "# print(f\"   Total sentences: {results['statistics']['total_sentences']}\")\n",
    "# print(f\"   Unique idioms: {results['statistics']['unique_expressions']}\")\n",
    "# print(f\"   Vocabulary: {results['lexical']['vocabulary_size']:,} words\")\n",
    "# print(f\"   Hapax legomena: {results['lexical_richness']['hapax_legomena_count']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explore Specific Examples\n",
    "\n",
    "### 7.1 Compare Literal vs Figurative for Same Idiom"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:42:42.575356Z",
     "start_time": "2025-11-10T13:42:42.565879Z"
    }
   },
   "source": [
    "# Show examples of literal vs figurative for same idiom\n",
    "sample_idiom = \"×©×‘×¨ ××ª ×”×¨××©\"  # \"broke the head\"\n",
    "\n",
    "print(f\"Examples for idiom: '{sample_idiom}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "idiom_samples = loader.df[loader.df['expression'] == sample_idiom]\n",
    "literal_examples = idiom_samples[idiom_samples['label'] == '××™×œ×•×œ×™'].head(3)\n",
    "figurative_examples = idiom_samples[idiom_samples['label'] == '×¤×™×’×•×¨×˜×™×‘×™'].head(3)\n",
    "\n",
    "print(\"\\nğŸ”¹ LITERAL Examples:\")\n",
    "for idx, row in literal_examples.iterrows():\n",
    "    print(f\"\\n{row['text']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ”¸ FIGURATIVE Examples:\")\n",
    "for idx, row in figurative_examples.iterrows():\n",
    "    print(f\"\\n{row['text']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for idiom: '×©×‘×¨ ××ª ×”×¨××©'\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¹ LITERAL Examples:\n",
      "\n",
      "×”×™×œ×“ ×©×‘×¨ ××ª ×”×¨××© ×›×©×”×ª×’×œ×’×œ ××”××™×˜×” ×”×—×“×©×” ×‘×¡×œ×•×Ÿ, ×•× ×œ×§×— ×œ×‘×“×™×§×” ×‘×‘×™×ª ×”×—×•×œ×™× ×”×§×¨×•×‘ ×©× ×’×™×œ×• ×¨×§ ×—×‘×œ×” ×§×œ×”.\n",
      "\n",
      "×”×™× ×©×‘×¨×” ××ª ×”×¨××© ×›×©× ×ª×§×œ×” ×‘××‘×Ÿ ×’×“×•×œ×” ×‘×©×‘×™×œ ×”×’×™× ×”\n",
      "\n",
      "×”×× ×©××¢×ª ×©×”×©×—×§×Ÿ ×©×‘×¨ ××ª ×”×¨××© ×‘×××¦×¢ ×”×”×¦×’×” ×œ××—×¨ ×©× ×¤×œ ××”×‘××”, ×•×”×§×”×œ ×”×”××•× ×œ× ×”×‘×™×Ÿ ×× ×–×• ×”×™×™×ª×” ×ª××•× ×” ××• ×—×œ×§ ××”×¢×œ×™×œ×”?\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¸ FIGURATIVE Examples:\n",
      "\n",
      "×× ×©×¨ ×”××•×¦×¨ ×œ× ×™××¦× ×¤×ª×¨×•×Ÿ ×œ××©×‘×¨ ×”×—××•×¨ ×©× ×•×¦×¨ ×‘×¢×§×‘×•×ª ×”××œ×—××” ×”××¨×•×›×” ×‘×™×•×ª×¨ ×¢×“ ×›×”, ×”×•× ×¢×“×™×™×Ÿ ×™××©×™×š ×œ×©×‘×•×¨ ××ª ×”×¨××© ×‘× ×™×¡×™×•×Ÿ ×œ×”×‘×™×Ÿ ××™×š ×œ×ª×§×Ÿ ××ª ×”×‘×¢×™×” ×”×ª×§×¦×™×‘×™×ª.\n",
      "\n",
      "×›××” ×–××Ÿ ×©×‘×¨×ª ××ª ×”×¨××© ×¢×œ × ×™×¡×•×— ×”××™×™×œ ×¢× ×”×¨×¢×™×•×Ÿ ×”××”×¤×›× ×™ ×”×–×” ×©×œ×š ×œ×“×™×¨×§×˜×•×¨×™×•×Ÿ, ×•×”×× ×‘×¡×•×£ ×©×œ×—×ª ××•×ª×• ×›×¤×™ ×©×”×•×?\n",
      "\n",
      "×”×™× ××¢×•×œ× ×œ× ×©×‘×¨×” ××ª ×”×¨××© ×›×œ ×›×š ×”×¨×‘×” ×–××Ÿ ×›×“×™ ×œ××¦×•× ×¨×¢×™×•×Ÿ ×™×¦×™×¨×ª×™ ×œ××ª× ×ª ×™×•× ×”×•×œ×“×ª ××¤×ª×™×¢×” ×›×–××ª, ×•×‘×¡×•×£, ××—×¨×™ ×™××™× ×©×œ ×—×™×¤×•×©×™×, ×”×—×œ×™×˜×” ×œ×”×–××™×Ÿ ×—×•×¤×©×” ×¨×•×× ×˜×™×ª ×‘××™×˜×œ×™×” ×›×”×¤×ª×¢×” ××•×©×œ××ª.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Examine Top Context Words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:42:45.585556Z",
     "start_time": "2025-11-10T13:42:45.580790Z"
    }
   },
   "source": "# Show top context words from collocation analysis\nif 'collocation_stats' in locals():\n    print(\"Top 15 Context Words Around Idioms:\")\n    print(\"=\"*50)\n    for i, (word, count) in enumerate(collocation_stats['top_20_context_overall'][:15], 1):\n        print(f\"{i:2d}. '{word}': {count:4d} occurrences\")\nelse:\n    print(\"Run analyze_collocations() first to see context words\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Context Words Around Idioms:\n",
      "==================================================\n",
      " 1. '×”×•×':  905 occurrences\n",
      " 2. '×”×™×':  826 occurrences\n",
      " 3. '×”×':  496 occurrences\n",
      " 4. '×œ×':  470 occurrences\n",
      " 5. '×¢×œ':  352 occurrences\n",
      " 6. '××ª':  343 occurrences\n",
      " 7. '××—×¨×™':  277 occurrences\n",
      " 8. '×›×“×™':  271 occurrences\n",
      " 9. '×¢×':  249 occurrences\n",
      "10. '×©×œ':  216 occurrences\n",
      "11. '×›×™':  179 occurrences\n",
      "12. '×”×™×”':  169 occurrences\n",
      "13. '×”×™×œ×“':  153 occurrences\n",
      "14. '××•×œ':  120 occurrences\n",
      "15. '××':  117 occurrences\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Examine Idioms with Most Variance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:42:50.933385Z",
     "start_time": "2025-11-10T13:42:50.917709Z"
    }
   },
   "source": [
    "# Show idioms with most morphological variance\n",
    "print(\"Idioms with Most Variant Forms:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count unique matched_expression per idiom\n",
    "variance_df = loader.df.groupby('expression')['matched_expression'].nunique().sort_values(ascending=False)\n",
    "\n",
    "for i, (idiom, variants) in enumerate(variance_df.head(10).items(), 1):\n",
    "    print(f\"{i:2d}. {idiom:40s}: {variants:2d} variants\")\n",
    "    \n",
    "    # Show sample variants\n",
    "    sample_variants = loader.df[loader.df['expression'] == idiom]['matched_expression'].value_counts().head(3)\n",
    "    for variant, count in sample_variants.items():\n",
    "        print(f\"    - {variant}: {count} times\")\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idioms with Most Variant Forms:\n",
      "==================================================\n",
      " 1. ×©× ×¨×’×œ×™×™×                               : 33 variants\n",
      "    - ×©× ×¨×’×œ×™×™×: 15 times\n",
      "    - ×©××” ×¨×’×œ×™×™×: 14 times\n",
      "    - ×©××• ×œ×• ×¨×’×œ×™×™×: 4 times\n",
      "\n",
      " 2. ×©×‘×¨ ××ª ×”×œ×‘                              : 31 variants\n",
      "    - ×©×‘×¨ ××ª ×”×œ×‘: 28 times\n",
      "    - ×©×‘×¨×” ××ª ×”×œ×‘: 13 times\n",
      "    - ×©×‘×¨×• ××ª ×”×œ×‘: 3 times\n",
      "\n",
      " 3. ×¤×ª×— ×“×œ×ª×•×ª                               : 27 variants\n",
      "    - ×¤×ª×— ×“×œ×ª×•×ª: 24 times\n",
      "    - ×¤×ª×—×” ×“×œ×ª×•×ª: 12 times\n",
      "    - ×¤×ª×—×• ×“×œ×ª×•×ª: 5 times\n",
      "\n",
      " 4. ×¡×’×¨ ×—×©×‘×•×Ÿ                               : 25 variants\n",
      "    - ×¡×’×¨ ×—×©×‘×•×Ÿ: 21 times\n",
      "    - ×¡×’×¨×” ×—×©×‘×•×Ÿ: 11 times\n",
      "    - ×œ×¡×’×•×¨ ×—×©×‘×•×Ÿ: 10 times\n",
      "\n",
      " 5. ×”×•×¨×™×“ ×¤×¨×•×¤×™×œ                            : 23 variants\n",
      "    - ×”×•×¨×™×“ ×¤×¨×•×¤×™×œ: 25 times\n",
      "    - ×”×•×¨×™×“×” ×¤×¨×•×¤×™×œ: 14 times\n",
      "    - ×œ×”×•×¨×“×ª ×¤×¨×•×¤×™×œ: 6 times\n",
      "\n",
      " 6. ×—×ª×š ×¤×™× ×”                                : 18 variants\n",
      "    - ×—×ª×š ×¤×™× ×”: 25 times\n",
      "    - ×—×ª×›×” ×¤×™× ×”: 12 times\n",
      "    - ×œ×—×ª×•×š ×¤×™× ×”: 9 times\n",
      "\n",
      " 7. ×§×™×‘×œ ×¡×˜×™×¨×”                              : 18 variants\n",
      "    - ×§×™×‘×œ ×¡×˜×™×¨×”: 44 times\n",
      "    - ×§×™×‘×œ×” ×¡×˜×™×¨×”: 14 times\n",
      "    - ×§×™×‘×œ×• ×¡×˜×™×¨×”: 4 times\n",
      "\n",
      " 8. ×©× ×¢×œ×™×• ×¤×¡                              : 18 variants\n",
      "    - ×©× ×¤×¡: 22 times\n",
      "    - ×©× ×¢×œ×™×• ×¤×¡: 17 times\n",
      "    - ×©××• ×¢×œ×™×• ×¤×¡: 11 times\n",
      "\n",
      " 9. ×™×¦× ××”×§×•×¤×¡×”                             : 17 variants\n",
      "    - ×™×¦× ××”×§×•×¤×¡×”: 36 times\n",
      "    - ×™×¦××” ××”×§×•×¤×¡×”: 12 times\n",
      "    - ×™×¦××• ××”×§×•×¤×¡×”: 6 times\n",
      "\n",
      "10. ×—×¦×” ×§×• ××“×•×                             : 17 variants\n",
      "    - ×—×¦×” ×§×• ××“×•×: 31 times\n",
      "    - ×—×¦×ª×” ×§×• ××“×•×: 14 times\n",
      "    - ×—×•×¦×” ×§×• ××“×•×: 8 times\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:42:56.683519Z",
     "start_time": "2025-11-10T13:42:56.675947Z"
    }
   },
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Sentences',\n",
    "        'Unique Idioms',\n",
    "        'Avg Sentence Length (tokens)',\n",
    "        'Avg Idiom Length (tokens)',\n",
    "        'Vocabulary Size',\n",
    "        'Type-Token Ratio',\n",
    "        'Hapax Legomena',\n",
    "        'Polysemous Idioms',\n",
    "        'Idioms at Start (%)',\n",
    "        'Sentences with Subclauses (%)',\n",
    "        'Mean Prefix Attachment Rate (%)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{len(loader.df):,}\",\n",
    "        f\"{loader.df['expression'].nunique()}\",\n",
    "        f\"{loader.df['num_tokens'].mean():.2f}\",\n",
    "        f\"{(loader.df['token_span_end'] - loader.df['token_span_start']).mean():.2f}\",\n",
    "        f\"{lexical_stats['vocabulary_size']:,}\" if 'lexical_stats' in locals() else \"Run lexical analysis\",\n",
    "        f\"{lexical_stats['ttr_overall']:.4f}\" if 'lexical_stats' in locals() else \"Run lexical analysis\",\n",
    "        f\"{richness_stats['hapax_legomena_count']:,} ({richness_stats['hapax_ratio']*100:.2f}%)\" if 'richness_stats' in locals() else \"Run richness analysis\",\n",
    "        f\"{polysemy_stats['polysemous_count']} (100%)\" if 'polysemy_stats' in locals() else \"Run polysemy analysis\",\n",
    "        f\"{position_stats['position_percentages']['start']:.2f}%\" if 'position_stats' in locals() else \"Run position analysis\",\n",
    "        f\"{complexity_stats['sentences_with_subclauses_pct']:.2f}%\" if 'complexity_stats' in locals() else \"Run complexity analysis\",\n",
    "        f\"{consistency_stats['prefix_attachment_rate']*100:.2f}%\" if 'consistency_stats' in locals() else \"Run consistency analysis\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE DATASET SUMMARY\n",
      "================================================================================\n",
      "                         Metric           Value\n",
      "                Total Sentences           4,800\n",
      "                  Unique Idioms              60\n",
      "   Avg Sentence Length (tokens)           14.95\n",
      "      Avg Idiom Length (tokens)            2.48\n",
      "                Vocabulary Size          17,787\n",
      "               Type-Token Ratio          0.2478\n",
      "                 Hapax Legomena 11,341 (63.76%)\n",
      "              Polysemous Idioms       60 (100%)\n",
      "            Idioms at Start (%)          87.06%\n",
      "  Sentences with Subclauses (%)          24.42%\n",
      "Mean Prefix Attachment Rate (%)          43.69%\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validation Checklist"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:43:12.259314Z",
     "start_time": "2025-11-10T13:43:12.242486Z"
    }
   },
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE VALIDATION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checklist = [\n",
    "    (\"Dataset loads successfully\", len(loader.df) > 0),\n",
    "    (\"4,800 total sentences\", len(loader.df) == 4800),\n",
    "    (\"Perfect label balance (50/50)\", abs(loader.df['label_2'].value_counts()[0] - loader.df['label_2'].value_counts()[1]) == 0),\n",
    "    (\"60 unique idioms\", loader.df['expression'].nunique() == 60),\n",
    "    (\"No duplicate rows\", loader.df.duplicated().sum() == 0),\n",
    "    (\"All idioms are polysemous\", True if 'polysemy_stats' in locals() and polysemy_stats['polysemous_count'] == 60 else False),\n",
    "    (\"High lexical diversity (>60% hapax)\", True if 'richness_stats' in locals() and richness_stats['hapax_ratio'] > 0.6 else False),\n",
    "    (\"Most idioms at sentence start\", True if 'position_stats' in locals() and position_stats['position_percentages']['start'] > 80 else False),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for criterion, passed in checklist:\n",
    "    status = \"âœ…\" if passed else \"âŒ\"\n",
    "    print(f\"{status} {criterion}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ‰ ALL VALIDATION CRITERIA PASSED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"âœ… Dataset is fully analyzed and validated\")\n",
    "    print(\"âœ… Ready for Mission 2.5: Dataset Splitting\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some criteria not met. Run all analyses above.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE VALIDATION CHECKLIST\n",
      "================================================================================\n",
      "âœ… Dataset loads successfully\n",
      "âœ… 4,800 total sentences\n",
      "âœ… Perfect label balance (50/50)\n",
      "âœ… 60 unique idioms\n",
      "âœ… No duplicate rows\n",
      "âœ… All idioms are polysemous\n",
      "âœ… High lexical diversity (>60% hapax)\n",
      "âœ… Most idioms at sentence start\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ ALL VALIDATION CRITERIA PASSED!\n",
      "================================================================================\n",
      "âœ… Dataset is fully analyzed and validated\n",
      "âœ… Ready for Mission 2.5: Dataset Splitting\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Comprehensive Analysis\n",
    "\n",
    "**Key Findings:**\n",
    "1. **Dataset Quality**: Perfectly balanced, no duplicates, 4,800 sentences\n",
    "2. **Polysemy**: All 60 idioms are polysemous (50/50 literal/figurative)\n",
    "3. **Position**: 87.96% of idioms appear at sentence start\n",
    "4. **Lexical Diversity**: Very high (63.43% hapax legomena)\n",
    "5. **Complexity**: Figurative sentences are more complex\n",
    "6. **Morphology**: Significant variance (41.40% prefix attachments)\n",
    "\n",
    "**Output Files:**\n",
    "- Statistics: `experiments/results/dataset_statistics_full.txt`\n",
    "- Visualizations: `paper/figures/` (17 total)\n",
    "\n",
    "**Next Steps:**\n",
    "- Mission 2.5: Dataset Splitting (Expression-Based Strategy)\n",
    "- Mission 3: Model Training and Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
