{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission 2: Comprehensive Dataset Analysis and Validation\n",
    "# Hebrew Idiom Detection Project - Enhanced with PART 1 + PART 2 Analyses\n",
    "\n",
    "**Purpose:** Comprehensive exploration, validation, and statistical analysis of the Hebrew idiom dataset\n",
    "\n",
    "**Date:** November 10, 2025\n",
    "\n",
    "**Analysis Coverage:**\n",
    "- PART 1: Required analyses (basic statistics, polysemy, position, lexical)\n",
    "- PART 2: Optional analyses (structural complexity, lexical richness, collocations, consistency)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:57:02.386508Z",
     "start_time": "2025-11-19T11:57:02.381256Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:22.507376Z",
     "iopub.status.busy": "2025-11-19T12:11:22.507216Z",
     "iopub.status.idle": "2025-11-19T12:11:30.901072Z",
     "shell.execute_reply": "2025-11-19T12:11:30.900656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n",
      "ğŸ“ Project root: /Users/igornazarenko/PycharmProjects/Final_Project_NLP\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import our data preparation module\n",
    "from src.data_preparation import DatasetLoader\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… Imports successful!\")\n",
    "print(f\"ğŸ“ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:57:05.981333Z",
     "start_time": "2025-11-19T11:57:05.950870Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.902609Z",
     "iopub.status.busy": "2025-11-19T12:11:30.902493Z",
     "iopub.status.idle": "2025-11-19T12:11:30.931139Z",
     "shell.execute_reply": "2025-11-19T12:11:30.930762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/data/expressions_data_tagged.csv\n",
      "âœ… Dataset loaded successfully!\n",
      "Total rows: 4800\n",
      "\n",
      "âœ… Dataset loaded: 4800 rows, 16 columns\n"
     ]
    }
   ],
   "source": [
    "# Initialize loader and load dataset\n",
    "loader = DatasetLoader()\n",
    "loader.load_dataset()  # IMPORTANT: Must explicitly load!\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded: {len(loader.df)} rows, {len(loader.df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:57:09.855810Z",
     "start_time": "2025-11-19T11:57:09.845315Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.932431Z",
     "iopub.status.busy": "2025-11-19T12:11:30.932366Z",
     "iopub.status.idle": "2025-11-19T12:11:30.939817Z",
     "shell.execute_reply": "2025-11-19T12:11:30.939479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (4800, 16)\n",
      "\n",
      "Column Names:\n",
      "['id', 'split', 'language', 'source', 'text', 'expression', 'matched_expression', 'span_start', 'span_end', 'token_span_start', 'token_span_end', 'num_tokens', 'label', 'label_2', 'iob2_tags', 'char_mask']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>expression</th>\n",
       "      <th>matched_expression</th>\n",
       "      <th>span_start</th>\n",
       "      <th>span_end</th>\n",
       "      <th>token_span_start</th>\n",
       "      <th>token_span_end</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>label_2</th>\n",
       "      <th>iob2_tags</th>\n",
       "      <th>char_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>×× ×©×¨ ×”××•×¦×¨ ×œ× ×™××¦× ×¤×ª×¨×•×Ÿ ×œ××©×‘×¨ ×”×—××•×¨ ×©× ×•×¦×¨ ×‘×¢×§×‘×•×ª ×”××œ×—××” ×”××¨×•×›×” ×‘×™×•×ª×¨ ×¢×“ ×›×”, ×”×•× ×¢×“×™×™×Ÿ ×™××©×™×š ×œ×©...</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×œ×©×‘×•×¨ ××ª ×”×¨××©</td>\n",
       "      <td>94</td>\n",
       "      <td>107</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O</td>\n",
       "      <td>000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>×›××” ×–××Ÿ ×©×‘×¨×ª ××ª ×”×¨××© ×¢×œ × ×™×¡×•×— ×”××™×™×œ ×¢× ×”×¨×¢×™×•×Ÿ ×”××”×¤×›× ×™ ×”×–×” ×©×œ×š ×œ×“×™×¨×§×˜×•×¨×™×•×Ÿ, ×•×”×× ×‘×¡×•×£ ×©×œ×—×ª ××•×ª×• ×›...</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×©×‘×¨×ª ××ª ×”×¨××©</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O</td>\n",
       "      <td>000000001111111111110000000000000000000000000000000000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>×”×™× ××¢×•×œ× ×œ× ×©×‘×¨×” ××ª ×”×¨××© ×›×œ ×›×š ×”×¨×‘×” ×–××Ÿ ×›×“×™ ×œ××¦×•× ×¨×¢×™×•×Ÿ ×™×¦×™×¨×ª×™ ×œ××ª× ×ª ×™×•× ×”×•×œ×“×ª ××¤×ª×™×¢×” ×›×–××ª, ×•×‘×¡...</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×©×‘×¨×” ××ª ×”×¨××©</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "      <td>000000000000011111111111100000000000000000000000000000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>×”×¦×•×•×ª ×›×•×œ×• ×©×‘×¨ ××ª ×”×¨××© ×¢×œ ××™×š ×œ×¢××•×“ ×‘×“×“×œ×™×™×Ÿ, ×ª×•×š ×›×“×™ ×©× ××œ×¥ ×œ×”×ª××•×“×“ ×¢× ×‘×¢×™×•×ª ×˜×›× ×™×•×ª ×‘×œ×ª×™ ×¦×¤×•×™×•×ª, ...</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O O O O O O</td>\n",
       "      <td>000000000001111111111100000000000000000000000000000000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>he</td>\n",
       "      <td>inhouse</td>\n",
       "      <td>××—×¨×™ ×©×“×™×‘×¨×ª×™ ×‘××©×š ×©×¢×” ×¢× ××©×ª×™ ×©×‘×¨×ª×™ ××ª ×”×¨××© ×œ×”×‘×™×Ÿ ××” ×”×™× ×‘×¢×¦× ×× ×¡×” ×œ×•××¨ ×œ×™.</td>\n",
       "      <td>×©×‘×¨ ××ª ×”×¨××©</td>\n",
       "      <td>×©×‘×¨×ª×™ ××ª ×”×¨××©</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>×¤×™×’×•×¨×˜×™×‘×™</td>\n",
       "      <td>1</td>\n",
       "      <td>O O O O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O</td>\n",
       "      <td>000000000000000000000000000000111111111111100000000000000000000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        split language   source  \\\n",
       "0   0  unspecified       he  inhouse   \n",
       "1   1  unspecified       he  inhouse   \n",
       "2   2  unspecified       he  inhouse   \n",
       "3   3  unspecified       he  inhouse   \n",
       "4   4  unspecified       he  inhouse   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  ×× ×©×¨ ×”××•×¦×¨ ×œ× ×™××¦× ×¤×ª×¨×•×Ÿ ×œ××©×‘×¨ ×”×—××•×¨ ×©× ×•×¦×¨ ×‘×¢×§×‘×•×ª ×”××œ×—××” ×”××¨×•×›×” ×‘×™×•×ª×¨ ×¢×“ ×›×”, ×”×•× ×¢×“×™×™×Ÿ ×™××©×™×š ×œ×©...   \n",
       "1  ×›××” ×–××Ÿ ×©×‘×¨×ª ××ª ×”×¨××© ×¢×œ × ×™×¡×•×— ×”××™×™×œ ×¢× ×”×¨×¢×™×•×Ÿ ×”××”×¤×›× ×™ ×”×–×” ×©×œ×š ×œ×“×™×¨×§×˜×•×¨×™×•×Ÿ, ×•×”×× ×‘×¡×•×£ ×©×œ×—×ª ××•×ª×• ×›...   \n",
       "2  ×”×™× ××¢×•×œ× ×œ× ×©×‘×¨×” ××ª ×”×¨××© ×›×œ ×›×š ×”×¨×‘×” ×–××Ÿ ×›×“×™ ×œ××¦×•× ×¨×¢×™×•×Ÿ ×™×¦×™×¨×ª×™ ×œ××ª× ×ª ×™×•× ×”×•×œ×“×ª ××¤×ª×™×¢×” ×›×–××ª, ×•×‘×¡...   \n",
       "3  ×”×¦×•×•×ª ×›×•×œ×• ×©×‘×¨ ××ª ×”×¨××© ×¢×œ ××™×š ×œ×¢××•×“ ×‘×“×“×œ×™×™×Ÿ, ×ª×•×š ×›×“×™ ×©× ××œ×¥ ×œ×”×ª××•×“×“ ×¢× ×‘×¢×™×•×ª ×˜×›× ×™×•×ª ×‘×œ×ª×™ ×¦×¤×•×™×•×ª, ...   \n",
       "4                          ××—×¨×™ ×©×“×™×‘×¨×ª×™ ×‘××©×š ×©×¢×” ×¢× ××©×ª×™ ×©×‘×¨×ª×™ ××ª ×”×¨××© ×œ×”×‘×™×Ÿ ××” ×”×™× ×‘×¢×¦× ×× ×¡×” ×œ×•××¨ ×œ×™.   \n",
       "\n",
       "    expression matched_expression  span_start  span_end  token_span_start  \\\n",
       "0  ×©×‘×¨ ××ª ×”×¨××©      ×œ×©×‘×•×¨ ××ª ×”×¨××©          94       107                18   \n",
       "1  ×©×‘×¨ ××ª ×”×¨××©       ×©×‘×¨×ª ××ª ×”×¨××©           8        20                 2   \n",
       "2  ×©×‘×¨ ××ª ×”×¨××©       ×©×‘×¨×” ××ª ×”×¨××©          13        25                 3   \n",
       "3  ×©×‘×¨ ××ª ×”×¨××©        ×©×‘×¨ ××ª ×”×¨××©          11        22                 2   \n",
       "4  ×©×‘×¨ ××ª ×”×¨××©      ×©×‘×¨×ª×™ ××ª ×”×¨××©          30        43                 6   \n",
       "\n",
       "   token_span_end  num_tokens      label  label_2  \\\n",
       "0              21          28  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "1               5          20  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "2               6          31  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "3               5          25  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "4               9          16  ×¤×™×’×•×¨×˜×™×‘×™        1   \n",
       "\n",
       "                                                                         iob2_tags  \\\n",
       "0        O O O O O O O O O O O O O O O O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O   \n",
       "1                        O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O   \n",
       "2  O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O O O O O O O O O O O   \n",
       "3              O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O O O O O O O O O O O O O O   \n",
       "4                                O O O O O O B-IDIOM I-IDIOM I-IDIOM O O O O O O O   \n",
       "\n",
       "                                                                                             char_mask  \n",
       "0  000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000011...  \n",
       "1  000000001111111111110000000000000000000000000000000000000000000000000000000000000000000000000000...  \n",
       "2  000000000000011111111111100000000000000000000000000000000000000000000000000000000000000000000000...  \n",
       "3  000000000001111111111100000000000000000000000000000000000000000000000000000000000000000000000000...  \n",
       "4                          000000000000000000000000000000111111111111100000000000000000000000000000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display basic info\n",
    "print(\"Dataset Shape:\", loader.df.shape)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(loader.df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "loader.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PART 1: REQUIRED ANALYSES\n",
    "\n",
    "### 3.1 Basic Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:57:14.641665Z",
     "start_time": "2025-11-19T11:57:14.624116Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.941103Z",
     "iopub.status.busy": "2025-11-19T12:11:30.941033Z",
     "iopub.status.idle": "2025-11-19T12:11:30.951129Z",
     "shell.execute_reply": "2025-11-19T12:11:30.950775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Total Sentences: 4800\n",
      "\n",
      "ğŸ“Š Label Distribution:\n",
      "  â€¢ ×¤×™×’×•×¨×˜×™×‘×™      :  2400 (50.00%)\n",
      "  â€¢ ××™×œ×•×œ×™         :  2400 (50.00%)\n",
      "\n",
      "ğŸ“Š Unique Idioms/Expressions: 60\n",
      "\n",
      "ğŸ“Š Expression Occurrence Statistics:\n",
      "  â€¢ Min occurrences per idiom: 80\n",
      "  â€¢ Max occurrences per idiom: 80\n",
      "  â€¢ Mean occurrences per idiom: 80.00\n",
      "  â€¢ Median occurrences per idiom: 80.00\n",
      "  â€¢ Std occurrences per idiom: 0.00\n",
      "\n",
      "ğŸ“Š Sentence Length Statistics (tokens):\n",
      "  â€¢ Average: 15.71 tokens\n",
      "  â€¢ Median:  12 tokens\n",
      "  â€¢ Std:     8.01 tokens\n",
      "  â€¢ Min:     5 tokens\n",
      "  â€¢ Max:     38 tokens\n",
      "\n",
      "ğŸ“Š Sentence Length Statistics (characters):\n",
      "  â€¢ Average: 83.04 chars\n",
      "  â€¢ Median:  63.00 chars\n",
      "  â€¢ Std:     42.55 chars\n",
      "  â€¢ Min:     22 chars\n",
      "  â€¢ Max:     193 chars\n",
      "\n",
      "ğŸ“Š Idiom Length Statistics (tokens):\n",
      "  â€¢ Average: 2.48 tokens\n",
      "  â€¢ Median:  2 tokens\n",
      "  â€¢ Std:     0.64 tokens\n",
      "  â€¢ Min:     2 tokens\n",
      "  â€¢ Max:     5 tokens\n",
      "\n",
      "ğŸ“Š Idiom Length Statistics (characters):\n",
      "  â€¢ Average: 11.39 chars\n",
      "  â€¢ Median:  11.00 chars\n",
      "  â€¢ Std:     3.15 chars\n",
      "  â€¢ Min:     5 chars\n",
      "  â€¢ Max:     23 chars\n",
      "\n",
      "ğŸ“Š Top 10 Most Frequent Expressions:\n",
      "   1. ×©×‘×¨ ××ª ×”×¨××©                              :  80 occurrences\n",
      "   2. ×©×™×—×§ ×‘××©                                 :  80 occurrences\n",
      "   3. ××™×‘×“ ××ª ×”×¨××©                             :  80 occurrences\n",
      "   4. ×œ×‘ ×–×”×‘                                   :  80 occurrences\n",
      "   5. ×—×•×ª×š ×›××• ×¡×›×™×Ÿ                            :  80 occurrences\n",
      "   6. ×”×™×™×ª×” ×‘×¢× × ×™×                             :  80 occurrences\n",
      "   7. ×—×˜×£ ×—×•×                                  :  80 occurrences\n",
      "   8. ×©×¤×›×” ××•×¨                                 :  80 occurrences\n",
      "   9. ×©×‘×¨ ×©×ª×™×§×”                                :  80 occurrences\n",
      "  10. ×¨×¥ ××—×¨×™ ×”×–× ×‘ ×©×œ ×¢×¦××•                     :  80 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Run basic statistics (includes expression occurrences, char lengths, etc.)\n",
    "basic_stats = loader.generate_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sentence Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:57:30.712400Z",
     "start_time": "2025-11-19T11:57:30.676978Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.952314Z",
     "iopub.status.busy": "2025-11-19T12:11:30.952245Z",
     "iopub.status.idle": "2025-11-19T12:11:30.970228Z",
     "shell.execute_reply": "2025-11-19T12:11:30.969853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SENTENCE TYPE ANALYSIS (Mission 2.4)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Sentence Type Distribution:\n",
      "  â€¢ Declarative    :  4405 (91.77%)\n",
      "  â€¢ Question       :   350 ( 7.29%)\n",
      "  â€¢ Exclamatory    :    45 ( 0.94%)\n",
      "\n",
      "ğŸ“Š Sentence Type by Label (Literal vs Figurative):\n",
      "label          ××™×œ×•×œ×™  ×¤×™×’×•×¨×˜×™×‘×™   All\n",
      "sentence_type                         \n",
      "Declarative      2216       2189  4405\n",
      "Exclamatory        28         17    45\n",
      "Question          156        194   350\n",
      "All              2400       2400  4800\n",
      "\n",
      "ğŸ“Š Percentage Distribution within each Label:\n",
      "label          ××™×œ×•×œ×™  ×¤×™×’×•×¨×˜×™×‘×™\n",
      "sentence_type                   \n",
      "Declarative     92.33      91.21\n",
      "Exclamatory      1.17       0.71\n",
      "Question         6.50       8.08\n",
      "\n",
      "ğŸ“Š Balance Check (are sentence types distributed evenly across labels?):\n",
      "  â€¢ Declarative    : Literal=50.3%, Figurative=49.7%\n",
      "  â€¢ Question       : Literal=44.6%, Figurative=55.4%\n",
      "  â€¢ Exclamatory    : Literal=62.2%, Figurative=37.8%\n",
      "\n",
      "ğŸ“Š Sentence Types by Top Expressions:\n",
      "\n",
      "  ×©×‘×¨ ××ª ×”×¨××©:\n",
      "    - Declarative: 71 (88.8%)\n",
      "    - Question: 6 (7.5%)\n",
      "    - Exclamatory: 3 (3.8%)\n",
      "\n",
      "  ×©×™×—×§ ×‘××©:\n",
      "    - Declarative: 77 (96.2%)\n",
      "    - Exclamatory: 2 (2.5%)\n",
      "    - Question: 1 (1.2%)\n",
      "\n",
      "  ××™×‘×“ ××ª ×”×¨××©:\n",
      "    - Declarative: 71 (88.8%)\n",
      "    - Question: 9 (11.2%)\n",
      "\n",
      "  ×œ×‘ ×–×”×‘:\n",
      "    - Declarative: 70 (87.5%)\n",
      "    - Question: 10 (12.5%)\n",
      "\n",
      "  ×—×•×ª×š ×›××• ×¡×›×™×Ÿ:\n",
      "    - Declarative: 77 (96.2%)\n",
      "    - Question: 3 (3.8%)\n",
      "\n",
      "  ×”×™×™×ª×” ×‘×¢× × ×™×:\n",
      "    - Declarative: 75 (93.8%)\n",
      "    - Question: 4 (5.0%)\n",
      "    - Exclamatory: 1 (1.2%)\n",
      "\n",
      "  ×—×˜×£ ×—×•×:\n",
      "    - Declarative: 76 (95.0%)\n",
      "    - Question: 4 (5.0%)\n",
      "\n",
      "  ×©×¤×›×” ××•×¨:\n",
      "    - Declarative: 75 (93.8%)\n",
      "    - Question: 4 (5.0%)\n",
      "    - Exclamatory: 1 (1.2%)\n",
      "\n",
      "  ×©×‘×¨ ×©×ª×™×§×”:\n",
      "    - Declarative: 74 (92.5%)\n",
      "    - Question: 6 (7.5%)\n",
      "\n",
      "  ×¨×¥ ××—×¨×™ ×”×–× ×‘ ×©×œ ×¢×¦××•:\n",
      "    - Declarative: 75 (93.8%)\n",
      "    - Question: 4 (5.0%)\n",
      "    - Exclamatory: 1 (1.2%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze sentence types (declarative, question, exclamatory)\n",
    "sentence_types = loader.analyze_sentence_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Idiom Position Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:57:56.040503Z",
     "start_time": "2025-11-19T11:57:56.029534Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.971430Z",
     "iopub.status.busy": "2025-11-19T12:11:30.971356Z",
     "iopub.status.idle": "2025-11-19T12:11:30.976296Z",
     "shell.execute_reply": "2025-11-19T12:11:30.975847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IDIOM POSITION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Position Ratio Statistics:\n",
      "  â€¢ Mean: 0.2801\n",
      "  â€¢ Median: 0.2000\n",
      "  â€¢ Std: 0.2114\n",
      "  â€¢ Range: [0.0000, 0.9355]\n",
      "\n",
      "ğŸ“Š Position Distribution:\n",
      "  â€¢ Start (0-33%): 3058 (63.71%)\n",
      "  â€¢ Middle (33-67%): 1429 (29.77%)\n",
      "  â€¢ End (67-100%): 313 (6.52%)\n",
      "\n",
      "ğŸ“Š Position Distribution by Label:\n",
      "\n",
      "  ×¤×™×’×•×¨×˜×™×‘×™:\n",
      "    â€¢ Start: 1543 (64.29%)\n",
      "    â€¢ Middle: 673 (28.04%)\n",
      "    â€¢ End: 184 (7.67%)\n",
      "    â€¢ Mean position ratio: 0.2794\n",
      "\n",
      "  ××™×œ×•×œ×™:\n",
      "    â€¢ Start: 1515 (63.12%)\n",
      "    â€¢ Middle: 756 (31.50%)\n",
      "    â€¢ End: 129 (5.38%)\n",
      "    â€¢ Mean position ratio: 0.2808\n",
      "\n",
      "ğŸ“Š Key Finding: Most idioms appear at sentence start!\n",
      "   Start: 63.71%\n",
      "   Middle: 29.77%\n",
      "   End: 6.52%\n"
     ]
    }
   ],
   "source": [
    "# Analyze where idioms appear in sentences\n",
    "position_stats = loader.analyze_idiom_position()\n",
    "\n",
    "# Show position distribution\n",
    "print(\"\\nğŸ“Š Key Finding: Most idioms appear at sentence start!\")\n",
    "print(f\"   Start: {position_stats['position_percentages']['start']:.2f}%\")\n",
    "print(f\"   Middle: {position_stats['position_percentages']['middle']:.2f}%\")\n",
    "print(f\"   End: {position_stats['position_percentages']['end']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Polysemy Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:58:06.745293Z",
     "start_time": "2025-11-19T11:58:06.734786Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.977557Z",
     "iopub.status.busy": "2025-11-19T12:11:30.977491Z",
     "iopub.status.idle": "2025-11-19T12:11:30.981468Z",
     "shell.execute_reply": "2025-11-19T12:11:30.981128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "POLYSEMY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Polysemy Statistics:\n",
      "  â€¢ Total expressions: 60\n",
      "  â€¢ Polysemous idioms (both literal & figurative): 60 (100.00%)\n",
      "  â€¢ Only literal: 0\n",
      "  â€¢ Only figurative: 0\n",
      "\n",
      "ğŸ“Š Top 10 Most Polysemous Idioms (by balance):\n",
      "  1. ××™×‘×“ ××ª ×”×¦×¤×•×Ÿ\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  2. ××™×‘×“ ××ª ×”×¨××©\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  3. × ×©××¨ ×××—×•×¨\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  4. × ×©×‘×¨ ××‘×¤× ×™×\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  5. × ×©×š ×©×¤×ª×™×™×\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  6. × ×ª×Ÿ ×’×–\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  7. × ×ª×Ÿ ×™×“\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  8. ×¡×’×¨ ×—×©×‘×•×Ÿ\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  9. ×¢×©×” ×¡×œ×˜\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "  10. ×¢×©×” ×¡×¦× ×”\n",
      "     Figurative: 40 (50.0%) | Literal: 40 (50.0%)\n",
      "\n",
      "ğŸ“Š Key Finding: All idioms are polysemous!\n",
      "   Total expressions: 60\n",
      "   Polysemous: 60 (100%)\n",
      "   Only literal: 0\n",
      "   Only figurative: 0\n"
     ]
    }
   ],
   "source": [
    "# Analyze polysemy (idioms in both literal & figurative contexts)\n",
    "polysemy_stats = loader.analyze_polysemy()\n",
    "\n",
    "# Show polysemy summary\n",
    "print(\"\\nğŸ“Š Key Finding: All idioms are polysemous!\")\n",
    "print(f\"   Total expressions: {polysemy_stats['total_expressions']}\")\n",
    "print(f\"   Polysemous: {polysemy_stats['polysemous_count']} (100%)\")\n",
    "print(f\"   Only literal: {polysemy_stats['only_literal_count']}\")\n",
    "print(f\"   Only figurative: {polysemy_stats['only_figurative_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Lexical Statistics (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:58:12.071682Z",
     "start_time": "2025-11-19T11:58:12.019025Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:30.982719Z",
     "iopub.status.busy": "2025-11-19T12:11:30.982646Z",
     "iopub.status.idle": "2025-11-19T12:11:31.014136Z",
     "shell.execute_reply": "2025-11-19T12:11:31.013657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEXICAL STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Overall Lexical Statistics:\n",
      "  â€¢ Vocabulary size (unique words): 18,784\n",
      "  â€¢ Total tokens: 75,412\n",
      "  â€¢ Type-Token Ratio (TTR): 0.2491\n",
      "  â€¢ Average unique words per sentence: 15.38\n",
      "  â€¢ Function word ratio: 0.1257 (12.57%)\n",
      "\n",
      "ğŸ“Š Top 20 Most Frequent Words:\n",
      "   1. '××ª': 2295 (3.04%)\n",
      "   2. '×œ×': 1289 (1.71%)\n",
      "   3. '×”×•×': 1105 (1.47%)\n",
      "   4. '×”×™×': 1004 (1.33%)\n",
      "   5. '×¢×œ':  918 (1.22%)\n",
      "   6. '×©×œ':  756 (1.00%)\n",
      "   7. '××':  623 (0.83%)\n",
      "   8. 'â€“':  559 (0.74%)\n",
      "   9. '×”×':  521 (0.69%)\n",
      "  10. '××—×¨×™':  518 (0.69%)\n",
      "  11. '×”×™×”':  509 (0.67%)\n",
      "  12. '×›×“×™':  475 (0.63%)\n",
      "  13. '×¢×':  426 (0.56%)\n",
      "  14. '×›×œ':  420 (0.56%)\n",
      "  15. '××‘×œ':  326 (0.43%)\n",
      "  16. '×›×™':  318 (0.42%)\n",
      "  17. '×–×”':  300 (0.40%)\n",
      "  18. '×”×™×™×ª×”':  289 (0.38%)\n",
      "  19. '×‘×™×Ÿ':  253 (0.34%)\n",
      "  20. '×œ×•':  251 (0.33%)\n",
      "\n",
      "ğŸ“Š Top 20 Words in Idioms:\n",
      "   1. '××ª':  965\n",
      "   2. '×”×¨××©':  240\n",
      "   3. '×©×‘×¨':  201\n",
      "   4. '×‘××§×•×':  160\n",
      "   5. '×‘×™×Ÿ':  160\n",
      "   6. '×”×–× ×‘':  159\n",
      "   7. '×™×¨×“':  119\n",
      "   8. '×¤×ª×—':  105\n",
      "   9. '×¢×¦××•':  104\n",
      "  10. '×©×‘×¨×”':   99\n",
      "  11. '×”×¨×™×':   95\n",
      "  12. '×œ×•':   83\n",
      "  13. '×¢×œ':   82\n",
      "  14. '×›××•':   81\n",
      "  15. '×‘××©':   80\n",
      "  16. '×™×“×™×™×':   80\n",
      "  17. '×”×˜×™×¤×•×ª':   80\n",
      "  18. '×”×›×œ×™×':   80\n",
      "  19. '×™×“':   80\n",
      "  20. '×¢×™× ×™×™×':   80\n",
      "\n",
      "ğŸ“Š Lexical Statistics by Label:\n",
      "\n",
      "  ×¤×™×’×•×¨×˜×™×‘×™:\n",
      "    â€¢ Vocabulary size: 11,225\n",
      "    â€¢ Total tokens: 38,579\n",
      "    â€¢ TTR: 0.2910\n",
      "    â€¢ Top 5 words: ['××ª', '×œ×', '×”×•×', '×”×™×', '×¢×œ']\n",
      "\n",
      "  ××™×œ×•×œ×™:\n",
      "    â€¢ Vocabulary size: 11,302\n",
      "    â€¢ Total tokens: 36,833\n",
      "    â€¢ TTR: 0.3068\n",
      "    â€¢ Top 5 words: ['××ª', '×œ×', '×”×•×', '×¢×œ', '×”×™×']\n",
      "\n",
      "ğŸ“Š Function Word Frequencies:\n",
      "  â€¢ '××ª': 2295 (3.04%)\n",
      "  â€¢ '×œ×': 1289 (1.71%)\n",
      "  â€¢ '×”×•×': 1105 (1.47%)\n",
      "  â€¢ '×¢×œ':  918 (1.22%)\n",
      "  â€¢ '×©×œ':  756 (1.00%)\n",
      "  â€¢ '××':  623 (0.83%)\n",
      "  â€¢ '×”×™×”':  509 (0.67%)\n",
      "  â€¢ '×¢×':  426 (0.56%)\n",
      "  â€¢ '××‘×œ':  326 (0.43%)\n",
      "  â€¢ '×›×™':  318 (0.42%)\n",
      "\n",
      "ğŸ“Š Key Finding: High lexical diversity!\n",
      "   Vocabulary size: 18,784 unique words\n",
      "   Type-Token Ratio: 0.2491\n",
      "   Avg unique words per sentence: 15.38\n"
     ]
    }
   ],
   "source": [
    "# Compute comprehensive lexical statistics\n",
    "lexical_stats = loader.analyze_lexical_statistics()\n",
    "\n",
    "# Show lexical summary\n",
    "print(\"\\nğŸ“Š Key Finding: High lexical diversity!\")\n",
    "print(f\"   Vocabulary size: {lexical_stats['vocabulary_size']:,} unique words\")\n",
    "print(f\"   Type-Token Ratio: {lexical_stats['ttr_overall']:.4f}\")\n",
    "print(f\"   Avg unique words per sentence: {lexical_stats['avg_unique_per_sentence']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PART 2: OPTIONAL/RECOMMENDED ANALYSES\n",
    "\n",
    "### 4.1 Structural Complexity Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:58:21.154682Z",
     "start_time": "2025-11-19T11:58:20.925838Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.015407Z",
     "iopub.status.busy": "2025-11-19T12:11:31.015333Z",
     "iopub.status.idle": "2025-11-19T12:11:31.177665Z",
     "shell.execute_reply": "2025-11-19T12:11:31.177231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STRUCTURAL COMPLEXITY ANALYSIS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Overall Structural Complexity:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  â€¢ Mean subclause markers per sentence: 0.28\n",
      "  â€¢ Mean subclause ratio: 0.0146\n",
      "  â€¢ Mean punctuation marks per sentence: 1.81\n",
      "  â€¢ Sentences with subclauses: 1177 (24.52%)\n",
      "\n",
      "ğŸ“Š Structural Complexity by Label:\n",
      "\n",
      "  ×¤×™×’×•×¨×˜×™×‘×™:\n",
      "    â€¢ Mean subclause markers: 0.31\n",
      "    â€¢ Mean subclause ratio: 0.0170\n",
      "    â€¢ Mean punctuation: 1.87\n",
      "\n",
      "  ××™×œ×•×œ×™:\n",
      "    â€¢ Mean subclause markers: 0.25\n",
      "    â€¢ Mean subclause ratio: 0.0122\n",
      "    â€¢ Mean punctuation: 1.75\n",
      "\n",
      "ğŸ“Š Key Finding: Figurative sentences are more complex!\n",
      "   Mean subclause markers: 0.28\n",
      "   Sentences with subclauses: 24.52%\n",
      "   Mean punctuation: 1.81\n"
     ]
    }
   ],
   "source": [
    "# Analyze structural complexity (subclauses, punctuation)\n",
    "complexity_stats = loader.analyze_structural_complexity()\n",
    "\n",
    "# Show complexity summary (using correct key names!)\n",
    "print(\"\\nğŸ“Š Key Finding: Figurative sentences are more complex!\")\n",
    "print(f\"   Mean subclause markers: {complexity_stats['mean_subclause_count']:.2f}\")\n",
    "print(f\"   Sentences with subclauses: {complexity_stats['sentences_with_subclauses_pct']:.2f}%\")\n",
    "print(f\"   Mean punctuation: {complexity_stats['mean_punctuation_count']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Lexical Richness Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:58:24.670201Z",
     "start_time": "2025-11-19T11:58:24.629712Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.178971Z",
     "iopub.status.busy": "2025-11-19T12:11:31.178898Z",
     "iopub.status.idle": "2025-11-19T12:11:31.201608Z",
     "shell.execute_reply": "2025-11-19T12:11:31.201237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEXICAL RICHNESS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Lexical Richness Statistics:\n",
      "  â€¢ Total tokens: 75,412\n",
      "  â€¢ Unique words: 18,784\n",
      "  â€¢ Type-Token Ratio (TTR): 0.2491\n",
      "  â€¢ Hapax legomena (words appearing once): 11,921 (63.46%)\n",
      "  â€¢ Dis legomena (words appearing twice): 2,850\n",
      "  â€¢ Maas Index: 0.0110\n",
      "\n",
      "ğŸ“Š Lexical Richness by Label:\n",
      "\n",
      "  ×¤×™×’×•×¨×˜×™×‘×™:\n",
      "    â€¢ Unique words: 11,225\n",
      "    â€¢ TTR: 0.2910\n",
      "    â€¢ Hapax legomena: 7,387 (65.81%)\n",
      "\n",
      "  ××™×œ×•×œ×™:\n",
      "    â€¢ Unique words: 11,302\n",
      "    â€¢ TTR: 0.3068\n",
      "    â€¢ Hapax legomena: 7,583 (67.09%)\n",
      "\n",
      "ğŸ“Š Key Finding: Very high lexical richness!\n",
      "   Hapax legomena: 11,921 (63.46%)\n",
      "   Dis legomena: 2,850\n",
      "   Maas Index: 0.0110\n"
     ]
    }
   ],
   "source": [
    "# Analyze lexical richness (hapax legomena, Zipf's law)\n",
    "richness_stats = loader.analyze_lexical_richness()\n",
    "\n",
    "# Show richness summary\n",
    "print(\"\\nğŸ“Š Key Finding: Very high lexical richness!\")\n",
    "print(f\"   Hapax legomena: {richness_stats['hapax_legomena_count']:,} ({richness_stats['hapax_ratio']*100:.2f}%)\")\n",
    "print(f\"   Dis legomena: {richness_stats['dis_legomena_count']:,}\")\n",
    "print(f\"   Maas Index: {richness_stats['maas_index']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Collocational Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:58:32.839843Z",
     "start_time": "2025-11-19T11:58:32.730216Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.203009Z",
     "iopub.status.busy": "2025-11-19T12:11:31.202928Z",
     "iopub.status.idle": "2025-11-19T12:11:31.271988Z",
     "shell.execute_reply": "2025-11-19T12:11:31.271539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLLOCATIONAL ANALYSIS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Collocational Statistics:\n",
      "  â€¢ Total context words (Â±3 tokens): 23,366\n",
      "  â€¢ Unique context words: 8,498\n",
      "\n",
      "ğŸ“Š Top 20 Context Words (Overall):\n",
      "   1. '×”×•×':  844 (3.61%)\n",
      "   2. '×”×™×':  745 (3.19%)\n",
      "   3. '×œ×':  493 (2.11%)\n",
      "   4. '×”×':  423 (1.81%)\n",
      "   5. '×¢×œ':  362 (1.55%)\n",
      "   6. '××ª':  328 (1.40%)\n",
      "   7. '×¢×':  256 (1.10%)\n",
      "   8. '×©×œ':  242 (1.04%)\n",
      "   9. '×›×“×™':  241 (1.03%)\n",
      "  10. '××—×¨×™':  234 (1.00%)\n",
      "  11. '×›×™':  177 (0.76%)\n",
      "  12. '×”×™×”':  172 (0.74%)\n",
      "  13. '×”×™×œ×“':  137 (0.59%)\n",
      "  14. '×œ××—×¨':  114 (0.49%)\n",
      "  15. '××':  114 (0.49%)\n",
      "  16. '××•×œ':  110 (0.47%)\n",
      "  17. '×›×œ':  107 (0.46%)\n",
      "  18. '×‘×–××Ÿ':  100 (0.43%)\n",
      "  19. '×•×œ×':   93 (0.40%)\n",
      "  20. '××‘×œ':   91 (0.39%)\n",
      "\n",
      "ğŸ“Š Top 10 Context Words by Label:\n",
      "\n",
      "  Literal:\n",
      "     1. '×”×•×':  378\n",
      "     2. '×”×™×':  318\n",
      "     3. '×¢×œ':  205\n",
      "     4. '×œ×':  202\n",
      "     5. '×”×':  191\n",
      "     6. '×›×“×™':  160\n",
      "     7. '××ª':  147\n",
      "     8. '×©×œ':  137\n",
      "     9. '×”×™×œ×“':  118\n",
      "    10. '×¢×':  115\n",
      "\n",
      "  Figurative:\n",
      "     1. '×”×•×':  466\n",
      "     2. '×”×™×':  427\n",
      "     3. '×œ×':  291\n",
      "     4. '×”×':  232\n",
      "     5. '××ª':  181\n",
      "     6. '×¢×œ':  157\n",
      "     7. '×¢×':  141\n",
      "     8. '××—×¨×™':  138\n",
      "     9. '×›×™':  116\n",
      "    10. '×©×œ':  105\n",
      "\n",
      "ğŸ“Š Key Finding: Rich context around idioms!\n",
      "   Total context words: 23,366\n",
      "   Unique context words: 8,498\n"
     ]
    }
   ],
   "source": [
    "# Analyze collocations (context words around idioms)\n",
    "collocation_stats = loader.analyze_collocations()\n",
    "\n",
    "# Show collocation summary\n",
    "print(\"\\nğŸ“Š Key Finding: Rich context around idioms!\")\n",
    "print(f\"   Total context words: {collocation_stats['total_context_words']:,}\")\n",
    "print(f\"   Unique context words: {collocation_stats['unique_context_words']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Annotation Consistency Analysis (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:58:37.702581Z",
     "start_time": "2025-11-19T11:58:37.591882Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.273328Z",
     "iopub.status.busy": "2025-11-19T12:11:31.273241Z",
     "iopub.status.idle": "2025-11-19T12:11:31.346949Z",
     "shell.execute_reply": "2025-11-19T12:11:31.346523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANNOTATION CONSISTENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Annotation Consistency:\n",
      "  â€¢ Prefix attachments found: 2172 (45.25%)\n",
      "  â€¢ Mean consistency rate per idiom: 0.3954\n",
      "\n",
      "ğŸ“Š Idioms with Variant Forms (Top 10):\n",
      "  1. ×©× ×¨×’×œ×™×™×: 35 variants (consistency: 0.19)\n",
      "  2. ×©×‘×¨ ××ª ×”×œ×‘: 32 variants (consistency: 0.34)\n",
      "  3. ×¤×ª×— ×“×œ×ª×•×ª: 29 variants (consistency: 0.29)\n",
      "  4. ×¡×’×¨ ×—×©×‘×•×Ÿ: 28 variants (consistency: 0.24)\n",
      "  5. ×”×•×¨×™×“ ×¤×¨×•×¤×™×œ: 23 variants (consistency: 0.31)\n",
      "  6. ×™×¦× ××”×§×•×¤×¡×”: 22 variants (consistency: 0.44)\n",
      "  7. ×—×ª×š ×¤×™× ×”: 18 variants (consistency: 0.31)\n",
      "  8. ×§×™×‘×œ ×¡×˜×™×¨×”: 18 variants (consistency: 0.55)\n",
      "  9. ×©× ×¢×œ×™×• ×¤×¡: 18 variants (consistency: 0.28)\n",
      "  10. ×©×‘×¨ ××ª ×”×¨××©: 17 variants (consistency: 0.41)\n",
      "\n",
      "ğŸ“Š Key Finding: Significant morphological variance!\n",
      "   Prefix attachments: 2172 (45.25%)\n",
      "   Mean consistency rate: 0.3954\n"
     ]
    }
   ],
   "source": [
    "# Analyze annotation consistency\n",
    "consistency_stats = loader.analyze_annotation_consistency()\n",
    "\n",
    "# Show consistency summary\n",
    "print(\"\\nğŸ“Š Key Finding: Significant morphological variance!\")\n",
    "print(f\"   Prefix attachments: {consistency_stats['prefix_attachment_count']} ({consistency_stats['prefix_attachment_rate']*100:.2f}%)\")\n",
    "print(f\"   Mean consistency rate: {consistency_stats['mean_consistency_rate']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create All Visualizations\n",
    "\n",
    "### 5.1 Standard Visualizations (PART 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T09:34:36.515330Z",
     "start_time": "2025-11-11T09:34:34.137986Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:31.348441Z",
     "iopub.status.busy": "2025-11-19T12:11:31.348370Z",
     "iopub.status.idle": "2025-11-19T12:11:35.216771Z",
     "shell.execute_reply": "2025-11-19T12:11:35.216295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating standard visualizations...\n",
      "\n",
      "================================================================================\n",
      "CREATING VISUALIZATIONS (Missions 2.2 & 2.4)\n",
      "================================================================================\n",
      "\n",
      "[1/6] Creating label distribution bar chart...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/label_distribution.png\n",
      "\n",
      "[2/6] Creating sentence length distribution histogram...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_length_distribution.png\n",
      "\n",
      "[3/6] Creating idiom length distribution histogram...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/idiom_length_distribution.png\n",
      "\n",
      "[4/6] Creating top 10 idioms bar chart...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/top_10_idioms.png\n",
      "\n",
      "[5/6] Creating sentence type distribution pie chart...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_type_distribution.png\n",
      "\n",
      "[6/6] Creating sentence type by label stacked bar chart...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_type_by_label.png\n",
      "\n",
      "[7/11] Creating sentence length boxplot by label...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_length_boxplot_by_label.png\n",
      "\n",
      "[8/11] Creating polysemy heatmap...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/polysemy_heatmap.png\n",
      "\n",
      "[9/11] Creating idiom position histogram...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/idiom_position_histogram.png\n",
      "\n",
      "[10/11] Creating idiom position by label bar chart...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/idiom_position_by_label.png\n",
      "\n",
      "[11/11] Creating sentence length violin plot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igornazarenko/PycharmProjects/Final_Project_NLP/src/data_preparation.py:1679: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(data=plot_data, x='label', y='num_tokens', ax=ax,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/sentence_length_violin_by_label.png\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL VISUALIZATIONS CREATED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Location: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures\n",
      "\n",
      "Created files:\n",
      "  1. label_distribution.png\n",
      "  2. sentence_length_distribution.png\n",
      "  3. idiom_length_distribution.png\n",
      "  4. top_10_idioms.png\n",
      "  5. sentence_type_distribution.png\n",
      "  6. sentence_type_by_label.png\n",
      "  7. sentence_length_boxplot_by_label.png (NEW)\n",
      "  8. polysemy_heatmap.png (NEW)\n",
      "  9. idiom_position_histogram.png (NEW)\n",
      "  10. idiom_position_by_label.png (NEW)\n",
      "  11. sentence_length_violin_by_label.png (NEW)\n",
      "\n",
      "âœ… Standard visualizations saved to paper/figures/\n"
     ]
    }
   ],
   "source": [
    "# Create all 11 standard visualizations\n",
    "print(\"Creating standard visualizations...\")\n",
    "loader.create_visualizations()\n",
    "print(\"\\nâœ… Standard visualizations saved to paper/figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Advanced Visualizations (PART 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:42:33.928148Z",
     "start_time": "2025-11-10T13:42:32.757136Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:35.218080Z",
     "iopub.status.busy": "2025-11-19T12:11:35.218000Z",
     "iopub.status.idle": "2025-11-19T12:11:37.040247Z",
     "shell.execute_reply": "2025-11-19T12:11:37.039772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced visualizations...\n",
      "\n",
      "================================================================================\n",
      "CREATING ADVANCED VISUALIZATIONS (PART 2)\n",
      "================================================================================\n",
      "\n",
      "[1/6] Creating Zipf's law plot...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/zipf_law_plot.png\n",
      "\n",
      "[2/6] Creating structural complexity comparison...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/structural_complexity_by_label.png\n",
      "\n",
      "[3/6] Creating collocation word clouds...\n",
      "   âš ï¸  wordcloud library not available. Skipping word clouds.\n",
      "      Install with: pip install wordcloud\n",
      "\n",
      "[4/6] Creating vocabulary diversity scatter plot...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/vocabulary_diversity_scatter.png\n",
      "\n",
      "[5/6] Creating hapax legomena comparison...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/hapax_legomena_comparison.png\n",
      "\n",
      "[6/6] Creating context words bar chart...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures/context_words_bar_chart.png\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL ADVANCED VISUALIZATIONS CREATED!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Location: /Users/igornazarenko/PycharmProjects/Final_Project_NLP/paper/figures\n",
      "\n",
      "Created advanced visualization files:\n",
      "  1. zipf_law_plot.png\n",
      "  2. structural_complexity_by_label.png\n",
      "  3. collocation_word_clouds.png (if wordcloud library available)\n",
      "  4. vocabulary_diversity_scatter.png\n",
      "  5. hapax_legomena_comparison.png\n",
      "  6. context_words_bar_chart.png\n",
      "\n",
      "âœ… Advanced visualizations saved to paper/figures/\n"
     ]
    }
   ],
   "source": [
    "# Create all 6 advanced visualizations\n",
    "print(\"Creating advanced visualizations...\")\n",
    "loader.create_advanced_visualizations()\n",
    "print(\"\\nâœ… Advanced visualizations saved to paper/figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. COMPREHENSIVE ANALYSIS - ALL IN ONE\n",
    "\n",
    "Alternatively, run everything at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.041715Z",
     "iopub.status.busy": "2025-11-19T12:11:37.041610Z",
     "iopub.status.idle": "2025-11-19T12:11:37.043295Z",
     "shell.execute_reply": "2025-11-19T12:11:37.042937Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run comprehensive analysis (PART 1 + PART 2, including visualizations)\n",
    "# NOTE: Only run this if you haven't run the analyses above\n",
    "\n",
    "# Uncomment to run:\n",
    "# loader_new = DatasetLoader()\n",
    "# loader_new.load_dataset()\n",
    "# results = loader_new.run_comprehensive_analysis(include_part2=True, create_visualizations=True)\n",
    "# print(\"\\nâœ… Comprehensive analysis complete!\")\n",
    "# print(f\"   Total sentences: {results['statistics']['total_sentences']}\")\n",
    "# print(f\"   Unique idioms: {results['statistics']['unique_expressions']}\")\n",
    "# print(f\"   Vocabulary: {results['lexical']['vocabulary_size']:,} words\")\n",
    "# print(f\"   Hapax legomena: {results['lexical_richness']['hapax_legomena_count']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explore Specific Examples\n",
    "\n",
    "### 7.1 Compare Literal vs Figurative for Same Idiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T09:34:47.412336Z",
     "start_time": "2025-11-11T09:34:47.404118Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.044477Z",
     "iopub.status.busy": "2025-11-19T12:11:37.044411Z",
     "iopub.status.idle": "2025-11-19T12:11:37.047911Z",
     "shell.execute_reply": "2025-11-19T12:11:37.047512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for idiom: '×©×‘×¨ ××ª ×”×¨××©'\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¹ LITERAL Examples:\n",
      "\n",
      "×”×™×œ×“ ×©×‘×¨ ××ª ×”×¨××© ×›×©×”×ª×’×œ×’×œ ××”××™×˜×” ×”×—×“×©×” ×‘×¡×œ×•×Ÿ, ×•× ×œ×§×— ×œ×‘×“×™×§×” ×‘×‘×™×ª ×”×—×•×œ×™× ×”×§×¨×•×‘ ×©× ×’×™×œ×• ×¨×§ ×—×‘×œ×” ×§×œ×”.\n",
      "\n",
      "×”×™× ×©×‘×¨×” ××ª ×”×¨××© ×›×©× ×ª×§×œ×” ×‘××‘×Ÿ ×’×“×•×œ×” ×‘×©×‘×™×œ ×”×’×™× ×”\n",
      "\n",
      "×”×× ×©××¢×ª ×©×”×©×—×§×Ÿ ×©×‘×¨ ××ª ×”×¨××© ×‘×××¦×¢ ×”×”×¦×’×” ×œ××—×¨ ×©× ×¤×œ ××”×‘××”, ×•×”×§×”×œ ×”×”××•× ×œ× ×”×‘×™×Ÿ ×× ×–×• ×”×™×™×ª×” ×ª××•× ×” ××• ×—×œ×§ ××”×¢×œ×™×œ×”?\n",
      "\n",
      "================================================================================\n",
      "ğŸ”¸ FIGURATIVE Examples:\n",
      "\n",
      "×× ×©×¨ ×”××•×¦×¨ ×œ× ×™××¦× ×¤×ª×¨×•×Ÿ ×œ××©×‘×¨ ×”×—××•×¨ ×©× ×•×¦×¨ ×‘×¢×§×‘×•×ª ×”××œ×—××” ×”××¨×•×›×” ×‘×™×•×ª×¨ ×¢×“ ×›×”, ×”×•× ×¢×“×™×™×Ÿ ×™××©×™×š ×œ×©×‘×•×¨ ××ª ×”×¨××© ×‘× ×™×¡×™×•×Ÿ ×œ×”×‘×™×Ÿ ××™×š ×œ×ª×§×Ÿ ××ª ×”×‘×¢×™×” ×”×ª×§×¦×™×‘×™×ª.\n",
      "\n",
      "×›××” ×–××Ÿ ×©×‘×¨×ª ××ª ×”×¨××© ×¢×œ × ×™×¡×•×— ×”××™×™×œ ×¢× ×”×¨×¢×™×•×Ÿ ×”××”×¤×›× ×™ ×”×–×” ×©×œ×š ×œ×“×™×¨×§×˜×•×¨×™×•×Ÿ, ×•×”×× ×‘×¡×•×£ ×©×œ×—×ª ××•×ª×• ×›×¤×™ ×©×”×•×?\n",
      "\n",
      "×”×™× ××¢×•×œ× ×œ× ×©×‘×¨×” ××ª ×”×¨××© ×›×œ ×›×š ×”×¨×‘×” ×–××Ÿ ×›×“×™ ×œ××¦×•× ×¨×¢×™×•×Ÿ ×™×¦×™×¨×ª×™ ×œ××ª× ×ª ×™×•× ×”×•×œ×“×ª ××¤×ª×™×¢×” ×›×–××ª, ×•×‘×¡×•×£, ××—×¨×™ ×™××™× ×©×œ ×—×™×¤×•×©×™×, ×”×—×œ×™×˜×” ×œ×”×–××™×Ÿ ×—×•×¤×©×” ×¨×•×× ×˜×™×ª ×‘××™×˜×œ×™×” ×›×”×¤×ª×¢×” ××•×©×œ××ª.\n"
     ]
    }
   ],
   "source": [
    "# Show examples of literal vs figurative for same idiom\n",
    "sample_idiom = \"×©×‘×¨ ××ª ×”×¨××©\"  # \"broke the head\"\n",
    "\n",
    "print(f\"Examples for idiom: '{sample_idiom}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "idiom_samples = loader.df[loader.df['expression'] == sample_idiom]\n",
    "literal_examples = idiom_samples[idiom_samples['label'] == '××™×œ×•×œ×™'].head(3)\n",
    "figurative_examples = idiom_samples[idiom_samples['label'] == '×¤×™×’×•×¨×˜×™×‘×™'].head(3)\n",
    "\n",
    "print(\"\\nğŸ”¹ LITERAL Examples:\")\n",
    "for idx, row in literal_examples.iterrows():\n",
    "    print(f\"\\n{row['text']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ”¸ FIGURATIVE Examples:\")\n",
    "for idx, row in figurative_examples.iterrows():\n",
    "    print(f\"\\n{row['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Examine Top Context Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:58:54.894725Z",
     "start_time": "2025-11-19T11:58:54.890499Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.049189Z",
     "iopub.status.busy": "2025-11-19T12:11:37.049105Z",
     "iopub.status.idle": "2025-11-19T12:11:37.051273Z",
     "shell.execute_reply": "2025-11-19T12:11:37.050854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Context Words Around Idioms:\n",
      "==================================================\n",
      " 1. '×”×•×':  844 occurrences\n",
      " 2. '×”×™×':  745 occurrences\n",
      " 3. '×œ×':  493 occurrences\n",
      " 4. '×”×':  423 occurrences\n",
      " 5. '×¢×œ':  362 occurrences\n",
      " 6. '××ª':  328 occurrences\n",
      " 7. '×¢×':  256 occurrences\n",
      " 8. '×©×œ':  242 occurrences\n",
      " 9. '×›×“×™':  241 occurrences\n",
      "10. '××—×¨×™':  234 occurrences\n",
      "11. '×›×™':  177 occurrences\n",
      "12. '×”×™×”':  172 occurrences\n",
      "13. '×”×™×œ×“':  137 occurrences\n",
      "14. '×œ××—×¨':  114 occurrences\n",
      "15. '××':  114 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Show top context words from collocation analysis\n",
    "if 'collocation_stats' in locals():\n",
    "    print(\"Top 15 Context Words Around Idioms:\")\n",
    "    print(\"=\"*50)\n",
    "    for i, (word, count) in enumerate(collocation_stats['top_20_context_overall'][:15], 1):\n",
    "        print(f\"{i:2d}. '{word}': {count:4d} occurrences\")\n",
    "else:\n",
    "    print(\"Run analyze_collocations() first to see context words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Examine Idioms with Most Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:59:34.008721Z",
     "start_time": "2025-11-19T11:59:33.995120Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.052589Z",
     "iopub.status.busy": "2025-11-19T12:11:37.052510Z",
     "iopub.status.idle": "2025-11-19T12:11:37.060403Z",
     "shell.execute_reply": "2025-11-19T12:11:37.059953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idioms with Most Variant Forms:\n",
      "==================================================\n",
      " 1. ×©× ×¨×’×œ×™×™×                               : 35 variants\n",
      "    - ×©× ×¨×’×œ×™×™×: 15 times\n",
      "    - ×©××” ×¨×’×œ×™×™×: 14 times\n",
      "    - ×©××• ×œ×• ×¨×’×œ×™×™×: 4 times\n",
      "\n",
      " 2. ×©×‘×¨ ××ª ×”×œ×‘                              : 32 variants\n",
      "    - ×©×‘×¨ ××ª ×”×œ×‘: 27 times\n",
      "    - ×©×‘×¨×” ××ª ×”×œ×‘: 13 times\n",
      "    - ×©×‘×¨×• ××ª ×”×œ×‘: 3 times\n",
      "\n",
      " 3. ×¤×ª×— ×“×œ×ª×•×ª                               : 29 variants\n",
      "    - ×¤×ª×— ×“×œ×ª×•×ª: 23 times\n",
      "    - ×¤×ª×—×” ×“×œ×ª×•×ª: 12 times\n",
      "    - ×¤×ª×—×• ×“×œ×ª×•×ª: 5 times\n",
      "\n",
      " 4. ×¡×’×¨ ×—×©×‘×•×Ÿ                               : 28 variants\n",
      "    - ×¡×’×¨ ×—×©×‘×•×Ÿ: 19 times\n",
      "    - ×œ×¡×’×•×¨ ×—×©×‘×•×Ÿ: 11 times\n",
      "    - ×¡×’×¨×” ×—×©×‘×•×Ÿ: 9 times\n",
      "\n",
      " 5. ×”×•×¨×™×“ ×¤×¨×•×¤×™×œ                            : 23 variants\n",
      "    - ×”×•×¨×™×“ ×¤×¨×•×¤×™×œ: 25 times\n",
      "    - ×”×•×¨×™×“×” ×¤×¨×•×¤×™×œ: 14 times\n",
      "    - ×œ×”×•×¨×“×ª ×¤×¨×•×¤×™×œ: 6 times\n",
      "\n",
      " 6. ×™×¦× ××”×§×•×¤×¡×”                             : 22 variants\n",
      "    - ×™×¦× ××”×§×•×¤×¡×”: 35 times\n",
      "    - ×™×¦××” ××”×§×•×¤×¡×”: 10 times\n",
      "    - ×œ×¦××ª ××”×§×•×¤×¡×”: 5 times\n",
      "\n",
      " 7. ×§×™×‘×œ ×¡×˜×™×¨×”                              : 18 variants\n",
      "    - ×§×™×‘×œ ×¡×˜×™×¨×”: 44 times\n",
      "    - ×§×™×‘×œ×” ×¡×˜×™×¨×”: 14 times\n",
      "    - ×§×™×‘×œ×• ×¡×˜×™×¨×”: 4 times\n",
      "\n",
      " 8. ×©× ×¢×œ×™×• ×¤×¡                              : 18 variants\n",
      "    - ×©× ×¤×¡: 22 times\n",
      "    - ×©× ×¢×œ×™×• ×¤×¡: 17 times\n",
      "    - ×©××• ×¢×œ×™×• ×¤×¡: 11 times\n",
      "\n",
      " 9. ×—×ª×š ×¤×™× ×”                                : 18 variants\n",
      "    - ×—×ª×š ×¤×™× ×”: 25 times\n",
      "    - ×—×ª×›×” ×¤×™× ×”: 12 times\n",
      "    - ×œ×—×ª×•×š ×¤×™× ×”: 9 times\n",
      "\n",
      "10. ×™×¨×“ ×œ×• ×”××¡×™××•×Ÿ                          : 17 variants\n",
      "    - ×™×¨×“ ×œ×• ×”××¡×™××•×Ÿ: 35 times\n",
      "    - ×™×¨×“ ×œ×” ×”××¡×™××•×Ÿ: 9 times\n",
      "    - ×™×¨×“ ×œ×™ ×”××¡×™××•×Ÿ: 8 times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show idioms with most morphological variance\n",
    "print(\"Idioms with Most Variant Forms:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count unique matched_expression per idiom\n",
    "variance_df = loader.df.groupby('expression')['matched_expression'].nunique().sort_values(ascending=False)\n",
    "\n",
    "for i, (idiom, variants) in enumerate(variance_df.head(10).items(), 1):\n",
    "    print(f\"{i:2d}. {idiom:40s}: {variants:2d} variants\")\n",
    "    \n",
    "    # Show sample variants\n",
    "    sample_variants = loader.df[loader.df['expression'] == idiom]['matched_expression'].value_counts().head(3)\n",
    "    for variant, count in sample_variants.items():\n",
    "        print(f\"    - {variant}: {count} times\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:59:38.330379Z",
     "start_time": "2025-11-19T11:59:38.322395Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.061681Z",
     "iopub.status.busy": "2025-11-19T12:11:37.061615Z",
     "iopub.status.idle": "2025-11-19T12:11:37.065198Z",
     "shell.execute_reply": "2025-11-19T12:11:37.064762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE DATASET SUMMARY\n",
      "================================================================================\n",
      "                         Metric           Value\n",
      "                Total Sentences           4,800\n",
      "                  Unique Idioms              60\n",
      "   Avg Sentence Length (tokens)           15.71\n",
      "      Avg Idiom Length (tokens)            2.48\n",
      "                Vocabulary Size          18,784\n",
      "               Type-Token Ratio          0.2491\n",
      "                 Hapax Legomena 11,921 (63.46%)\n",
      "              Polysemous Idioms       60 (100%)\n",
      "            Idioms at Start (%)          63.71%\n",
      "  Sentences with Subclauses (%)          24.52%\n",
      "Mean Prefix Attachment Rate (%)          45.25%\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Sentences',\n",
    "        'Unique Idioms',\n",
    "        'Avg Sentence Length (tokens)',\n",
    "        'Avg Idiom Length (tokens)',\n",
    "        'Vocabulary Size',\n",
    "        'Type-Token Ratio',\n",
    "        'Hapax Legomena',\n",
    "        'Polysemous Idioms',\n",
    "        'Idioms at Start (%)',\n",
    "        'Sentences with Subclauses (%)',\n",
    "        'Mean Prefix Attachment Rate (%)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{len(loader.df):,}\",\n",
    "        f\"{loader.df['expression'].nunique()}\",\n",
    "        f\"{loader.df['num_tokens'].mean():.2f}\",\n",
    "        f\"{(loader.df['token_span_end'] - loader.df['token_span_start']).mean():.2f}\",\n",
    "        f\"{lexical_stats['vocabulary_size']:,}\" if 'lexical_stats' in locals() else \"Run lexical analysis\",\n",
    "        f\"{lexical_stats['ttr_overall']:.4f}\" if 'lexical_stats' in locals() else \"Run lexical analysis\",\n",
    "        f\"{richness_stats['hapax_legomena_count']:,} ({richness_stats['hapax_ratio']*100:.2f}%)\" if 'richness_stats' in locals() else \"Run richness analysis\",\n",
    "        f\"{polysemy_stats['polysemous_count']} (100%)\" if 'polysemy_stats' in locals() else \"Run polysemy analysis\",\n",
    "        f\"{position_stats['position_percentages']['start']:.2f}%\" if 'position_stats' in locals() else \"Run position analysis\",\n",
    "        f\"{complexity_stats['sentences_with_subclauses_pct']:.2f}%\" if 'complexity_stats' in locals() else \"Run complexity analysis\",\n",
    "        f\"{consistency_stats['prefix_attachment_rate']*100:.2f}%\" if 'consistency_stats' in locals() else \"Run consistency analysis\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validation Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:59:43.014907Z",
     "start_time": "2025-11-19T11:59:42.994162Z"
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T12:11:37.066576Z",
     "iopub.status.busy": "2025-11-19T12:11:37.066470Z",
     "iopub.status.idle": "2025-11-19T12:11:37.075909Z",
     "shell.execute_reply": "2025-11-19T12:11:37.075546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE VALIDATION CHECKLIST\n",
      "================================================================================\n",
      "âœ… Dataset loads successfully\n",
      "âœ… 4,800 total sentences\n",
      "âœ… Perfect label balance (50/50)\n",
      "âœ… 60 unique idioms\n",
      "âœ… No duplicate rows\n",
      "âœ… All idioms are polysemous\n",
      "âœ… High lexical diversity (>60% hapax)\n",
      "âŒ Most idioms at sentence start\n",
      "\n",
      "âš ï¸ Some criteria not met. Run all analyses above.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE VALIDATION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checklist = [\n",
    "    (\"Dataset loads successfully\", len(loader.df) > 0),\n",
    "    (\"4,800 total sentences\", len(loader.df) == 4800),\n",
    "    (\"Perfect label balance (50/50)\", abs(loader.df['label_2'].value_counts()[0] - loader.df['label_2'].value_counts()[1]) == 0),\n",
    "    (\"60 unique idioms\", loader.df['expression'].nunique() == 60),\n",
    "    (\"No duplicate rows\", loader.df.duplicated().sum() == 0),\n",
    "    (\"All idioms are polysemous\", True if 'polysemy_stats' in locals() and polysemy_stats['polysemous_count'] == 60 else False),\n",
    "    (\"High lexical diversity (>60% hapax)\", True if 'richness_stats' in locals() and richness_stats['hapax_ratio'] > 0.6 else False),\n",
    "    (\"Most idioms at sentence start\", True if 'position_stats' in locals() and position_stats['position_percentages']['start'] > 80 else False),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for criterion, passed in checklist:\n",
    "    status = \"âœ…\" if passed else \"âŒ\"\n",
    "    print(f\"{status} {criterion}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ‰ ALL VALIDATION CRITERIA PASSED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"âœ… Dataset is fully analyzed and validated\")\n",
    "    print(\"âœ… Ready for Mission 2.5: Dataset Splitting\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some criteria not met. Run all analyses above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Comprehensive Analysis\n",
    "\n",
    "**Key Findings:**\n",
    "1. **Dataset Quality**: Perfectly balanced, no duplicates, 4,800 sentences\n",
    "2. **Polysemy**: All 60 idioms are polysemous (50/50 literal/figurative)\n",
    "3. **Position**: 87.96% of idioms appear at sentence start\n",
    "4. **Lexical Diversity**: Very high (63.43% hapax legomena)\n",
    "5. **Complexity**: Figurative sentences are more complex\n",
    "6. **Morphology**: Significant variance (41.40% prefix attachments)\n",
    "\n",
    "**Output Files:**\n",
    "- Statistics: `experiments/results/dataset_statistics_full.txt`\n",
    "- Visualizations: `paper/figures/` (17 total)\n",
    "\n",
    "**Next Steps:**\n",
    "- Mission 2.5: Dataset Splitting (Expression-Based Strategy)\n",
    "- Mission 3: Model Training and Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
