# ============================================================================
# Training Configuration for Hebrew Idiom Detection
# File: experiments/configs/training_config.yaml
# Mission 4.1: Training Configuration Setup
# ============================================================================

# Model settings
model_name: "alephbert-base"                # Short name for logging
model_checkpoint: "onlplab/alephbert-base"  # HuggingFace model ID
max_length: 128                             # Maximum sequence length

# Task settings
task: "sequence_classification"             # Options: "sequence_classification", "token_classification"
num_labels: 2                               # 2 for sequence classification (literal/figurative)
                                            # 3 for token classification (O, B-IDIOM, I-IDIOM)

# Training mode
training_mode: "full_finetune"              # Options: "zero_shot", "full_finetune", "frozen_backbone"

# Hyperparameters (from PRD Section 5.1)
# These can be overridden by Optuna in Mission 4.3-4.5 or via CLI
learning_rate: 2e-5                         # Initial learning rate
batch_size: 16                              # Training batch size
num_epochs: 5                               # Number of training epochs
warmup_ratio: 0.1                           # Warmup ratio for learning rate scheduler
weight_decay: 0.01                          # L2 regularization
gradient_accumulation_steps: 1              # Gradient accumulation steps (increase for larger effective batch size)
fp16: null                                  # Mixed precision training (null=auto-detect, true=force enable, false=disable)
                                            # Auto-detection: enables fp16 automatically on CUDA GPUs for 2-3x speedup
                                            # Requires: CUDA GPU with compute capability >= 7.0 (Volta, Turing, Ampere+)
                                            # Benefits: ~2-3x faster training, ~50% less memory, minimal accuracy impact (<0.1% F1)
seed: 42                                    # Random seed for reproducibility

# Advanced training techniques (NEW - improves generalization)
label_smoothing: 0.1                        # Label smoothing factor (0.1 optimal for 3-class IOB2 and binary classification)
                                            # Prevents overconfidence, improves generalization to unseen idioms
                                            # Range: 0.0-0.2 (0.1 is standard, 0.0 disables)
lr_scheduler_type: "cosine"                 # Learning rate scheduler type
                                            # Options: "linear", "cosine", "cosine_with_restarts", "polynomial", "constant"
                                            # Cosine provides smooth decay, good for fine-tuning Transformers
use_crf: true                               # Use CRF layer for Task 2 (token classification) - IOB2 constraint enforcement
                                            # Enforces valid tag transitions (I-IDIOM must follow B-IDIOM)
                                            # Improves span F1 by 1-2%, prevents invalid sequences
                                            # Set to false for faster training (no constraint enforcement)

# Data paths
train_file: "data/splits/train.csv"         # Training data
dev_file: "data/splits/validation.csv"      # Validation data (for early stopping)
test_file: "data/splits/test.csv"           # Test data (final evaluation)

# Output settings
output_dir: "experiments/results/"          # Base output directory
save_steps: 500                             # Save checkpoint every N steps
eval_steps: 500                             # Evaluate on validation set every N steps
logging_steps: 100                          # Log metrics every N steps
save_total_limit: 2                         # Keep only last N checkpoints
load_best_model_at_end: true                # Load best checkpoint at end of training
metric_for_best_model: "f1"                 # Metric to use for selecting best model
greater_is_better: true                     # Higher F1 is better

# Early stopping
early_stopping_patience: 3                  # Stop if no improvement for N evaluations
early_stopping_threshold: 0.0001            # Minimum change to qualify as improvement

# Device
device: "cuda"                              # Options: "cuda", "cpu", "mps"

# Evaluation settings
eval_strategy: "steps"                      # Options: "steps", "epoch", "no" (was: evaluation_strategy in transformers <4.30)
save_strategy: "steps"                      # Options: "steps", "epoch", "no"

# Logging
report_to: "tensorboard"                    # Options: "tensorboard", "wandb", "none" (tensorboard enables learning curves)
logging_dir: "experiments/logs/"            # Directory for training logs (TensorBoard files saved here)

# Additional settings
dataloader_num_workers: 0                   # Number of workers for data loading (0 for single-threaded)
remove_unused_columns: false                # Keep all columns in dataset
label_names: ["labels"]                     # Column names for labels
