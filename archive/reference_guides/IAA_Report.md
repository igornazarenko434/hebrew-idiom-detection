# Inter-Annotator Agreement (IAA) Report

## 1. Overview
This report provides an overview of dataset annotation reliability, including Inter-Annotator Agreement (IAA), Cohen’s Kappa coefficient, disagreement counts, and correction statistics.

## 2. Dataset Summary
- Dataset size: 4,800 items
- Balanced dataset: 2,400 positive (1), 2,400 negative (0)
- Total disagreements: 66 items (1.375%)
  - 0→1 disagreements: 1
  - 1→0 disagreements: 65

## 3. Agreement Metrics
- Observed Agreement (IAA): 98.625%
- Expected Agreement (Chance): 50%
- Cohen’s Kappa: 0.9725 (indicating near-perfect reliability)

## 4. Correction Statistics
- Changed items (non-label errors): 223
- Percentage corrected: 4.65%

## 5. Confusion Matrix
*(The original document did not include values in the matrix table.)*

## 6. Notes and Recommendations
- Very high agreement suggests clear annotation guidelines and strong annotator consistency.
- Review corrected items to identify patterns in non-label errors.
- Consider periodic calibration sessions to maintain consistency.
